[33mcommit d496423cdb7f7d5272f14d517681202a0b9cbe41[m[33m ([m[1;36mHEAD[m[33m -> [m[1;32mversionbump1218[m[33m, [m[1;31mupstream/main[m[33m, [m[1;31mupstream/HEAD[m[33m, [m[1;32mmain[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Dec 18 18:02:28 2025

    Integrate LLVM at llvm/llvm-project@8f264586d752 (#2884)

[33mcommit 6fabd27b15885179a3b6a601ea1e4171f2ed2c91[m[33m ([m[1;32mversionbump18[m[33m)[m
Author: pstarkcdpr <paul.stark@cdprojektred.com>
Date:   Tue Dec 16 19:20:43 2025

    Avoid assert with convolutions with no strides or dilates. Issue #2829 (#2880)
    
    See issue #2829. A convolution without strides or dilates was causing
    spurious asserts.
    
    ---------
    
    Signed-off-by: Paul Stark <paul.stark@cdprojektred.com>

[33mcommit 3934f366c978ee061081b393036382a622c0acd3[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed Dec 10 22:38:04 2025

    Bump patch version after integrate 1.13.5 -> 1.13.6 (#2883)

[33mcommit 1ef9e390b5295e676d2b864fe1924bc2f3f4cf0f[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Dec 4 21:45:27 2025

    Integrate LLVM at llvm/llvm-project@0c2701fe7fa0 (#2878)

[33mcommit c2aa753f83cb3645866169aec3dea548b05a6b3a[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue Dec 2 21:50:41 2025

    Update Bazel dependencies (#2879)

[33mcommit 65e76128953767b58ccd4d6f5fe406625d22bb29[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 19 21:17:03 2025

    Bump patch version after integrate 1.13.4 -> 1.13.5 (#2877)

[33mcommit dfafb2e2d7c8a326a6ea0615b8c571174a24f2d7[m
Author: Kent Slaney <kentslaney@users.noreply.github.com>
Date:   Wed Nov 19 16:51:42 2025

    Spec's scatter example has incorrect dimensions (#2876)
    
    In scatter's example, the dimensions for `%update` don't match between
    the commented input and the MLIR.
    
    
    https://github.com/openxla/stablehlo/blob/96acdcb7724f4a9eec6d2e5af2597b0750c13948/docs/spec.md?plain=1#L5739-L5749
    
    
    https://github.com/openxla/stablehlo/blob/96acdcb7724f4a9eec6d2e5af2597b0750c13948/docs/spec.md?plain=1#L5763
    
    ```python
    >>> jnp.array([
    ...            [
    ...             [[1, 1], [1, 1], [1, 1]],
    ...             [[1, 1], [1, 1], [1, 1]]
    ...            ],
    ...            [
    ...             [[1, 1], [1, 1], [1, 1]],
    ...             [[1, 1], [1, 1], [1, 1]]
    ...            ]
    ...           ]).shape
    (2, 2, 3, 2)
    ```

[33mcommit 85fe3af9ab56e80243299d3d9c7e3000dfa24832[m
Author: vvish <vv.os.swe@gmail.com>
Date:   Wed Nov 19 16:35:27 2025

    Added stablehlo divide fp to tosa reciprocal+mul conversion (#2873)
    
    According to [TOSA
    documentation](https://mlir.llvm.org/docs/Dialects/TOSA/#tosaintdiv-mlirtosaintdivop)
    for FP division "_Floating point divide should use RECIPROCAL and MUL_":
    This PR implements conversion form stablehlo.divide into combination of
    TOSA ops.

[33mcommit 96acdcb7724f4a9eec6d2e5af2597b0750c13948[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Nov 14 19:13:00 2025

    Integrate LLVM at llvm/llvm-project@2bc22ea02edd (#2875)

[33mcommit 3acda594d678e285ed0c8b4179bc1e90348e5997[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Nov 7 19:28:43 2025

    Bump patch version after integrate 1.13.3 -> 1.13.4 (#2871)

[33mcommit 3f27c53c20b9021ccab8b5f673e2c72e5b9cd6aa[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Nov 6 20:06:26 2025

    Integrate LLVM at llvm/llvm-project@42a8ff877d47; migrate op-creation API (#2870)
    
    In addition to bumping the LLVM version, this PR updates call sites of
    an old op-creation API which is marked as deprecated in the latest LLVM
    version.
    
    The old API was of the form `rewriter.create<OpType>(...)`, now replaced
    with `OpType::create(rewriter, ...)`.

[33mcommit 0b5b4a21e71d58df22369860e52ce5ffb382084b[m
Author: Daniel Falbel <dfalbel@gmail.com>
Date:   Wed Nov 5 00:17:04 2025

    Add StableHLO shape assertion check pass and related tests (#2869)
    
    This PR adds the CheckShapeAssertion from XLA into StableHLO:
    
    
    https://github.com/openxla/xla/blob/949bb09eb7d0bb563f5b564fd7d0782e0687796e/xla/python/refine_polymorphic_shapes.cc#L72-L98
    
    This idea was discussed in the discord channel:
    https://discord.com/channels/999073994483433573/999074539138990131/1409946189721374803
    
    This allows taking programs exported from JAX and using just the
    satblehlo binary to produce a program that can be compile and executed
    with PJRT plugins.
    
    ----
    
    
    A usage example is for instance, export a program such as:
    
    ```
    import jax
    from jax import export
    import jax.numpy as jnp
    import numpy as np
    
    def f1(x, y): # x: f32[a, 1], y : f32[a, 4]
     return x + y
    
    # Assuming you have some actual args with concrete shapes
    x = np.ones((3, 1), dtype=np.int32)
    y = np.ones((3, 4), dtype=np.int32)
    args_specs = export.symbolic_args_specs((x, y), 'a, ...')
    exp = export.export(jax.jit(f1))(* args_specs)
    
    code = exp.mlir_module()
    print(code)
    ```
    
    This gives us stableHLO like:
    
    ```
    #loc = loc(unknown)
    #loc2 = loc("x")
    #loc3 = loc("y")
    module @jit_f1 attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
      func.func public @main(%arg0: tensor<?x1xi32> loc(unknown), %arg1: tensor<?x4xi32> loc(unknown)) -> (tensor<?x4xi32> {jax.result_info = "result"}) {
        %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)
        %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x1xi32>) -> tensor<i32> loc(#loc7)
        %1 = stablehlo.get_dimension_size %arg1, dim = 0 : (tensor<?x4xi32>) -> tensor<i32> loc(#loc7)
        %2 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc8)
        stablehlo.custom_call @shape_assertion(%2, %0) {api_version = 2 : i32, error_message = "Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 1),args[1].shape = (a, 4). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://docs.jax.dev/en/latest/export/shape_poly.html#shape-assertion-errors for more details.", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc9)
        %3 = stablehlo.compare  EQ, %1, %0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc10)
        stablehlo.custom_call @shape_assertion(%3, %1, %0) {api_version = 2 : i32, error_message = "Input shapes do not match the polymorphic shapes specification. Found inconsistency between dimension size args[1].shape[0] (= {0}) and the specification 'a' (= {1}). Using the following polymorphic shapes specifications: args[0].shape = (a, 1),args[1].shape = (a, 4). Obtained dimension variables: 'a' = {1} from specification 'a' for dimension args[0].shape[0] (= {1}), . Please see https://docs.jax.dev/en/latest/export/shape_poly.html#shape-assertion-errors for more details.", has_side_effect = true} : (tensor<i1>, tensor<i32>, tensor<i32>) -> () loc(#loc9)
        %4 = call @_wrapped_jax_export_main(%0, %arg0, %arg1) : (tensor<i32>, tensor<?x1xi32>, tensor<?x4xi32>) -> tensor<?x4xi32> loc(#loc)
        return %4 : tensor<?x4xi32> loc(#loc)
      } loc(#loc)
      func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = "a"} loc(unknown), %arg1: tensor<?x1xi32> loc("x"), %arg2: tensor<?x4xi32> loc("y")) -> (tensor<?x4xi32> {jax.result_info = "result"}) {
        %c = stablehlo.constant dense<4> : tensor<1xi32> loc(#loc12)
        %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc12)
        %1 = stablehlo.concatenate %0, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc12)
        %2 = stablehlo.dynamic_broadcast_in_dim %arg1, %1, dims = [0, 1] : (tensor<?x1xi32>, tensor<2xi32>) -> tensor<?x4xi32> loc(#loc12)
        %3 = stablehlo.add %2, %arg2 : tensor<?x4xi32> loc(#loc12)
        return %3 : tensor<?x4xi32> loc(#loc)
      } loc(#loc)
    } loc(#loc)
    #loc1 = loc("<string>":13:6 to :46)
    #loc4 = loc("<string>":7:8 to :13)
    #loc5 = loc("<module>"(#loc1))
    #loc6 = loc("f1"(#loc4))
    #loc7 = loc("dimension_size"(#loc5))
    #loc8 = loc("ge"(#loc5))
    #loc9 = loc("shape_assertion"(#loc5))
    #loc10 = loc("eq"(#loc5))
    #loc11 = loc(callsite(#loc6 at #loc5))
    #loc12 = loc("jit(f1)/add"(#loc11))
    ```
    
    Before executing with PJRT one needs to refine shapes and remove the
    shape assertions, with eg:
    
    ```
    stablehlo-opt hello.mlir -stablehlo-refine-arguments="types='tensor<10x1xi32>, tensor<10x4xi32>'" -stablehlo-refine-shapes --stablehlo-check-shape-assertions
    ```
    
    Which gives us:
    
    ```
    module @jit_f1 attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
      func.func public @main(%arg0: tensor<10x1xi32>, %arg1: tensor<10x4xi32>) -> (tensor<10x4xi32> {jax.result_info = "result"}) {
        %c = stablehlo.constant dense<10> : tensor<i32>
        %c_0 = stablehlo.constant dense<true> : tensor<i1>
        %0 = call @_wrapped_jax_export_main(%arg0, %arg1) : (tensor<10x1xi32>, tensor<10x4xi32>) -> tensor<10x4xi32>
        return %0 : tensor<10x4xi32>
      }
      func.func private @_wrapped_jax_export_main(%arg0: tensor<10x1xi32>, %arg1: tensor<10x4xi32>) -> (tensor<10x4xi32> {jax.result_info = "result"}) {
        %c = stablehlo.constant dense<[10, 4]> : tensor<2xi32>
        %0 = stablehlo.dynamic_broadcast_in_dim %arg0, %c, dims = [0, 1] : (tensor<10x1xi32>, tensor<2xi32>) -> tensor<10x4xi32>
        %1 = stablehlo.add %0, %arg1 : tensor<10x4xi32>
        return %1 : tensor<10x4xi32>
      }
    }
    ```

[33mcommit 4c0d4841519aed22e3689c30b72a0e4228051249[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Oct 23 21:52:40 2025

    Bump patch version after integrate 1.13.2 -> 1.13.3 (#2868)

[33mcommit baaf7475f8925cb0c5f9580408b3c0385f888487[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Oct 22 18:49:03 2025

    Integrate LLVM at llvm/llvm-project@32de3b9ef9e7 (#2867)

[33mcommit 4227a87e42cdff3cfd76d0e4536f9a99d0c33d6e[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Sat Oct 18 16:14:41 2025

    Integrate LLVM at llvm/llvm-project@3a6b818132e3 (#2865)

[33mcommit 0649133763d77414ca8387807ed7b4c956dd20ed[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Oct 13 17:41:10 2025

    Typo fix on cholesky op, bool attr is not a 0-d tensor (#2864)
    
    Thanks to @joelberkeley for pointing this out on discord

[33mcommit 4d20289028b4ab70cf35074ea67bfbb9b6ba9591[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Oct 10 04:21:37 2025

    Optimize folding transposes that operate on splats (#2863)
    
    Instead of expanding and iterating over the splat to create the new
    constant, we now just replace the splat constant's dimensions as
    specified by the transpose op.
    
    When tested on the same input program that brought this issue to light,
    this fix improved the optimizer's execution time from 8.68 s to 0.80 s,
    a 985% speedup.

[33mcommit ef07ca906efde403eecd0d5f26ce7285cc8804b9[m
Author: vvish <vv.os.swe@gmail.com>
Date:   Tue Oct 7 19:12:16 2025

    Convert StableHLO reshape to TOSA reshape (#2861)
    
    [tosa.reshape](https://mlir.llvm.org/docs/Dialects/TOSA/#tosareshape-mlirtosareshapeop)
    accepts shape as a second argument.
    Conversion of the StableHLO reshape requires insertion of the
    [tosa.const_shape](https://mlir.llvm.org/docs/Dialects/TOSA/#tosaconst_shape-mlirtosaconstshapeop)
    op as an argument for the tosa.reshape.

[33mcommit e1d2f65804e072d8f4496f967cbf94983d14f279[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue Oct 7 18:18:08 2025

    Bump patch version after integrate 1.13.1 -> 1.13.2 (#2862)

[33mcommit 0a4440a5c8de45c4f9649bf3eb4913bf3f97da0d[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Sat Oct 4 00:05:40 2025

    Update CI toolchain; integrate LLVM at llvm/llvm-project@113f01aa82d0 (#2860)
    
    - Integrate LLVM at llvm/llvm-project@113f01aa82d0
    - Update `clang`, `clang++`, and `lld` from 14 to 20
    - Clang 14 doesn't recognize the `-Wno-deprecated-literal-operator`
    flag, which is now set by a dependency's build config.
    - Update system C++ headers from `g++-12` to `libstdc++-13-dev`
    - The older `g++-12` system headers used language features that are now
    recognized as deprecated in Clang 20.
    - Update `python` from 3.10 to 3.11
      - This now matches our minimum officially supported Python version.
    - Update `nanobind` from 2.4 to 2.9
    - This now matches the version used by LLVM (which also happens to be
    the latest stable version).

[33mcommit 500b49459b2bd2f822644d4d9a43109e537d51e4[m
Author: Mateusz Sok√≥≈Ç <8431159+mtsokol@users.noreply.github.com>
Date:   Tue Sep 23 18:36:56 2025

    Add SVG for `broadcast_in_dim` (#2858)
    
    A missing graphic for `broadcast_in_dim` operation.

[33mcommit be61781ab8e7dfe725419187f4e17372a1f99037[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Sep 5 17:37:22 2025

    Integrate LLVM at llvm/llvm-project@cb2f0d0a5f14 (#2857)

[33mcommit 6b92cceb100285aa02f60d6b8792cfbbddfe48b9[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Aug 22 20:01:28 2025

    Don't try to fold case ops with unsupported types (#2856)
    
    The case-op folder pattern currently supports only int and float types
    (and tensors thereof). We plan to extend this in future, but for now,
    fail to match if the op's return type is unsupported.

[33mcommit c00a02e92e07cb4e283a32e1c39c840bce3393f3[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Aug 22 00:22:19 2025

    Bump patch version after integrate 1.13.0 -> 1.13.1 (#2855)

[33mcommit 4e374691e4908e31220d69c8bc678deb18ff6438[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Aug 21 19:38:13 2025

    Add more thorough support for float folding. (#2853)
    
    Add float support to several folder patterns.

[33mcommit 20b8ee58da51d373ab73331f39236dadba0b942b[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed Aug 20 19:47:22 2025

    Fix Bazel targets in StableHLO builder README (#2852)

[33mcommit 54ed073f70daf51724676efc122d6c45dc8c2938[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Mon Aug 18 23:02:39 2025

    Integrate LLVM at llvm/llvm-project@fc44a4fcd3c5 (#2851)

[33mcommit f8c4137072460bc90df4d1e47f886bddb614ad9b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Aug 18 22:13:28 2025

    StableHLO Builder APIs (#2846)
    
    # Declarative MLIR Builder APIs
    
    Goal: Provide a builder that abstracts away the notion of location and
    insertion
    point for use cases that construct full graphs from C++.
    
    See `MlirBuilderTest.cpp` for examples.
    
    ## Usage
    
    The builders look fairly similar to XlaBuilder's declarative style, see
    `MlirBuilderTest.cpp` for a few example programs:
    
    ```c++
    StablehloModuleBuilder mb;
    {  // Build Main Func
      ScopedBuilderLocation loc(mb.get(), FileLineColLoc(mb.get(), "main.mlir"));
      func::FunctionBuilder fb(mb.get(), mb->getLoc(), "main");
      auto type4xi64 = RankedTensorType::get({4}, fb.getOpBuilder().getI64Type());
      auto arg0 = func::Argument(fb, type4xi64);
      auto cst = stablehlo::Constant(fb, 1);
      auto add = chlo::BroadcastAdd(arg0, cst);
      auto topkAndIndices = chlo::TopK(add, 2);
      auto broadcast =
          stablehlo::BroadcastInDim(topkAndIndices[0].getType(), cst, {});
      auto equal = tosa::Equal(topkAndIndices[0], broadcast);
      func::Return(fb, {equal});
    }
    
    mb->build()->dump();
    // module {
    //  func.func @main(%arg0: tensor<4xi64>) -> tensor<2xi1> {
    //    %c = stablehlo.constant dense<1> : tensor<i64>
    //    %0 = chlo.broadcast_add %arg0, %c : (tensor<4xi64>, tensor<i64>) -> tensor<4xi64>
    //    %values, %indices = chlo.top_k(%0, k = 2) : tensor<4xi64> -> (tensor<2xi64>, tensor<2xi32>)
    //    %1 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<2xi64>
    //    %2 = tosa.equal %values, %1 : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
    //    return %2 : tensor<2xi1>
    //  }
    // }
    ```

[33mcommit 937150fc464c618b26cf9659e0e1939a732132c3[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 15 23:43:11 2025

    Bump patch version after integrate 1.12.3 -> 1.12.4 (#2850)

[33mcommit 4b77ceebad1b25545b84f571e6a1a779c0244b13[m[33m ([m[1;32mllbmbump1022[m[33m)[m
Author: R0CKSTAR <yeahdongcn@gmail.com>
Date:   Fri Aug 15 17:42:01 2025

    README.md: fix build instructions (#2848)
    
    This PR resolves two issues in the build instructions and updates the
    test results.
    
    ```bash
    /bin/sh: 1: ccache: not found
    [11/156] Building CXX object stablehlo/transforms/conversions/CMakeFiles/obj.StablehloTypeConversion.dir/TypeConversion.cpp.o
    FAILED: stablehlo/transforms/conversions/CMakeFiles/obj.StablehloTypeConversion.dir/TypeConversion.cpp.o
    ccache /usr/bin/c++ -DGTEST_HAS_RTTI=0 -D_DEBUG -D_GLIBCXX_ASSERTIONS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__SHORT_FILE__=\"TypeConversion.cpp\" -I/root/stablehlo/llvm-project/llvm/include -I/root/stablehlo/llvm-build/include -I/root/stablehlo/llvm-project/mlir/include -I/root/stablehlo/llvm-build/tools/mlir/include -I/root/stablehlo -I/root/stablehlo/build -fPIC -fno-semantic-interposition -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -Wimplicit-fallthrough -Wno-nonnull -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wdelete-non-virtual-dtor -Wsuggest-override -Wno-comment -Wno-misleading-indentation -Wctad-maybe-unsupported -fdiagnostics-color -ffunction-sections -fdata-sections -O2 -g -DNDEBUG -std=gnu++17   -D_DEBUG -D_GLIBCXX_ASSERTIONS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS  -fno-exceptions -funwind-tables -fno-rtti -UNDEBUG -gsplit-dwarf -ggnu-pubnames -fsanitize=address -fsanitize=undefined -fsanitize=leak -fno-omit-frame-pointer -MD -MT stablehlo/transforms/conversions/CMakeFiles/obj.StablehloTypeConversion.dir/TypeConversion.cpp.o -MF stablehlo/transforms/conversions/CMakeFiles/obj.StablehloTypeConversion.dir/TypeConversion.cpp.o.d -o stablehlo/transforms/conversions/CMakeFiles/obj.StablehloTypeConversion.dir/TypeConversion.cpp.o -c /root/stablehlo/stablehlo/transforms/conversions/TypeConversion.cpp
    /bin/sh: 1: ccache: not found
    [20/156] Building StablehloOps.cpp.inc...
    ninja: build stopped: subcommand failed.
    ```
    
    ### Testing Done
    
    ```
    root@0-4-35-gpu-mi300x1-192gb-devcloud-atl1:~/stablehlo# (hash="$(cat ./build_tools/llvm_version.txt)"; cd llvm-project && git fetch origin "$hash" && git checkout "$hash")
    From https://github.com/llvm/llvm-project
     * branch                      6d4a0935c850ec3ddfc70c4ba97b98adc35c676e -> FETCH_HEAD
    HEAD is now at 6d4a0935c850 [lldb][test] Skip TestExprDefinitionInDylib on Windows
    ```
    
    Signed-off-by: Xiaodong Ye <yeahdongcn@gmail.com>

[33mcommit fb3d7b1cdbfdc071593bcb28bc32a5e8ad212276[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Aug 15 17:21:19 2025

    Fix a return-type bug in some folded case branches (#2849)
    
    When folding case ops with side effects, the prior logic could sometimes
    leave branches with mismatched return types. We now explicitly create
    constants matching the expected return types in order to ensure they
    always match.

[33mcommit 9e31595970950fe0bed423d01506bb746aa2d331[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 13 22:27:47 2025

    Use tf-nightly in jax notebook (#2847)
    
    `tensorflow-cpu` is far outside of the compat window, last release was
    in March. This leads to errors like:
    https://github.com/openxla/stablehlo/actions/runs/16919652788
    
    Instead lets use nightly since TF releases seem to be slowing down
    otherwise.

[33mcommit 9018c682b99eb20d5874a4e38271ce63d7393879[m[33m ([m[1;33mtag: [m[1;33mv1.12.3[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Aug 13 01:44:19 2025

    Integrate LLVM at llvm/llvm-project@6d4a0935c850 (#2841)

[33mcommit 6b41e3a3422947ebeae87b63c00baca3d175cfed[m
Author: Bixia Zheng <bixia@google.com>
Date:   Tue Aug 12 20:04:16 2025

    [RFC] Add Buffer Types. (#2839)
    
    This RFC proposes adding explicit buffer types to support XLA/HLO buffer
    types.
    
    ---------
    
    Co-authored-by: Matthias Guenther <mrguenther@google.com>

[33mcommit 1ea2acad33f0f4fbc74d214ef7ab5a4f84f59c82[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Mon Aug 11 20:57:52 2025

    Add a TODO pertaining to CaseOp folding (#2845)
    
    In a certain corner case, we don't simplify already-dead `case` ops as
    much as we could. This change adds a (currently disabled) test for that
    case as well as a TODO reminding us to address it. It's a fairly
    insignificant optimization oppurtunity, but it's probably worth
    addressing in future.

[33mcommit 660d1cc10b9dde88c6b9050a74e1d2cd8cc694f5[m
Author: Hugo Mano <hugo@hmano.com>
Date:   Fri Aug 8 21:45:01 2025

    integrations/c: fix and ensure C integration is actually compliant (#2838)
    
    This pull request introduces a new overload for the
    `serializePortableArtifact` function to simplify usage, updates related
    C API functions, and adds a compliance test for C compatibility. The
    changes primarily focus on improving the usability and robustness of the
    StableHLO unified C API.
    
    I also added a new C compliance test (`test_c_compliance.c`) to verify
    that StableHLO C API headers are compatible with C compilers. This
    ensures the API adheres to C standards.
    
    Close #2837

[33mcommit a423157c6720f56a8097543a820175f8062a86d5[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Aug 8 20:57:39 2025

    Fold `case` ops with constant branch indices (#2843)
    
    When a `case` op is given a constant branch-selection index, replace the
    `case` op with the contents of the always-active branch. If any branches
    are non-trivially dead (i.e. dead but not functionally pure), the `case`
    op will not be deleted but will always execute a now-empty branch (since
    the original contents of the active branch have been inlined).

[33mcommit 0327c2d5a9126ff31123834ed75f44bf9e44843d[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Aug 7 20:09:12 2025

    Refactor the WhileOp folder patterns a bit (#2842)

[33mcommit 99b0925e064b24c3623bf6fdb20de70732ffe81e[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue Aug 5 21:57:44 2025

    Bump patch version after integrate 1.12.2 -> 1.12.3 (#2840)

[33mcommit e07debd5e257ec1e118f18c54068977b89f03b2f[m[33m ([m[1;33mtag: [m[1;33mv1.12.2[m[33m)[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Jul 25 17:01:28 2025

    Integrate; skip LLVM bump due to upstream issue. (#2836)

[33mcommit 69d6dae46e1c7de36e6e6973654754f05353cba5[m
Author: penguin_wwy <940375606@qq.com>
Date:   Wed Jul 16 16:22:06 2025

    Integrate LLVM at llvm/llvm-project@f8cb7987c64d (#2830)

[33mcommit cb9d9113f0fa5afd03e1c0efd169e1347f86e24e[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue Jul 15 20:57:27 2025

    Set default op-folding element limit to 1. (#2832)
    
    This limit applies when constant-folding ops like `iota` whose outputs
    can be substantially larger than their inputs. This means that we won't
    fold `iota` ops of significant size by default.

[33mcommit 58fe179bfc40d4fae170d11849d5c4068acdceeb[m
Author: Samuel Marks <807580+SamuelMarks@users.noreply.github.com>
Date:   Fri Jul 11 13:25:21 2025

    [README.md] Fix quoting and use `--depth=1` for `git` (#2812)

[33mcommit 1ba6b72f8316ae0977189de020439bab8c676d9a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jul 11 13:22:52 2025

    Allow reduce-like ops to type infer without body region (#2822)
    
    We want to be able to use the `create<ReduceOp>(values,inits,dim)` API
    that relies on type inference, but type inference relies on a body
    existing.
    
    Today, most users expect the reduce body to have arguments similar to
    the input types, with the _option_ to use different precisions in the
    body. If using type inference APIs we should still allow this and you
    cannot use a body region with different types. If you use the API where
    you explicitly set output type (no reliance on type inference) then you
    can have body region with different input argument element types.

[33mcommit 6f7b4ab8f96dc65cf3c8e9824836117d2934cc45[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Jul 1 16:13:15 2025

    Bump patch version after integrate 1.12.1 -> 1.12.2 (#2827)

[33mcommit 9df3b5564c2101c251a73e04c82df76aacfcf471[m[33m ([m[1;33mtag: [m[1;33mv1.12.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Jun 30 19:36:32 2025

    Integrate LLVM at llvm/llvm-project@7a3356951053 (#2826)

[33mcommit 9d690899ff9dd6a283d525946d43c862eb5fc932[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Mon Jun 30 18:12:40 2025

    Bump patch version after integrate 1.12.0 -> 1.12.1 (#2825)

[33mcommit 955fa7e6e3b0a6411edc8ff6fcce1e644440acbd[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Jun 26 17:17:20 2025

    Integrate LLVM at llvm/llvm-project@af65cb68f553 (#2823)

[33mcommit 17092dc2a902f837e31fa84803fff0f06ecdcb87[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jun 24 23:32:50 2025

    Add source target pairs to send/recv ops (#2821)
    
    Implementation of RFC:
    
    https://github.com/openxla/stablehlo/blob/main/rfcs/20250421-source-target-pairs.md
    
    ---------
    
    Co-authored-by: Rosie Zou <rosiezou@google.com>

[33mcommit 172164449ff2ddc8e5f6c12c29bc96eb68f30139[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Jun 19 01:18:03 2025

    Bump patch version after integrate 1.11.1 -> 1.11.2 (#2819)

[33mcommit 644aa13a89c4adeda8d398cff7185fe58335b5cc[m[33m ([m[1;33mtag: [m[1;33mv1.12.0[m[33m, [m[1;33mtag: [m[1;33mv1.11.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 18 21:46:41 2025

    Bump patch version after integrate 1.11.0 -> 1.11.1 (#2816)

[33mcommit 8c4c3d6961f1c501720e4dbe7318bf621cd83803[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Jun 18 20:25:05 2025

    Integrate LLVM at llvm/llvm-project@2c440232e261 (#2818)

[33mcommit 7464da16a5cc55d48d293b68c5bcfff30e5839e9[m
Author: William Moses <gh@wsmoses.com>
Date:   Wed Jun 18 12:52:57 2025

    Don't segfault verifying while without valid terminator (#2817)
    
    verification of an invalid while op otherwise causes a segfault instead
    of throwing the proper error message

[33mcommit 20a9bc1604eca22ce46d1d2a827e1eba0b85d4dc[m[33m ([m[1;33mtag: [m[1;33mv1.11.0[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jun 13 21:55:54 2025

    Integrate LLVM at llvm/llvm-project@842377882a3f (#2815)

[33mcommit 9d0b3cc0a79ab058b78f8932bfc707c36fd315a2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 12 19:20:42 2025

    Bump minor version for mixed serializaiton support (#2814)
    
    Co-authored-by: Bart Chrzaszcz <bartchr@google.com>

[33mcommit 5fb3a5bc4234bfd014daf8e6d57f2214be7a0b78[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jun 2 18:25:03 2025

    Bump patch version after integrate 1.10.10 -> 1.10.11 (#2810)

[33mcommit b1e99f510647e9982b842b34e738ed178464e7c3[m
Author: Rosie Zou <rosiezou@users.noreply.github.com>
Date:   Mon Jun 2 16:11:42 2025

    [RFC] Add source_target_pairs attribute to send and recv ops in stableHLO (#2784)

[33mcommit ba4ab03e44cbdc58027cdc23c3e27170395b9290[m[33m ([m[1;33mtag: [m[1;33mv1.10.10[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 30 16:58:39 2025

    Integrate LLVM at llvm/llvm-project@2b8bff6f66fd (#2809)
    
    - Add `buildReduceBody` function
    https://github.com/openxla/xla/commit/1b392d5db50eb079a93e0a55a23ecf90b12fc22d
    - Add option for mixed serializaiton to python api
    https://github.com/openxla/xla/commit/e7137a383809a24875a95237b1d1f6485acdf710
    - Only verify replica IDs once
    https://github.com/openxla/xla/commit/5a8e2465492d9980d199078508c580742d57e58a

[33mcommit 9346d3d3a231d4e3ab2d2d6fc87ab0071917773f[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed May 28 19:24:25 2025

    Add a (theoretically redundant) null check. (#2808)
    
    The `CustomCallUnregisteredBackendConfigToFfi` pattern previously
    skipped a certain null check since the value was never supposed to be
    null, but checking it anyway seems prudent on second thought.

[33mcommit 772b0b67e39e62342274d9a771bc3a698f1e41a7[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed May 28 01:10:21 2025

    Canonicalize `mhlo.backend_config` to `backend_config` (#2807)
    
    When `mhlo.backend_config` is specified and `backend_config` is either
    unset or empty, move the contents of `mhlo.backend_config` into
    `backend_config`.

[33mcommit a85bcc1dd33d2dbc05670b914644971bf3e49671[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Sat May 17 00:12:13 2025

    Bump patch version after integrate 1.10.9 -> 1.10.10 (#2805)

[33mcommit b211ccc4868aafb345611a83fa195b8f5cf4307c[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri May 16 23:38:09 2025

    Mark non-pure StableHLO ops with both R/W effects. (#2804)
    
    Mark all StableHLO ops with any side effects as having both MemRead and
    MemWrite side effects to ensure we don't accidentally DCE them.
    
    A recent test showed that a `recv` op was getting removed while a `send`
    op wasn't. This change fixes it so that neither is removed.

[33mcommit 889764690ccccd3e10b5e9586a189d82dbf8eb25[m[33m ([m[1;33mtag: [m[1;33mv1.10.9[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu May 15 20:22:18 2025

    Integrate LLVM at llvm/llvm-project@741fef3a4453 (#2803)

[33mcommit 5837b2a6ce192622d31625a41af67519607417a6[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue May 13 22:06:08 2025

    Fix typo in `InfeedResource::getName()`. (#2802)
    
    `::mlir::stablehlo::side_effects::InfeedResource::getName()` previously
    returned "OutfeedResource" instead of "InfeedResource"; this fixes it.

[33mcommit ba556d759005634bdabade24ebacdc17554a8464[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Tue May 13 20:44:01 2025

    Optionally DCE loops more liberally (opt-in). (#2801)
    
    Firstly, this fixes a bug where we assumed a missing `has_side_effect`
    attribute meant that a `custom_call` had every side effect even though
    the attribute was documented as semantically `false` if unset.
    
    Secondly, added an option (disabled by default) to assume, for purposes
    of eliminating dead `while` ops, that ops are pure unless they say
    otherwise via the MLIR `MemoryEffects` interface. The default behavior
    is still to play it safe and assume ops have side effects if they don't
    specify one way or the other, but enabling the new option can result in
    significantly smaller IR. (Note that MHLO optimizations already make
    this assumption when removing dead `while` ops, which suggests that it's
    typically safe to enable the option.)

[33mcommit ddc7cf9ec9c1ace8ca2755174e0d93a66c34cc5c[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu May 8 21:02:27 2025

    Bump patch version after integrate 1.10.8 -> 1.10.9 (#2799)

[33mcommit d04fac09573114edad22c7d7af27ae0030c7a04e[m
Author: penguin_wwy <940375606@qq.com>
Date:   Thu May 8 04:07:05 2025

    Remove unused LINK_LIBS in cmake (#2798)
    
    With `STABLEHLO_BUILD_EMBEDDED=ON`, this unused LINK_LIBS will cause a
    cmake issue
    ```
       CMake Error at /home/runner/_work/torch-mlir/torch-mlir/externals/llvm-project/mlir/cmake/modules/AddMLIR.cmake:271 (message):
        StablehloTypeConversion specifies LINK_LIBS LLVMSupport, but LINK_LIBS
        cannot be used for LLVM libraries.  Please use LINK_COMPONENTS instead.
      Call Stack (most recent call first):
        /home/runner/_work/torch-mlir/torch-mlir/externals/llvm-project/mlir/cmake/modules/AddMLIR.cmake:383 (_check_llvm_components_usage)
        /home/runner/_work/torch-mlir/torch-mlir/externals/stablehlo/stablehlo/transforms/conversions/CMakeLists.txt:16 (add_mlir_library)
    ```

[33mcommit 630c315b1d2821dd1181137315eda93875096216[m[33m ([m[1;33mtag: [m[1;33mv1.10.8[m[33m)[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed May 7 18:18:32 2025

    Integrate LLVM at llvm/llvm-project@2d287f51eff2 (#2797)

[33mcommit f099516568f0bb08c9343d1acb870100664e1cef[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue May 6 20:19:54 2025

    Bump patch version after integrate 1.10.7 -> 1.10.8 (#2796)

[33mcommit 89affc548432ef7da01c61bf2ebfd337d4f6b4f2[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Mon May 5 19:46:54 2025

    Add patterns for folding Reduce ops. (#2795)
    
    Add four missing patterns targeting Reduce ops:
    - FoldReduceOpReducingZeroDims
    - FoldReduceOpToConstantInitializer
    - FoldReduceOpWithRedundantResults
    - LowerBoolSplatConstantsIntoReduceOpRegion
    
    Issue: #2736

[33mcommit 26eda51e71f1ba71270643ae6fee5d92d1c61274[m
Author: Tapasvi Patel <133996364+tapspatel@users.noreply.github.com>
Date:   Mon May 5 16:53:40 2025

    Separated stablehlo pass utils into separate dialect library to fix cmake build for stablehlo optimization passes library (#2794)
    
    When attempting to link against `StablehloOptimizationPasses`, it throws
    a linker error for functionality found in `PassUtils.cpp`.
    
    Error
    ```
    /usr/bin/ld: lib/libStablehloOptimizationPasses.a(StablehloAggressiveSimplification.cpp.o): in function `mlir::Value mlir::stablehlo::getConstantLike<int>(mlir::OpBuilder&, mlir::Location, int, mlir::Value)':
    StablehloAggressiveSimplification.cpp:(.text._ZN4mlir9stablehlo15getConstantLikeIiEENS_5ValueERNS_9OpBuilderENS_8LocationET_S2_[_ZN4mlir9stablehlo15getConstantLikeIiEENS_5ValueERNS_9OpBuilderENS_8LocationET_S2_]+0x5c): undefined reference to `mlir::stablehlo::getConstantLikeImpl(mlir::OpBuilder&, mlir::Location, mlir::Attribute, mlir::Value)'
    clang++-17: error: linker command failed with exit code 1 (use -v to see invocation)
    ```
    
    The bazel build creates a separate pass utils library
    
    ```
    cc_library(
        name = "stablehlo_pass_utils",
        srcs = [
            "stablehlo/transforms/PassUtils.cpp",
        ],
        hdrs = [
            "stablehlo/transforms/PassUtils.h",
        ],
        strip_include_prefix = ".",
        deps = [
            ":base",
            ":chlo_ops",
            ":stablehlo_ops",
            ":stablehlo_pass_inc_gen",
            "@llvm-project//llvm:Support",
            "@llvm-project//mlir:ComplexDialect",
            "@llvm-project//mlir:IR",
            "@llvm-project//mlir:Support",
        ],
    )
    ```
    
    This change mimics the same behavior but for cmake based build to fix
    the build error.

[33mcommit b58c38bfb67245ccfe8d2d644ee1dbc615e5a900[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri May 2 02:56:37 2025

    Add a pattern for folding `stablehlo.while` no-ops (#2793)
    
    If a `stablehlo.while` op's condition is a constant with a value of
    `false`, replace the op with its input.
    
    Make an exception for preserving dead ops with side effects, which can't
    currently be DCE'd safely despite being unreachable. This is because
    dependent code is currently allowed to depend on the existence of ops
    with hypothetical side effects that show up in the unoptimized IR but
    can't actually occur.
    
    Issue: #2736

[33mcommit 3536df3cc59af54e40fcdc21587a977e753d946e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 2 02:56:08 2025

    Remove mentions of deprecated MLIR functions from docs (#2791)

[33mcommit f00a9756776646753b1bb83057ad730ee2496b60[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 2 02:47:25 2025

    Disallow empty globs in bazel (#2792)
    
    Issue was discovered and fixed in
    https://github.com/openxla/stablehlo/pull/2743, but lets enforce this to
    avoid breaking downstream users going forward.
    
    This bazel flag will be flipped to default in bazel8 so might as well
    get ahead of things too.

[33mcommit 5a55c570679e102ea25a59a9d2037f8f42e0c5e4[m
Author: Sam Kellett <samkellett@gmail.com>
Date:   Thu May 1 15:18:40 2025

    Changes to allow StableHLO to be added as a Bzlmod dep (#2743)
    
    To add StableHLO as a dependency in the WORKSPACE (due to it depending
    on LLVM, which doesn't support Bzlmod yet) but in a Bazel project that
    uses Bzlmod then a patch is required to get it to work.
    
    The WORKSPACE would have something like this in it:
    ```
    git_repository(
        name = "stablehlo",
        commit = "48a1e14edc8219577fcad53de1924876f855f431",
        patch_args = ["-p1"],
        patches = [
            "//patches:stablehlo/00-bazel-dep.patch",
        ],
        remote = "https://github.com/openxla/stablehlo.git",
    )
    ```
    
    This PR is the contents of that patch file.

[33mcommit bf48a51c6d69e12d27f74ae6aee2f21d8bccad28[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu May 1 15:04:20 2025

    Fix an outdated pass-creation function declaration (#2790)
    
    The declaration for a pass-creation function was located in the wrong
    header file, resulting in a linker error if someone tried to use it.
    This change moves the declaration to the correct header and updates its
    function signature per PR #2775.

[33mcommit 63246aea4a0310274d1eaebe1ec150a63e935a28[m[33m ([m[1;33mtag: [m[1;33mv1.10.7[m[33m)[m
Author: penguin_wwy <940375606@qq.com>
Date:   Tue Apr 29 21:38:56 2025

    Integrate LLVM at llvm/llvm-project@c60f24dca96d (#2788)
    
    fix #2786

[33mcommit 56aa7870abf85bd161061edf758f91c812e01d59[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Apr 28 18:17:53 2025

    Bump patch version after integrate 1.10.6 -> 1.10.7 (#2785)

[33mcommit 6a55cc6df7b054561cd7c614e32f4b3aaf3e2611[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Apr 25 22:32:43 2025

    Add a pattern for folding no-op Pad ops. (#2789)
    
    Issue: #2736

[33mcommit 6d4f0f85966c9b3054577532f53800c37b9a7904[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Wed Apr 23 19:17:07 2025

    Refactor StableHLO passes to support options; add option for op-folding element limit. (#2775)
    
    Instead of hard-coding the max number of elements in a folded operation,
    set the limit via a config option. (The default value matches the old
    constant's value of 65536.)

[33mcommit a54938f0651d3b4b7be9771848eda2463c92a8e7[m[33m ([m[1;33mtag: [m[1;33mv1.10.6[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Apr 23 02:26:53 2025

    Integrate LLVM at llvm/llvm-project@0078cf79adc2 (#2783)

[33mcommit 890c87ff0ccec7f31d70f8debd90a51e7cda1df1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Apr 23 00:15:00 2025

    Bump patch version after integrate 1.10.5 -> 1.10.6 (#2782)

[33mcommit f1f035fea33dcfdd7c471eb7f39174b344003117[m[33m ([m[1;33mtag: [m[1;33mv1.10.5[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Apr 21 17:43:41 2025

    Integrate LLVM at llvm/llvm-project@cbbf562d1c2a (#2781)
    
    Also integrating https://github.com/openxla/stablehlo/pull/2777 fixes
    
    ---------
    
    Co-authored-by: Tai Ly <Tai.Ly@arm.com>

[33mcommit 848b4a1a6033022d09646bb2144ee3f91552cec4[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Apr 17 18:20:04 2025

    Give the StableHLO Aggressive Simp. patterns names (#2780)
    
    This improves stack traces as well as general code readability.

[33mcommit e3ae120a9397241e517da985b4072f0c646b6183[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Apr 11 18:37:19 2025

    Bump patch version after integrate 1.10.4 -> 1.10.5 (#2776)

[33mcommit 515c8efe1383ab4e25da6d7191caa970c115e40d[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Apr 11 17:15:23 2025

    Update input type in ODS documentation comment (#2768)
    
    Following up on PR #1872, this updates the ODS documentation comment to
    show the new input type of `array<i64: foo>` instead of the old
    `dense<foo> : tensor<Nxi64>`.

[33mcommit 40c67618e4b52a15e50fe4fce4027508d1b0ef78[m
Author: William Moses <gh@wsmoses.com>
Date:   Fri Apr 11 17:15:00 2025

    Add terminator check during type validation (#2769)
    
    If this check isn't there, printing stablehlo of an op under
    construction (and thus without a terminator) will unnecessarily
    segfault/crash

[33mcommit ac530ab7e96f81d26a779d57777b1d3bc08c2e1d[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Apr 11 17:14:31 2025

    Fix- PR "cmake-build" Check stuck at "pending"  Status Issue (#2774)
    
    [Draft]
    I don't know why this fix is working!
    
    workflow name: `CMake Build`
    JobID: `cmake-build`
    
    Issue: As soon as Checks start running for a PR, `cmake-build` check is
    going to a pending status. Never changed to `in-progress` or
    `completed`. Note - There is a already a check with `CMake Build /
    cmake-build (pull_request)` which runs alright.
    
    In Branch protection setting of the repo, `cmake-build` is marked as
    `required`. If we mark it as not `required`, this extra `cmake-build`
    disappears.
    
    Why this fix: updating....

[33mcommit 8d9a84b5efbd1fe57cfcb84c6fa38f751bdbabe8[m[33m ([m[1;33mtag: [m[1;33mv1.10.4[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Apr 9 18:00:20 2025

    Integrate LLVM at llvm/llvm-project@69f59d59cb02 (#2772)

[33mcommit 81d2ca2d5893f29cbcf7db1ffe934474d65e4571[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Mon Apr 7 21:37:14 2025

    Bump patch version after integrate 1.10.3 -> 1.10.4 (#2766)

[33mcommit 4bf77d23bd9150782a70d85fda9c12a2dec5328c[m[33m ([m[1;33mtag: [m[1;33mv1.10.3[m[33m)[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Thu Apr 3 17:09:59 2025

    Integrate LLVM at llvm/llvm-project@799e9053641a (#2765)

[33mcommit a3c7de92425e8035437dae67ab2318a82eca79a1[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Mar 31 20:11:01 2025

    Bump patch version after integrate 1.10.2 -> 1.10.3 (#2763)

[33mcommit ce884776b36cd72c361a610501b0cf7eeb121370[m
Author: Google Admin <github-admin@google.com>
Date:   Thu Mar 27 14:21:29 2025

    Update runners to ubuntu-24.04 from deprecated ubuntu-20.04 label (#2761)
    
    This is a LSC run by http://go/ghss to upgrade all depreacated
    ubuntu-20.04 runners to ubuntu-24.04.
    
    On April 1, 2025, GitHub will stop supporting ubuntu-20.04 runners and
    workflows configured with this label will cease to run. This PR is an
    attempt to save you work by doing the upgrade for you.
    
    WARNING: We do not know if the updated label is compatiable with your
    workflow or not.
    
    If you do not want to merge this PR, feel free to close it and deal with
    the problem yourselves.
    
    More context and feedback: http://b/406537467
    
    Co-authored-by: Bill Napier <napier@google.com>

[33mcommit be8ce602efbd90fd677247075745bf16eb4b31ac[m[33m ([m[1;33mtag: [m[1;33mv1.10.2[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Mar 27 07:26:15 2025

    Integrate LLVM at llvm/llvm-project@ac9049df7e62 (#2762)

[33mcommit 20ada3012911f882629aa3cbae0b3ee4eac6fd23[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Mar 25 16:48:20 2025

    Bump patch version after integrate 1.10.1 -> 1.10.2 (#2759)

[33mcommit 0f7a522630658cfd825951a9551ad568bfda032f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 25 15:59:30 2025

    Add more MHLO folders to StableHLO (#2753)
    
    Some of these may seem unsafe (simplifying float `X*1.0` or `X*0.0` may
    not account for NaNs properly), but both MHLO and XLA simplifications do
    this optim so probably generally safe.

[33mcommit 8993ece024e330cc61c57ab4cc12ac7bdd2b5ce5[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 25 15:39:49 2025

    Use isa for dialect matching instead of string comparison (#2757)

[33mcommit af8ba04d6682fada77595c69ce5bd36611a919e8[m[33m ([m[1;33mtag: [m[1;33mv1.10.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 24 21:48:52 2025

    Integrate LLVM at llvm/llvm-project@dd3addf954ac (#2758)
    
    fixes https://github.com/openxla/stablehlo/issues/2751

[33mcommit 88550288a7e86e3629a842384fbc34a0ee1ecc49[m
Author: Christopher Bate <cbate@nvidia.com>
Date:   Mon Mar 24 15:40:57 2025

    Fix crash on ComplexType in PointwiseToLinalgMapConverter (#2754)
    
    A recent change added code to greedily materialize splat constants, but
    the code would crash when used with `complex<..>` types.

[33mcommit 9b65803012e5a5642a4334daf7a0c7fa3384de46[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 24 15:38:54 2025

    Bump patch version after integrate 1.10.0 -> 1.10.1 (#2755)

[33mcommit 44e2d2fb1398144bc35ea860d40ef9e87390f259[m
Author: Tai Ly <tai.ly@arm.com>
Date:   Mon Mar 24 15:38:40 2025

    [stablehlo][tosa] Fix for tosa dialect updates (#2756)
    
    This fixes up legalization passes for converting to/from tosa for tosa
    dialect changes
    
    - tosa rescale has multiplier, shift, input_zp, output_zp changed from
    attribute to operands with specific type requirements
    - tosa rescale attribute "double_round" changed to "rounding_mode"
    - tosa negate has additional input_zp and output_zp operands
    - tosa const attribute name changed from "value" to "values"
    
    Signed-off-by: Tai Ly <tai.ly@arm.com>

[33mcommit 5bf0fef081a51d5ea8395a6f831c088aa5cea018[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Mar 20 18:56:37 2025

    Extend legalize-quant-to-math pass to support composite op (#2723)
    
    **Note to the reviewers:**
    This PR is based on https://github.com/openxla/stablehlo/pull/2722. The
    changes related to this PR is localized in
    `stablehlo/transforms/StablehloLegalizeQuantToMath.cpp` and
    `stablehlo/tests/transforms/stablehlo_legalize_quant_to_int.mlir` files
    only.
    
    ## Summary
    
    The `quant-to-math` legalization of composite op can be realized as:
    
    1. Apply legalization to its decomposition.
    1. Convert the quantized signature of the composite op to the integer
    signature.
    
    Note that both 1 and 2 are achieved __almost for free__ by the existing
    patterns.
    * (1) By virtue of the fact that the existing pass applies to every func
    in module
    * (2) As part of
    [ConvertGenericOp](https://github.com/openxla/stablehlo/blob/03597b1e592129f0c79e99e5ed65dac7ebee240f/stablehlo/transforms/StablehloLegalizeQuantToMath.cpp#L1310)
    conversion pattern.
    
    Together with https://github.com/openxla/stablehlo/pull/2722, we can do
    something like
    
    ### Step 1
    ```
    $ cat input.mlir
    func.func @decompose_composite_op(%arg0 : tensor<2xf32>, %arg1 : tensor<2xf32>) -> tensor<2xf32> {
      %0 = stablehlo.uniform_quantize %arg0 : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 0.1:2>>
      %1 = stablehlo.uniform_quantize %arg1 : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 0.1:2>>
      %2 = stablehlo.add %0, %1 : tensor<2x!quant.uniform<i8:f32, 0.1:2>>
      %3 = stablehlo.uniform_dequantize %2 : (tensor<2x!quant.uniform<i8:f32, 0.1:2>>) -> tensor<2xf32>
      return %3 : tensor<2xf32>
    }
    ```
    
    ### Step 2: Apply
    https://github.com/openxla/stablehlo/blob/3a0cd9d12166d8426777206339b8562be64c55bc/stablehlo/transforms/Passes.td#L413
    pass
    
    As part of applying the pass, we are providing the attribute names for
    quantize/dequantize composites. With that we may get something like
    
    ```mlir
    func.func @decompose_composite_ops(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {
        %0 = stablehlo.composite "stablehlo.uniform_quantize" %arg0 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128 : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_quantize.impl_0} : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        %1 = stablehlo.composite "stablehlo.uniform_quantize" %arg1 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128 : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_quantize.impl} : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        %2 = stablehlo.composite "stablehlo.add" %0, %1 {decomposition = @stablehlo.add.impl} : (tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>, tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        %3 = stablehlo.composite "stablehlo.uniform_dequantize" %2 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128 : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_dequantize.impl} : (tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>) -> tensor<2xf32>
        return %3 : tensor<2xf32>
      }
      func.func private @stablehlo.uniform_dequantize.impl(%arg0: tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>) -> tensor<2xf32> {
        %0 = stablehlo.uniform_dequantize %arg0 : (tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>) -> tensor<2xf32>
        return %0 : tensor<2xf32>
      }
      func.func private @stablehlo.add.impl(%arg0: tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>, %arg1: tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>> {
        %0 = stablehlo.add %arg0, %arg1 : tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        return %0 : tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
      }
      func.func private @stablehlo.uniform_quantize.impl(%arg0: tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>> {
        %0 = stablehlo.uniform_quantize %arg0 : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        return %0 : tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
      }
      func.func private @stablehlo.uniform_quantize.impl_0(%arg0: tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>> {
        %0 = stablehlo.uniform_quantize %arg0 : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
        return %0 : tensor<2x!quant.uniform<i8:f32, 1.000000e-01>>
      }
    ```
    
    ### Step 3: Apply `stablehlo-legalize-quant-to-int` pass
    
    ```mlir
    func.func @decompose_composite_ops(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {
        %0 = stablehlo.composite "stablehlo.uniform_quantize" %arg0 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128
     : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_quantize.impl_0} : (tensor<2xf32>) -> tensor<2xi8>
        %1 = stablehlo.composite "stablehlo.uniform_quantize" %arg1 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128
     : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_quantize.impl} : (tensor<2xf32>) -> tensor<2xi8>
        %2 = stablehlo.composite "stablehlo.add" %0, %1 {decomposition = @stablehlo.add.impl} : (tensor<2xi8>, tensor<2xi8>) -> tensor<2xi8>
        %3 = stablehlo.composite "stablehlo.uniform_dequantize" %2 {composite_attributes = {expressed_type = f32, scale = 1.000000e-01 : f64, storage_type = i8, storage_type_max = 127 : i64, storage_type_min = -128
    : i64, zero_point = 0 : i64}, decomposition = @stablehlo.uniform_dequantize.impl} : (tensor<2xi8>) -> tensor<2xf32>
        return %3 : tensor<2xf32>
      }
      func.func private @stablehlo.uniform_dequantize.impl(%arg0: tensor<2xi8>) -> tensor<2xf32> {
        // ... decomposition of stablehlo.uniform_dequantize
      }
      func.func private @stablehlo.add.impl(%arg0: tensor<2xi8>, %arg1: tensor<2xi8>) -> tensor<2xi8> {
        // decomposition of quantized stablehlo.add
      }
      func.func private @stablehlo.uniform_quantize.impl(%arg0: tensor<2xf32>) -> tensor<2xi8> {
        // ... decomposition of stablehlo.uniform_quantize
      }
    
      func.func private @stablehlo.uniform_quantize.impl_0(%arg0: tensor<2xf32>) -> tensor<2xi8> {
        // ... decomposition of stablehlo.uniform_quantize
      }
    ```
    
    
    cc @mahmoud-abuzaina

[33mcommit 66f90d5c04d4a16122e9caaedd68a294eb781444[m[33m ([m[1;33mtag: [m[1;33mv1.10.0[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Mar 20 17:06:42 2025

    Integrate LLVM at llvm/llvm-project@0230d63b4a8b (#2752)
    
    Other than the usual integration, this PR also addresses the followings:
    
    1. Disables certain build and tests for
    https://github.com/openxla/stablehlo/issues/2751.
    
    In stablehlo/conversions/tosa/transforms/CMakeLists.txt, just commenting
    the problematic files is not enough because
    
    ```
     LLVM's build system enforces that all source files are added to a build target
    ```
    
    Hence we added add `PARTIAL_SOURCES_INTENDED` to the target
    specification, though it is discouraged.
    
    1. Fixes `build_tools/integrate/llvm_bump_revision.sh` to account for
    unset value of `$1` as enforced by `set -o nounset`

[33mcommit 3a0cd9d12166d8426777206339b8562be64c55bc[m[33m ([m[1;32mfresh[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Mar 19 21:00:26 2025

    Pass to wrap StableHLO ops in composite (#2722)
    
    Wraps StableHLO operations in `stablehlo.composite` operations.
    
        For instance, consider a simple StableHLO program:
    
        ```mlir
    func.func @main(%arg0 : tensor<2xf32>, %arg1 : tensor<2xf32>) ->
    tensor<2xf32> {
            %0 = stablehlo.add %arg0, %arg1 : tensor<2xf32>
            return %0 : tensor<2xf32>
          }
        ```
    
    Applying this pass to wrap `stablehlo.add` operations will result in the
        following program:
    
        ```mlir
    func.func @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) ->
    tensor<2xf32> {
    %0 = stablehlo.composite "stablehlo.add" %arg0, %arg1 {decomposition =
    @stablehlo.add.impl} : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xf32>
            return %0 : tensor<2xf32>
          }
    func.func private @stablehlo.add.impl(%arg0: tensor<2xf32>, %arg1:
    tensor<2xf32>) -> tensor<2xf32> {
            %0 = stablehlo.add %arg0, %arg1 : tensor<2xf32>
            return %0 : tensor<2xf32>
          }
        ```
    
        Notes:
    
    - The `name` attribute of the generated `stablehlo.composite` operation
    will always be the same as the name of the original operation that was
    wrapped (e.g., if you wrap a `stablehlo.add` operation, the composite
            will also be named `"stablehlo.add"`).
          - The private function that encapsulates the original operation
            (referenced by the `decomposition` attribute of the
            `stablehlo.composite` operation) will be named using the pattern
    `<op_name>.impl[.N]`, where `<op_name>` is the name of the original
    operation, and `N` is a unique integer identifier generated to prevent
            naming conflicts within the module.
    
        This pass can be used in three distinct ways:
    
        **Mode 1: Command-line Usage**
    
    This mode is the simplest, using the `stablehlo-opt` utility with the
    `op-names` (a comma-separated list of operation names) and `version` (an
    integer version number) options. It wraps **all instances** of specified
    operations. The attributes of the newly created `stablehlo.composite`
    operation will be the same as the attributes of the original operation.
    
        **Usage Example:**
    
        ```bash
    stablehlo-opt input.mlir
    --stablehlo-wrap-in-composite=op-names='stablehlo.add,stablehlo.mul' -o
    output.mlir
        ```
    
        **Mode 2: Programmatic Single-Op Wrapping**
    
        This mode provides programmatic control to wrap
        **a specific operation instance** and returns a pointer to the newly
        created `stablehlo.composite` operation.
    
        **Example (C++):**
    
        ```cpp
          // To wrap a specific stablehlo.add instance
    
    mlir::stablehlo::AddOp addOp = ...; // The op instanced to be wrapped.
          mlir::ModuleOp module = addOp->getParentOfType<mlir::ModuleOp>();
          mlir::OpBuilder builder(addOp);
    mlir::NamedAttrList attrs = ...; // Attributes to be set on the
    composite op.
          int32_t version = 0; // Composite version.
    
    mlir::stablehlo::CompositeOp compositeOp =
    mlir::stablehlo::wrapOperationInComposite(builder, addOp, attrs,
    version, module);
          addOp.replaceAllUsesWith(compositeOp);
        ```
    
    **Mode 3: Programmatic Module-Wide Wrapping with Attribute Predicates**
    
    This mode extends programmatic wrapping to the entire module, offering
    fine-grained control over which operations are wrapped and their
    attributes.
    This is achieved by using the `createStablehloWrapInCompositePass` API,
        which takes an `AttributePredicateMap` as an argument.
    
    The `AttributePredicateMap` is a map that dictates which operations
    should
    be considered for wrapping and how their attributes should be handled.
    Its
        semantics are as follows:
    
    - **Keys (mlir::TypeID):** `TypeID` of an MLIR operation. If an
    operation's
    `TypeID` matches a key in the map, it becomes a candidate for wrapping.
        - **Values (Lambda Functions):** Lambda function of type
    `std::function<std::optional<NamedAttrList>(Operation*)>`. This function
            is applied to each candidate operation.
            - **Input:** An `mlir::Operation*`, which is an instance of the
              operation type corresponding to the `TypeID` key.
            - **Return Value:** An `std::optional<NamedAttrList>`.
              - If the lambda returns a `NamedAttrList` (wrapped in
                `std::optional`), the operation is wrapped in a
    `stablehlo::composite` operation, and the returned attributes are
                used to set the composite's attributes.
    - If the lambda returns `std::nullopt`, the operation is **not**
                wrapped. This allows for selective wrapping based on custom
                criteria.
    
        **Example (C++):**
    
        ```cpp
    
        // ... inside a pass or function ...
    
        stablehlo::AttributePredicateMap attributePredicateMap;
    
        attributePredicateMap[mlir::TypeID::get<mlir::stablehlo::AddOp>()] =
          [](mlir::Operation* op) -> std::optional<mlir::NamedAttrList> {
          // Custom logic to determine if and how to wrap the operation.
          // Example: Only wrap if it's on a specific type.
          if (op->getOperand(0).getType().isa<mlir::Float32Type>()) {
            return mlir::NamedAttrList(op->getAttrs());
          }
          return std::nullopt; // Do not wrap.
        };
    
    pm.addPass(createStablehloWrapInCompositePass(attributePredicateMap,
    compositeVersion));
        if (mlir::failed(pm.run(module))) {
          return;
        }
        ```

[33mcommit 350021b4e3d5b01b2a3e9322deacba45ee207dd4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Mar 19 19:33:04 2025

    Fix Scatter Spec  Diagram (#2746)
    
    fixes https://github.com/openxla/stablehlo/issues/2744
    
    The change is the label of the 4th column of the table from
    
    ```
    Full start index:
    scatter start index via scatter_dims_to_operand_dims = [0, 1]
    ```
    
    to
    
    ```
    Full start index:
    scatter start index via scatter_dims_to_operand_dims = [2, 1]
    ```

[33mcommit 64d8481c1c9a62f4680fc9ba1da4cc625de1c5ef[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 19 18:15:47 2025

    Add remaining unary result accuracies (#2749)
    
    Remaining implementation of
    https://github.com/openxla/stablehlo/blob/main/rfcs/20241015-result-accuracy.md
    
    Co-authored-by: Rachel Han <hanrach@google.com>

[33mcommit 80929da628161be180d759d84b34a526ed39516c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Mar 17 18:47:22 2025

    Preserve unregistered attrs in while op simplification (#2748)
    
    StableHLO port of MHLO change
    https://github.com/openxla/xla/commit/cc4fbd8dd81274a9bbed8c34309944c30f5678f4

[33mcommit cc46e08f6b07efbffb7ee8a493725db86741279e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Mar 17 16:59:01 2025

    Integrate LLVM at llvm/llvm-project@f01e760c0836 (#2747)
    
    Temporarily rolling back VHLO error case while users integrate the
    change.

[33mcommit c4e2e6a91ac31b5e4d54115c2d6398034348c189[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Mar 17 15:33:11 2025

    Add CHLO CAPI and PythonAPI for ragged dot (#2737)
    
    Port of
    https://github.com/openxla/xla/commit/aa913a801a7e627645be4d0a0f6fdf36f4c1cfc5
    
    ---------
    
    Co-authored-by: Praveen Narayanan <pravnar@google.com>

[33mcommit abe25d74fe35e0d11ca1e50547164edf5f721a54[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Mar 14 17:40:06 2025

    Fix a casting bug in `DynamicIotaOpToBroadcast`. (#2745)
    
    The transformation sometimes generated an invalid `index_cast` op
    between two non-index int types.
    
    Issue #2727

[33mcommit ec66eef1ef763f3d67f2b6dfe7c08056140fe195[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Mar 10 22:23:06 2025

    Bump patch version after integrate 1.9.7 -> 1.9.8 (#2741)

[33mcommit ee05f9f4ad24bb7e82d4c791315a1d3b5703b827[m[33m ([m[1;32mfresh_workspace[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Mar 10 18:38:34 2025

    Minor spell-corrections from the integrate doc (#2740)

[33mcommit e34c3f6e4148a2e7a0e818465dd796d65ae92305[m[33m ([m[1;33mtag: [m[1;33mv1.9.7[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Sat Mar 8 00:52:46 2025

    Integrate LLVM at llvm/llvm-project@e739ce2e10e6 (#2739)

[33mcommit 7f49b5d9b72f48486bc8ef19671af16e5eb92ffa[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Mar 7 20:24:22 2025

    Add CHLO generated documentation (#2738)

[33mcommit 030f0f7dfd3edab5f0b4f496cc420701b9318ef6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Mar 6 23:05:34 2025

    Bump patch version after integrate 1.9.6 -> 1.9.7 (#2734)

[33mcommit 7b7d6ad438bfcb2f15b8c339c12bb4a7845467f1[m[33m ([m[1;33mtag: [m[1;33mv1.9.6[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 4 22:48:27 2025

    Fix location versioning (#2732)
    
    Two bugs:
    - Serialization floors version to `X.Y.0` so we can't use `1.8.4` as a
    version. Makes sense, incompats should be confined to major version
    bumps
    - Fix bug in the logic for valid location checking, was checking the
    parent location not the active location. Update test file to also test
    serialization behavior.

[33mcommit 1251035a149a7c86da3dbfd98716fb778b37d6d9[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 4 21:01:21 2025

    Bazel build only build affected targets on pull_request (#2729)
    
    - It's painful that doc-only changes compile the entire codebase and run
    all tests.
    - We can't use `paths-ignore` from github actions because it marks jobs
    as pending and we have branch protections enabled which require all
    status checks to be green.
    - This seems like a generally good change to build what you change,
    after merge to main we will still build and test all.
    
    Calculates diff in GH actions using https://github.com/Tinder/bazel-diff
    
    ## Example 1, this PR, GH actions files only, no impacted build files:
    
    ```sh
    $ curl -Lo /tmp/bazel-diff.jar https://github.com/Tinder/bazel-diff/releases/latest/download/bazel-diff_deploy.jar
    $ ./build_tools/github_actions/ci_build_bazel.sh /tmp/bazel-diff.jar $(git merge-base main HEAD) $(git rev-parse HEAD)
    Generating Hashes for Revision 'd2708ad6cca730d4c17317388a7acbf8eac2c9f6'
    Loading: 0 packages loaded
    
    Generating Hashes for Revision '1565f5bac3ed5df5e0bdd38843fb84efde28e1d1'
    Loading: 0 packages loaded
    
    Determining Impacted Targets
    
    No impacted targets from change.
    ```
    
    ## Example 2, modify `StablehloOps.cpp`, fairly core file, should test
    ~all:
    
    ```sh
    $ ./build_tools/github_actions/ci_build_bazel.sh /tmp/bazel-diff.jar $(git merge-base main HEAD) $(git rev-parse HEAD)
    Generating Hashes for Revision 'd2708ad6cca730d4c17317388a7acbf8eac2c9f6'
    Loading: 0 packages loaded
    
    Generating Hashes for Revision '310993d82d94b0cf7a5389e659e35ac32cd32089'
    Loading: 0 packages loaded
    
    Determining Impacted Targets
    
    [3188] Impacted Targets between d2708ad6cca730d4c17317388a7acbf8eac2c9f6 and 310993d82d94b0cf7a5389e659e35ac32cd32089:
    //:interpreter_ops
    //:interpreter_passes
    //:linalg_passes
    //:reference_api
    [...]
    //stablehlo/testdata:abs_int32_20_20.mlir.test
    //stablehlo/testdata:abs_int64_20_20.mlir.test
    [...]
    //stablehlo/tests:check_ops
    //stablehlo/tests:chlo/chlo_legalize_to_stablehlo.mlir.test
    //stablehlo/tests:chlo/chlo_legalize_to_stablehlo_broadcast.mlir.test
    //stablehlo/tests:infer_chlo.mlir.test
    //stablehlo/tests:infer_stablehlo.mlir.test
    //stablehlo/tests:interpret/abs.mlir.test
    [...]
    ```

[33mcommit dfd577489d880bad4ec2f474ddfc961e91ef44a6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Mar 4 17:37:55 2025

    Integrate LLVM at llvm/llvm-project@fd9a882ce31c (#2731)
    
    Co-authored-by: Bart Chrzaszcz <bartchr@google.com>

[33mcommit d8bd2c3ca0c515d60648ff20dbd747a5816b1b1f[m
Author: Matthias Guenther <mrguenther@google.com>
Date:   Fri Feb 28 22:29:23 2025

    Bump patch version after integrate 1.9.5 -> 1.9.6 (#2730)

[33mcommit 0836ecbf42e7fb116d9fde06ce60d4b9551109fb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 28 17:55:17 2025

    [awesome] Add shield to reactant on openxla awesome page (#2728)

[33mcommit d2708ad6cca730d4c17317388a7acbf8eac2c9f6[m
Author: William Moses <gh@wsmoses.com>
Date:   Fri Feb 28 17:02:33 2025

    Add Reactant.jl (#2726)

[33mcommit 32c9877a712b10d2a67f32d868c55e028c3ae4e6[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 27 23:03:48 2025

    Add source location downgrade to compatibility expander (#2725)
    
    Fixes https://github.com/openxla/stablehlo/issues/2724
    
    Compatibility expander is an opt-in pass used in PJRT/ODML/etc to try to
    provide forward compatibility even when a new feature is used.
    
    These can be costly or impact performance as they usually decompose a
    single op into many, etc.
    
    Note - This is only run when requesting to downgrade to a version before
    1.8.5, meaning it will not impact any PJRT users today, only for jax2tf
    or other export workflows.

[33mcommit 03597b1e592129f0c79e99e5ed65dac7ebee240f[m[33m ([m[1;33mtag: [m[1;33mv1.9.5[m[33m, [m[1;32mfix-typo[m[33m)[m
Author: Matthias Guenther <7648799+mrguenther@users.noreply.github.com>
Date:   Tue Feb 25 21:19:54 2025

    Integrate LLVM at llvm/llvm-project@d23da7d6300e (#2720)
    
    Per https://github.com/openxla/stablehlo/tree/main/build_tools/integrate.

[33mcommit e3ce339cb38610c1141a6f66c48b631e6bbee659[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Feb 25 07:26:59 2025

    Bump patch version after integrate 1.9.4 -> 1.9.5 (#2719)

[33mcommit e85c83f3427bc772a2173e633715d8f82cba4036[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 25 01:42:00 2025

    Fix the spec example of get_tuple_element (#2718)

[33mcommit 5e9b356b928955a308d87746b1b32294f1f3eebe[m[33m ([m[1;33mtag: [m[1;33mv1.9.4[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Feb 21 20:09:14 2025

    Integrate LLVM at llvm/llvm-project@43d71baae36c (#2717)

[33mcommit c95da49181cc6eca55665a87368112fce6c6b097[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 21 17:53:39 2025

    Bump patch version after integrate 1.9.3 -> 1.9.4 (#2716)

[33mcommit 459897561d365ef97caba46984847f9184d472ec[m[33m ([m[1;33mtag: [m[1;33mv1.9.3[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Feb 13 23:19:36 2025

    Integrate LLVM at llvm/llvm-project@0e779ad4998e (#2713)

[33mcommit f1f32d84bdf148184f6a017b122ca4c289f0f5c3[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Feb 12 22:59:17 2025

    Bump patch version after integrate 1.9.2 -> 1.9.3 (#2712)

[33mcommit debd620e9e9032793d3284aa1acbf1d3a570ad6e[m
Author: Corentin Kerisit <corentin.kerisit@gmail.com>
Date:   Wed Feb 12 22:34:27 2025

    Fix duplicate symbols for downstream CAPIObjects users (#2711)
    
    Should fix failure in https://github.com/openxla/xla/pull/22438
    
    I need just a bit more time to validate this entirely on Linux

[33mcommit 04c5a341abe43f50ea2f8eca97fcc07763230c00[m[33m ([m[1;33mtag: [m[1;33mv1.9.2[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 6 19:50:29 2025

    Integrate LLVM at llvm/llvm-project@f8287f6c373f (#2710)

[33mcommit 4731ce6c4189b4faaa8c454cce3b4056d47cd475[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 6 19:43:12 2025

    Use reference to tensor data when converting to dense elements attr (#2709)
    
    Found that the enzyme project was doing something like this, but didn't
    support complex numbers (our method does). Making this function the best
    of both worlds then will update enzyme.

[33mcommit da09d9ee60f88d903b9e8076ff6c203a41a9c522[m
Author: Corentin Kerisit <corentin.kerisit@gmail.com>
Date:   Thu Feb 6 18:05:45 2025

    Expose a StableHLO Dialect CAPI that doesn't requires building upstream dialects (#2705)
    
    This is an attempt PR to address #2593
    
    This PR introduces 2 seperate targets:
    1. `stablehlo_dialect_capi` which embeds StableHLO + Serialization.
    2. `stablehlo_unified_capi` which embeds `stablehlo_dialect_capi` as
    well as transformation passes (incl linalg) and reference interpreter.
    
    Note:
    
    - I kept CHLO C API as a separate target as it was. I could include it
    in `stablehlo_dialect_capi` but since it was already exposed as a
    separate target and not included in the former `stablehlo_capi` I didn't
    do it just yet.
    
    - `stablehlo_passes` is still included in `dialect_capi` because it's
    needed by serialisation.
    
    - I kept an alias `stablehlo_capi` on `stablehlo_unified_capi` just so
    that downstreams don't have to rename the target they depend on. Let me
    know if this is un-needed.
    
    - I had to extract a small type conversion delcared in `linagl_passes`
    but used in `stablehlo_passes` which forced `stablehlo_passes` to depend
    on `linagl_passes`.
    I named this extracted target `stablehlo_type_conversions` and put it
    under `stablehlo/transforms/conversions`. Let me know if this is not the
    right location.
    
    ## Dependency Graph
    
    ### `stablehlo_dialect_capi`
    ![graphviz
    (5)](https://github.com/user-attachments/assets/faf7d62c-2cc2-4f11-b255-33b4e249112e)
    
    ### `stablehlo_unified_capi`
    ![graphviz
    (6)](https://github.com/user-attachments/assets/6aa6edf9-9b90-41ca-b741-30255a87a2a1)
    
    
    Finally I tested integration in XLA and JAX as well as python bindings
    to validate that it links correctly.
    
    Let me know what you think and if you see further changes to add to this
    PR :)

[33mcommit 2720b9029944eb71af39937116699909ccaadd3b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 4 23:08:34 2025

    Add target independent optimization pass (#2707)

[33mcommit 21e80789c81a8ec3ce96a285369b2d903934921a[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Feb 3 21:43:05 2025

    Bump patch version after integrate 1.9.1 -> 1.9.2 (#2706)

[33mcommit 7775e3e2f98fe458f227cdc41aec567535d1c796[m[33m ([m[1;33mtag: [m[1;33mv1.9.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Sat Feb 1 00:56:21 2025

    Integrate LLVM at llvm/llvm-project@956c0707d909 (#2704)

[33mcommit bde336e3febe18279b9c70343bcb9a3eb10c36b3[m
Author: bubblepipe <30717258+bubblepipe@users.noreply.github.com>
Date:   Sat Feb 1 00:07:10 2025

    Update tosa.mul op in StableHLO-to-TOSA Pass in compliance with MLIR upstream and TOSA-v1.0 (#2702)
    
    This PR updates the StableHLO to TOSA legalization pass to align with
    the recent changes in the MLIR upstream. Specifically,
    [the¬†`tosa::MulOp`¬†operation has being modified to comply with the
    TOSA-v1.0
    specification](https://github.com/llvm/llvm-project/pull/121953). The
    shift parameter of the MUL operator, which was previously an attribute,
    has been moved to an SSA operand.¬†
    
    The upstream changes caused a compilation failure in the StableHLO to
    TOSA conversion pass because the¬†`tosa.mul`¬†operation now expects 3
    operand groups instead of 2, as reflected in the error message:
    
    ```jsx
    error: invalid number of operand groups for `tosa.mul`; expected 3, but got 2
         with op<tosa.mul>(input0, input1) {shift = attr<"0 : i8">};
              ^
    ```
    
    This PR added a zero constant as the additional argument for the shift
    parameter to maintain compatibility with the updated `tosa.mul`
    operation.

[33mcommit b62dc66da9946b4c400c0d99c9d5bb8e04edaee6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Jan 30 02:17:56 2025

    Bump patch version after integrate 1.9.0 -> 1.9.1 (#2703)

[33mcommit 48a1e14edc8219577fcad53de1924876f855f431[m[33m ([m[1;33mtag: [m[1;33mv1.9.0[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Jan 29 04:32:38 2025

    Integrate LLVM at llvm/llvm-project@aa65f93b71de (#2701)
    
    Co-authored-by: Rachel Han <hanrach@google.com>

[33mcommit 7c50d4efeaea30bff6aa5e46c7f71170f5aa06af[m[33m ([m[1;32mllvmbump0128_v1[m[33m, [m[1;32mllvmbump0128[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jan 29 01:47:38 2025

    Add ResultAccuracy to ExpOp (#2694)
    
    Implementation of RFC: https://github.com/openxla/stablehlo/pull/2592
    
    For ExpOp.
    
    TODO: Modify spec.md
    
    ---------
    
    Co-authored-by: Rachel Han <hanrach@google.com>

[33mcommit 8993ef703e5f98baa0da56346621e07932c68d8c[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Jan 28 23:07:17 2025

    Bump patch version after integrate 1.8.11 -> 1.8.12 (#2700)

[33mcommit 37750d07c3e26a526cc5aa9e35fc86ab04f01d7e[m
Author: Kasper Nielsen <kasper0406@gmail.com>
Date:   Tue Jan 28 17:26:15 2025

    Add stablehlo-coreml to awesome.md (#2699)

[33mcommit 540d48f2dc09a9edf15dc5c5f435579ee7616ec4[m
Author: Jan <janpfeifer@users.noreply.github.com>
Date:   Mon Jan 27 15:49:56 2025

    Added GoMLX and gopjrt for awesome.md (#2696)

[33mcommit b9d4c70200f4e7e73d7707c6b431087faae872e1[m
Author: Scott Todd <scott.todd0@gmail.com>
Date:   Mon Jan 27 15:49:38 2025

    Add IREE project link to awesome.md. (#2698)
    
    Happy to iterate on the specific phrasing, suggestions welcome.
    
    I wonder if some reorganization would help too, since IREE could fit
    into either the "PJRT Plugins" or "Edge Compilation" sections.

[33mcommit c02033c1cdce0e15761bfa36761bd446437b73f4[m
Author: Scott Todd <scott.todd0@gmail.com>
Date:   Mon Jan 27 15:44:56 2025

    Add source link to awesome.md. (#2697)
    
    I couldn't find an easy link back to this source file from
    https://openxla.org/stablehlo/awesome.
    
    I _think_ this style of URL should be fine, but I might be missing some
    website detail (for example, the hosted website has support for
    translations).
    
    A site-wide solution may be available for some source files. For
    example:
    * The mkdocs-material site generator has "edit this page" and "view
    source of this page" actions:
    https://squidfunk.github.io/mkdocs-material/setup/adding-a-git-repository/#code-actions.
    * Some tensorflow examples include "View source on GitHub" buttons for
    notebooks: https://www.tensorflow.org/guide/keras/functional_api

[33mcommit c27ba678a712a401e4a6db75ec0ef9e1ce9e1777[m[33m ([m[1;33mtag: [m[1;33mv1.8.11[m[33m)[m
Author: Scott Todd <scott.todd0@gmail.com>
Date:   Thu Jan 23 23:21:28 2025

    Expose populateStablehloToLinalgConversionPatterns function (#2695)
    
    This function declaration has existed since the code was forked from
    IREE in https://github.com/openxla/stablehlo/pull/1817, but the
    implementation was kept private (static function within an anonymous
    namespace).
    
    I'm now trying to switch IREE from having its own implementation to
    using the upstream implementation from this project in
    https://github.com/iree-org/iree/pull/19792, and I would like to access
    these patterns directly, instead of through the
    `StablehloLegalizeToLinalgPass`. With the patterns I can run conversion
    including my own sets of additional patterns, while a pass runs in
    isolation.
    
    I'm also deleting the `populateLegalizeChloPatterns`,
    `populateLegalizeControlFlowPatterns`, and
    `populateLegalizeShapeComputationPatterns` declarations, which were not
    migrated from IREE and are also dangling without implementations.

[33mcommit 01baa234a3324e58995d787d6c136f9b3a03df98[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jan 22 20:18:38 2025

    Bump patch version after integrate 1.8.10 -> 1.8.11 (#2692)

[33mcommit 8cd9444b78ccec3e42a4b21105a5a547c021e823[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Jan 21 20:57:11 2025

    Integrate LLVM at llvm/llvm-project@e2402615a5a7 (#2693)

[33mcommit 23d7f60f4f604f9d0ee89023e333422d996537fa[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Jan 17 23:37:12 2025

    Add PrintOp to the Interpreter dialect (#2687)
    
    While writing an MLIR pass, I realized there's no easy way to inspect
    the intermediate states of tensors while debugging them. ProbeOp does
    something similar, but this does not provide a human readable way of
    inspecting them given arbitrary SSA value.

[33mcommit c125b3284819fec57120231cf4430657dab7b881[m[33m ([m[1;33mtag: [m[1;33mv1.8.10[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jan 16 00:58:25 2025

    Bump patch version after integrate 1.8.9 -> 1.8.10 (#2691)

[33mcommit 459e481b77f8537aae3f8e8e8ad9550721afe202[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jan 15 02:21:47 2025

    Integrate LLVM at llvm/llvm-project@b270525f730b (#2689)

[33mcommit 7c151f5887c4c42b95026458a95b90440ce3d18c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jan 13 17:14:38 2025

    Remove debug comments (#2688)

[33mcommit b2d36c56613163084e369cf2df7b2527608f74d9[m[33m ([m[1;33mtag: [m[1;33mv1.8.9[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jan 13 16:35:24 2025

    Integrate LLVM at llvm/llvm-project@faa3f752896903 (#2686)
    
    Updated VHLO test files, there was an upstream change that modified the
    in-memory use-list in a fully compatible way, likely caused by something
    about the order of transforms applied, or the inclusion of new
    unreazlied_casts as intermediate values.

[33mcommit 51028d94cf2b3bca147609a1975e654c3fd024b4[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Wed Jan 8 21:37:25 2025

    Add StableHLO complex exponential to stablehlo-complex-math-expander pass (#2682)
    
    As in the title. This PR is created on top of the branch of
    https://github.com/openxla/stablehlo/pull/2681.
    
    This PR improves the accuracy of JAX complex exp function as follows:
    
    ```
    Before
    ------
    
    test_unary[exp-jax-cpu-complex64-default] maximal ULP difference: 4294967296
    ULP difference == 0: 1964868
    ULP difference == 1: 133291
    ULP difference == 2: 1035
    ULP difference == 4294967296: 1606
    
    test_unary[exp-jax-cuda-complex64-default] maximal ULP difference: 4294967296
    ULP difference == 0: 1787925
    ULP difference == 1: 300591
    ULP difference == 2: 10657
    ULP difference == 3: 79
    ULP difference == 4294967296: 1548
    
    After
    -----
    
    test_unary[exp-jax-cpu-complex64-default] maximal ULP difference: 2
    ULP difference == 0: 1966101
    ULP difference == 1: 133662
    ULP difference == 2: 1037
    
    test_unary[exp-jax-cuda-complex64-default] maximal ULP difference: 3
    ULP difference == 0: 1788889
    ULP difference == 1: 301112
    ULP difference == 2: 10720
    ULP difference == 3: 79
    ```
    
    The corresponding accuracy patterns are available in
    https://github.com/pearu/functional_algorithms/issues/44#issuecomment-2569553310

[33mcommit f67c73d702271e1eb7ee83141de388c36247113e[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Wed Jan 8 21:10:17 2025

    Add StableHLO complex log to stablehlo-complex-math-expander pass (#2681)
    
    As in the title. This PR is created on top of the branch of
    https://github.com/openxla/stablehlo/pull/2679.
    
    This PR improves the accuracy of JAX complex `log` function as follows:
    ```
    Before
    ------
    test_accuracy.py::test_unary[log-jax-cpu-complex64-default] maximal ULP difference: 4294967296
    ULP difference == 0: 1612359
    ULP difference == 1: 487617
    ULP difference == 2: 306
    ULP difference == 3: 140
    ULP difference == 4: 68
    ULP difference == 5: 49
    ULP difference == 6: 26
    ULP difference == 7: 35
    ULP difference == 8: 19
    ULP difference == 9: 15
    ULP difference == 10: 15
    ULP difference >= 11: 151
    
    test_accuracy.py::test_unary[log-jax-cuda-complex64-default] maximal ULP difference: 4294967296
    ULP difference == 0: 1797796
    ULP difference == 1: 301398
    ULP difference == 2: 1044
    ULP difference == 3: 169
    ULP difference == 4: 85
    ULP difference == 5: 51
    ULP difference == 6: 32
    ULP difference == 7: 20
    ULP difference == 8: 7
    ULP difference == 9: 13
    ULP difference == 10: 8
    ULP difference >= 11: 177
    
    After
    -----
    test_accuracy.py::test_unary[log-jax-cpu-complex64-default] maximal ULP difference: 3
    ULP difference == 0: 1581126
    ULP difference == 1: 519652
    ULP difference == 2: 19
    ULP difference == 3: 3
    
    test_accuracy.py::test_unary[log-jax-cuda-complex64-default] maximal ULP difference: 2
    ULP difference == 0: 1775914
    ULP difference == 1: 324185
    ULP difference == 2: 701
    ```
    
    The corresponding accuracy patterns are available in
    https://github.com/pearu/functional_algorithms/issues/46#issuecomment-2566567331
    .

[33mcommit 822cedff31998092c197e9b1a2d4e0d54a85728c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jan 8 18:52:47 2025

    [StableHLO Quantization] Add quantization documentation (#2684)

[33mcommit 0008018c96f25a657c6b2b705a4c78711d2cfce0[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Wed Jan 8 18:51:39 2025

    Add StableHLO complex sqrt to stablehlo-complex-math-expander pass (#2679)
    
    As in the title.
    
    The [existing implementation of
    `sqrt`](https://github.com/openxla/xla/blob/30caa6782b2f49c9ecb8f4727d8628a12ee9a861/xla/service/elemental_ir_emitter.cc#L2111)
    on complex inputs uses polar form of complex sqrt which is
    inaccurate/incorrect on about 28 % of uniformly distributed samples over
    all complex plane. The JAX complex sqrt accuracy statistics is as
    follows:
    
    ```
    test_unary[sqrt-jax-cuda-complex64-default] maximal ULP difference: 2792619172
    ULP difference == 0: 297588
    ULP difference == 1: 1106945
    ULP difference == 2: 78921
    ULP difference == 3: 5924
    ULP difference == 4: 2938
    ULP difference == 5: 2231
    ULP difference == 6: 1855
    ULP difference == 7: 1648
    ULP difference == 8: 1449
    ULP difference == 9: 1263
    ULP difference == 10: 1089
    ULP difference >= 11: 598949
    
    test_unary[sqrt-jax-cpu-complex64-default] maximal ULP difference: 2760370645
    ULP difference == 0: 699582
    ULP difference == 1: 759750
    ULP difference == 2: 58127
    ULP difference == 3: 3237
    ULP difference == 4: 1665
    ULP difference == 5: 1185
    ULP difference == 6: 1021
    ULP difference == 7: 871
    ULP difference == 8: 804
    ULP difference == 9: 653
    ULP difference == 10: 622
    ULP difference >= 11: 573283
    ```
    
    This PR provides an algorithm for complex sqrt that is accurate up to
    3/6 ULP difference error on complex samples. The corresponding JAX
    complex sqrt accuracy statistics is as follows:
    ```
    test_unary[sqrt-jax-cuda-complex64-default] maximal ULP difference: 5
    ULP difference == 0: 1060571
    ULP difference == 1: 1008268
    ULP difference == 2: 31136
    ULP difference == 3: 686
    ULP difference == 4: 129
    ULP difference == 5: 10
    
    test_unary[sqrt-jax-cpu-complex64-default] maximal ULP difference: 2
    ULP difference == 0: 1348868
    ULP difference == 1: 751504
    ULP difference == 2: 428
    ```
    
    It is interesting to note that although the same algorithm is used for
    both CUDA and CPU platforms, then the expected maximal ULP difference is
    4 (obtained from applying the algorithm to numpy arrays). Hence
    - the accuracy of complex sqrt on CPU is better than expected
    - the accuracy of complex sqrt on CUDA is worse than expected because
    CUDA sqrt produces slightly different results from std sqrt on float
    inputs.

[33mcommit c06b1a10d66069505ccbab72b21f6fa0547c617d[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Mon Dec 23 17:26:03 2024

    Augment bazelrc to support asan & ubsan (#2197)
    
    A prior failure in the google3 internal codebase showed that we were not
    executing the sanitizers like we thought we were.
    
    This change augments bazelrc to do asan and ubsan via config. It
    leverages the default feature set offered by bazelrc.
    
    I tested the commit priot to 474caa8370dccc753e4693a2ff4221864b227597 to
    validate that the bug was exhibited.
    
    I've added the sanitizers to the default Bazel build which also builds
    debug builds.
    This may come at an increase cost. If undesirable we can split the job
    out.

[33mcommit aee33b539c9ab92fd603bd22524d268e8b14c434[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Dec 23 17:19:20 2024

    Add refinement test for if/case (#2675)
    
    cc @bartchr808 for pointing out this test gap

[33mcommit 960b2318ab3c96b13a5b053485b71e155ef41351[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 20 23:39:59 2024

    Bump patch version after integrate 1.8.8 -> 1.8.9 (#2677)

[33mcommit 38bb2f9bf63b714e8a49fe34a478139058ee1660[m[33m ([m[1;33mtag: [m[1;33mv1.8.8[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 20 19:03:08 2024

    Cleaning up tests in chlo_ops.mlir backported twice (#2676)

[33mcommit 8c7946c8c94182d238600fb0775a5ba594a43bd7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 20 17:26:39 2024

    Integrate LLVM at llvm/llvm-project@e86910337f98 (#2674)

[33mcommit adbeeca967549059dec9880fcc27806cdb78eb11[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Thu Dec 19 15:36:17 2024

    Add StableHLO complex log1p operation. Add pass stablehlo-complex-math-expander (#2636)
    
    As in the title.
    
    This PR introduces a new pass stablehlo-complex-math-expander that
    expands StableHLO complex functions in terms of StableHLO real
    functions. Currently, only StableHLO_Log1pOp on complex inputs is
    included in this expander, more will be added as follow-ups.
    
    The provided complex `log1p` operation fixes the inaccuracy problems in
    jax.numpy.log1p for complex inputs `x+i*y` when `x` is close to `-0.5 *
    y * y` which triggers catastrophic cancellations when using
    straightforward definition of log1p on complex inputs. The current state
    of `jax.numpy.log1p` inaccuracies is given in
    https://github.com/pearu/functional_algorithms/issues/47 .
    
    With this PR, the accuracy statistics of log1p is:
    ```
    complex64:
    ULP difference == 0 count is 961936
    ULP difference == 1 count is 39644
    ULP difference == 2 count is 455
    ULP difference == 3 count is 0
    ULP difference >= 4 count is 0
    
    complex128:
    ULP difference == 0 count is 988144
    ULP difference == 1 count is 13891
    ULP difference == 2 count is 0
    ULP difference == 3 count is 0
    ULP difference >= 4 count is 0
    ```

[33mcommit 33f19a4fd69adafa609bce52d18c3d0d591f3190[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Dec 18 18:36:56 2024

    Bump patch version after integrate 1.8.7 -> 1.8.8 (#2672)

[33mcommit 1ff299ee789d143bffc5c360e61b6a6864be2dc9[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 18 17:56:43 2024

    Integrate LLVM at llvm/llvm-project@af20aff35ec3 (#2673)
    
    Pulling in the additional StableHLO internal changes via
    [temporary.patch](https://raw.githubusercontent.com/openxla/xla/refs/heads/main/third_party/stablehlo/temporary.patch)

[33mcommit 9e0c9e34d02af255bfe8f896ba7dc910758c6ecf[m[33m ([m[1;31morigin/main[m[33m, [m[1;31morigin/HEAD[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Dec 16 19:40:46 2024

    Add 'large' test qualifier for lit tests (#2671)
    
    Some tests fail in forked repo CI since they take just over 60s, thus
    requiring something bigger than `small` test size in bazel.
    
    Example, my forked repo:
    https://github.com/GleasonK/stablehlo/actions/runs/12329876949/job/34414702213
    
    This PR splits lit tests so that all `testdata` files ending in
    `*.large.mlir` will be run with more RAM and timeout.
    
    Closes https://github.com/openxla/stablehlo/issues/2590

[33mcommit 54dafb1c4104707fb15d35dfa125631c4761f158[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Dec 16 19:00:42 2024

    Integrate LLVM at llvm/llvm-project@af20aff35ec3 (#2670)

[33mcommit 38fe0f49d9b2bb70a36d3c535680070f6a5595e7[m[33m ([m[1;33mtag: [m[1;33mv1.8.7[m[33m)[m
Author: Vimarsh Sathia <39610523+vimarsh6739@users.noreply.github.com>
Date:   Sun Dec 15 02:53:50 2024

    Expose materialize functions in Chlo to Stablehlo lowering (#2665)
    
    We want to perform constant propogation through `chlo.lgamma` in
    Enzyme-JaX
    
    
    [Kevin](https://github.com/EnzymeAD/Enzyme-JAX/pull/182#discussion_r1876814931)
    mentioned he was open to exposing some materialize functions (which are
    currently static, and not callable from [our
    pass](https://github.com/EnzymeAD/Enzyme-JAX/blob/main/src/enzyme_ad/jax/Passes/EnzymeHLOOpt.cpp)
    atm)
    
    
    
    @wsmoses @GleasonK

[33mcommit 2f6be83b18d0c51350a570074113643619a9998d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 12 20:00:06 2024

    Integrate LLVM at llvm/llvm-project@0876c11ceeb0 (#2666)

[33mcommit ef176a130f28196dcb4a5735d0f2f6ed0f85bd5d[m
Author: Max Manainen <maximmanainen@gmail.com>
Date:   Mon Dec 9 18:20:08 2024

    Add StableHLO to linalg conversions to python bindings (#2660)
    
    Testing function:
    ```
    from mlir.dialects import stablehlo
    from mlir.ir import Context, Location, Module
    import mlir.dialects.arith
    from mlir.passmanager import PassManager
    
    mlir_text = """
    func.func @dot_general(%arg0: tensor<?x?x?xf32>,
                      %arg1: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> {
      %0 = "stablehlo.dot_general"(%arg0, %arg1) {
        dot_dimension_numbers = #stablehlo.dot<
          lhs_batching_dimensions = [1],
          lhs_contracting_dimensions = [2],
          rhs_batching_dimensions = [2],
          rhs_contracting_dimensions = [1]
        >,
        precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>],
        someattr
      } : (tensor<?x?x?xf32>, tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
      func.return %0 : tensor<?x?x?xf32>
    }
    """
    
    with Context() as ctx:
        stablehlo.register_dialect(ctx)
        stablehlo.register_stablehlo_passes()
    
    
        module = Module.parse(mlir_text)
    
        pm = PassManager.parse(
            "builtin.module(func.func("
            "shape-legalize-to-stablehlo,"
            "stablehlo-aggressive-folder,"
            "stablehlo-aggressive-simplification,"
            "stablehlo-legalize-to-linalg"
            "))"
        )
    
        pm.run(module.operation)
    
        print(f"{module}")
    ```
    
    Before this change we get an error:
    ```
        pm = PassManager.parse(
             ^^^^^^^^^^^^^^^^^^
    ValueError: MLIR Textual PassPipeline Parser:1:103: error: 'stablehlo-legalize-to-linalg' does not refer to a registered pass or pass pipeline
    func.func(shape-legalize-to-stablehlo,stablehlo-aggressive-folder,stablehlo-aggressive-simplification,stablehlo-legalize-to-linalg)
    ```
    
    Now we get an expected result:
    ```
    #map = affine_map<(d0, d1, d2, d3) -> (d1, d0, d3)>
    #map1 = affine_map<(d0, d1, d2, d3) -> (d2, d3, d0)>
    #map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
    module {
      func.func @dot_general(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> {
        %c1 = arith.constant 1 : index
        %dim = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
        %c0 = arith.constant 0 : index
        %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
        %c0_1 = arith.constant 0 : index
        %dim_2 = tensor.dim %arg1, %c0_1 : tensor<?x?x?xf32>
        %from_elements = tensor.from_elements %dim, %dim_0, %dim_2 : tensor<3xindex>
        %c0_3 = arith.constant 0 : index
        %extracted = tensor.extract %from_elements[%c0_3] : tensor<3xindex>
        %c1_4 = arith.constant 1 : index
        %extracted_5 = tensor.extract %from_elements[%c1_4] : tensor<3xindex>
        %c2 = arith.constant 2 : index
        %extracted_6 = tensor.extract %from_elements[%c2] : tensor<3xindex>
        %0 = tensor.empty(%extracted, %extracted_5, %extracted_6) : tensor<?x?x?xf32>
        %cst = arith.constant 0.000000e+00 : f32
        %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%arg0, %arg1 : tensor<?x?x?xf32>, tensor<?x?x?xf32>) outs(%1 : tensor<?x?x?xf32>) attrs =  {someattr} {
        ^bb0(%in: f32, %in_7: f32, %out: f32):
          %3 = arith.mulf %in, %in_7 : f32
          %4 = arith.addf %out, %3 : f32
          linalg.yield %4 : f32
        } -> tensor<?x?x?xf32>
        return %2 : tensor<?x?x?xf32>
      }
    }
    ```

[33mcommit 36bbe2798ebb9e43ed7837a1051f922a16f9dac1[m
Author: Pavithra Eswaramoorthy <pavithraes@outlook.com>
Date:   Mon Dec 9 18:19:38 2024

    Workflow to run tutorials in CI (#2663)
    
    This PR creates a workflow to execute the Jupyter notebooks in
    `docs/tutorials`. Couple of notes:
    - It installs dependencies as defined in the notebook with `!pip install
    ...`, making it a little slow. We can instead create a
    `requirements.txt` and [cache
    dependencies](https://github.com/actions/setup-python#caching-packages-dependencies)
    for efficient runs, but then we won't be testing the actual user
    workflows running the tutorials on kaggle/colab
    - The workflow only _executes_ the notebooks to make sure there are no
    errors. The notebooks are not altered. We can also update the notebooks
    in-place with the freshly executed versions for consistent output. The
    only concern here is the workflow might fail to push commits to the PR
    if it were opened from forks due to permission issues.
    
    The PR also updates `savedmodel-embed.ipynb`; `target_version` is now a
    required argument in `stablehlo_to_tf_saved_model()`
    
    Tested on my fork:
    https://github.com/pavithraes/stablehlo/actions/runs/12239259032/job/34139262358#step:5:1
    
    ---------
    
    Signed-off-by: Pavithra Eswaramoorthy <pavithraes@outlook.com>

[33mcommit b44460a13096f48058abff6f79654901127b1861[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Dec 6 20:48:00 2024

    Bump patch version after integrate 1.8.6 -> 1.8.7 (#2662)

[33mcommit b3d3cacde8994df313297e68713ed74c2ca279ee[m[33m ([m[1;33mtag: [m[1;33mv1.8.6[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Dec 6 18:15:26 2024

    Integrate LLVM at llvm/llvm-project@2ccf7ed277df (#2661)
    
    + backport to gh changes from the temporary.patch

[33mcommit 74e5cf420aad2a4b79768990a2e6b0ddb8dc4cd5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Dec 5 16:36:02 2024

    Remove a spurious comment from VhloTypes.td (#2653)

[33mcommit 87675c10261ebb6b54daa8ef71cebeaded4b1bd5[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Dec 4 22:40:14 2024

    Bump patch version after integrate 1.8.5 -> 1.8.6 (#2657)

[33mcommit 1ab7ac642b0ca4961e87676617bf2370d389723f[m
Author: Rachel Han <185134389+hanrach9@users.noreply.github.com>
Date:   Wed Dec 4 00:54:35 2024

    [RFC] Add result accuracy to unary ops (#2592)
    
    This is a proposal to add `result_accuracy` attribute to the following
    transcendental unary ops to StableHLO: `sin`, `cos`, `tan`, `tanh`,
    `sqrt`, `rsqrt`,
    `cbrt`, `exp`, `expm1`, `log`, `logp1`, `logistic` and `erf`. Any
    feedback would be appreciated, thanks!

[33mcommit d1db6dfe13488b4735a6f08c324145625447fb6d[m[33m ([m[1;33mtag: [m[1;33mv1.8.5[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Dec 3 19:56:08 2024

    Integrate LLVM at llvm/llvm-project@bd92e4620433 (#2656)

[33mcommit db2ce93922933fc21e17311b4b001c25b46024e7[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Dec 2 15:48:58 2024

    Bump patch version after integrate 1.8.4 -> 1.8.5 (#2651)

[33mcommit 7a7baa05393bb67e9d1d2b25f959cbc444fc88a1[m
Author: Christopher Bate <cbate@nvidia.com>
Date:   Mon Nov 25 23:35:58 2024

    [transforms] Fix simplification patterns for `stablehlo.(and|or)` (#2638)
    
    Fixes an issue in `stablehlo-aggressive-simplification` where `%1` in
    the below would get replaced by `%arg0`:
    
    ```
      %0 = stablehlo.constant dense<1> : tensor<2xi32>
      %1 = stablehlo.and %0, %arg0 : tensor<2xi32>
    ```
    
    The pattern was checking whether `%0` is equal to `0b1` and was
    only tested on bools. A similar bug existed for `stablehlo.and`. Fixed
    by just making sure the constant is integer with all bits set to 1.

[33mcommit f21104d05371210da8e849901f75e07d3c1ff0d2[m[33m ([m[1;33mtag: [m[1;33mv1.8.4[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Nov 21 22:48:48 2024

    Integrate LLVM at llvm/llvm-project@33fcd6acc755 (#2635)

[33mcommit 7efac85aaaaa5e4e0233313c2782e4bc5d62b472[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 20 14:33:02 2024

    Don't require inlining for shape refinement (#2631)
    
    Currently we require MLIR function inlining before refining shapes.
    Refining shapes in order and following call operations (enforcing no
    recursive calls) should allow for `refine(dynamic_export, static_args)
    == static_export` to be true

[33mcommit 2d42d1207ab9c51a25ce1bc8ebce5365b44b41dc[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 18 22:41:28 2024

    Upload integrate tools for integrate transparency (#2627)
    
    Moving scripts that our StableHLO oncall rotation runs ~2x/wk to
    integrate StableHLO into XLA / other OpenXLA projects.
    
    If any additional changes are needed that should be a part of our oncall
    duties, they can be proposed to be added to these scripts.
    
    Integrating some of the work @apivovarov did into these scripts.

[33mcommit 138294918d0d0e7dd2a245f04f6dae0e8c5f04b9[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Mon Nov 18 18:35:21 2024

    Add CHLO square operation. (#2625)
    
    As in the title.
    
    Providing `square` as a CHLO operation enables fixing inaccuracy
    problems in jax.numpy.square for complex inputs with large real and
    imaginary parts.
    
    This PR replaces https://github.com/openxla/stablehlo/pull/2623 and
    enables eliminating `square` lowering introduced in
    https://github.com/jax-ml/jax/pull/24874
    
    The details of square inaccuracies is given in
    https://github.com/pearu/functional_algorithms/issues/52 . Roughly
    speaking, the source of inaccuracies in square(z).real is the usage of
    z.real ** 2 - z.imag ** 2 that terms will overflow when abs(z.real/imag)
    > sqrt(largest). The overflow does not happen when using (z.real -
    z.imag) * (z.real + imag) formula instead.
    
    As a side-effect to using the 0.11.1 version of function_algorithms,
    several mlir test scripts for asin/asinh/acos/... functions are updated
    as well.

[33mcommit f151a38023c582d4ffe681f3899f5a1512e39c5f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Nov 15 20:21:31 2024

    Bump patch version after integrate 1.8.3 -> 1.8.4 (#2626)

[33mcommit b1c1115f070493845ad92bd4a4461f77a2aa628d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 13 23:30:35 2024

    Update roadmap with Q4'24 / H1'25 plans (#2624)

[33mcommit 37487a8ee0783f29eff64910702074ba52bf6c5c[m[33m ([m[1;33mtag: [m[1;33mv1.8.3[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 13 21:35:24 2024

    Integrate LLVM at llvm/llvm-project@246b57cb2086 (#2620)

[33mcommit 830b9787c58b400f380c2f945bd00a0675307cc3[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Nov 13 04:14:52 2024

    awesome.md (#2622)

[33mcommit 6b79a2e4da6dc87a89748f38abebae7c4b0d0619[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Tue Nov 12 17:18:36 2024

    Bump WEEK_12 to v1.6.0, WEEK_4 to 1.7.7 (#2621)
    
    ```
    WEEK_4: v1.7.7 2024-10-04 37 days ago
    WEEK_12: v1.6.0 2024-08-16 87 days ago
    ```

[33mcommit 619c3262e298b63dcaba0544e9023c792e2b8291[m
Author: Pavithra Eswaramoorthy <pavithraes@outlook.com>
Date:   Mon Nov 11 23:35:58 2024

    Update PyTorch export tutorial (#2617)
    
    This PR updates the "Exporting StableHLO from PyTorch" tutorial.
    Specifically:
    
    - Update dependencies, combine the tensorflow dependency in the first
    section to avoid warnings (about using tensorflow-cpu) on Kaggle
    - Minor updates to narration and links, along with an introduction
    statement
    - Removed dynamic batch size code, addedd notes instead
    
    (Supersedes #2616, same updates but we get a rich diff here)
    
    ---------
    
    Signed-off-by: Pavithra Eswaramoorthy <pavithraes@outlook.com>

[33mcommit 4d85f95a737cbd97d769e7449ef8fac8138bae18[m[33m ([m[1;32munary_test[m[33m, [m[1;32mawsome[m[33m)[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Tue Nov 5 16:20:16 2024

    Automatically update WEEK_4 WEEK_12 versions (#2585)
    
    This PR adds `update_version_h_cpp.sh` script
    
    The script will:
    - Automatically calculate `WEEK_4` and `WEEK_12` versions based on
    Version tag date and current date.
    - Bump CurrentVersion by increasing patch version by 1
    
    This script should be executed at the beginning of the next development
    cycle, after performing the git tagging.
    
    This script:
    1. Updates current version in `stablehlo/dialect/Version.h`
    
    ```cpp
      static Version getCurrentVersion() { return Version(x, y, z); }
    ```
    New version will be (x, y, z + 1)
    
    2. Replaces WEEK_4 and WEEK12 versions in stablehlo/dialect/Version.cpp
    ```
          return Version(a, b, c);  // WEEK_4 ANCHOR: DO NOT MODIFY
          return Version(m, n, p);  // WEEK_12 ANCHOR: DO NOT MODIFY
    ```
    - WEEK_4 version - The most recent git tag that was created at least 28
    days ago.
    - WEEK_12 version - The most recent git tag that was created at least 84
    days ago.
    
    ### Usage Example:
    
    ```bash
    $ build_tools/update_version_h_cpp.sh
    
    Next Current Version: 1, 8, 3
    WEEK_4 Version: 1, 7, 6
    WEEK_12 Version: 1, 5, 1
    Saving /local/home/user/workspace/stablehlo/build_tools/../stablehlo/dialect/Version.h...
      static Version getCurrentVersion() { return Version(1, 8, 3); }
    Saving /local/home/user/workspace/stablehlo/build_tools/../stablehlo/dialect/Version.cpp...
          return Version(1, 7, 6);  // WEEK_4 ANCHOR: DO NOT MODIFY
          return Version(1, 5, 1);  // WEEK_12 ANCHOR: DO NOT MODIFY
    Done
    ```
    
    #### Result:
    
    ```bash
    $ git diff
    diff --git a/stablehlo/dialect/Version.cpp b/stablehlo/dialect/Version.cpp
    index b4ea0d21..d107ba57 100644
    --- a/stablehlo/dialect/Version.cpp
    +++ b/stablehlo/dialect/Version.cpp
    @@ -83,9 +83,9 @@ Version Version::fromCompatibilityRequirement(
         case CompatibilityRequirement::NONE:
           return Version::getCurrentVersion();
         case CompatibilityRequirement::WEEK_4:
    -      return Version(1, 7, 5);  // WEEK_4 ANCHOR: DO NOT MODIFY
    +      return Version(1, 7, 6);  // WEEK_4 ANCHOR: DO NOT MODIFY
         case CompatibilityRequirement::WEEK_12:
    -      return Version(1, 5, 0);  // WEEK_12 ANCHOR: DO NOT MODIFY
    +      return Version(1, 5, 1);  // WEEK_12 ANCHOR: DO NOT MODIFY
         case CompatibilityRequirement::MAX:
           return Version::getMinimumVersion();
       }
    diff --git a/stablehlo/dialect/Version.h b/stablehlo/dialect/Version.h
    index 2ea74427..a6fea6b3 100644
    --- a/stablehlo/dialect/Version.h
    +++ b/stablehlo/dialect/Version.h
    @@ -38,7 +38,7 @@ class Version {
       static FailureOr<Version> fromString(llvm::StringRef versionRef);
    
       /// Return a Version representing the current VHLO dialect version.
    -  static Version getCurrentVersion() { return Version(1, 8, 2); }
    +  static Version getCurrentVersion() { return Version(1, 8, 3); }
    
       /// Return a Version representing the minimum supported VHLO dialect version.
       static Version getMinimumVersion() { return Version(0, 9, 0); }
    
    ```

[33mcommit 1d011d838a7c2af07fd275298d302569ad17cd5a[m
Author: Pavithra Eswaramoorthy <pavithraes@outlook.com>
Date:   Tue Nov 5 16:19:27 2024

    Update JAX-export tutorial (#2611)
    
    This PR updates "Exporting StableHLO from JAX" tutorial. Specifically:
    
    - Move from `jax.experimental.export` to `jax.export`, mainly involves
    explicit use of `jax.jit`
    - Modification of narrative text for accuracy, consistency, and
    additional links
    - Add a couple of introductory sentences

[33mcommit fd8ff44d0b0caadb824a3de6002ac5b0568dc6d3[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Nov 4 17:26:01 2024

    Bump patch version after integrate 1.8.2 -> 1.8.3 (#2615)

[33mcommit 3532cafed81ed11727b172baa1460568ae1f5ae4[m[33m ([m[1;32mversionbump1101[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Nov 1 22:20:23 2024

    Minor backport from the itegrate (#2614)

[33mcommit 92e33a7ce2484f2e0083baee7f0dd558f56363f8[m[33m ([m[1;33mtag: [m[1;33mv1.8.2[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Nov 1 18:58:27 2024

    Integrate LLVM at llvm/llvm-project@95c2d798148f (#2613)

[33mcommit 5d15ab064f165cc6773ef4ba949ac083ae8e1fea[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Oct 30 23:32:52 2024

    Add MHLO simplifications to StableHLO (#2608)
    
    Porting over / re-organizing useful simplifications from MHLO to
    StableHLO for broader community use.
    
    This includes all tests from
    [xla/mlir_hlo/tests/Dialect/mhlo/canonicalize/canonicalize.mlir](https://github.com/openxla/xla/blob/2c4b82cab1679273044188da5de780ec8f0eefad/xla/mlir_hlo/tests/Dialect/mhlo/canonicalize/canonicalize.mlir#L4)
    
    Closes https://github.com/openxla/stablehlo/issues/2607
    
    Overview of simplifications now in StableHLO:
    
    ```
    add(X, 0) -> X
    add(cst, X) -> add(X, cst)
    add(cst,cst) -> cst
    and(X, 0) -> 0
    and(X, 1) -> X
    and(cst, X) -> and(X, cst)
    broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...]) [if same numel & rank]
    broadcast_in_dim(X, [iota...]) -> X
    broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...]) [if same numel]
    broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...]) -> broadcast_in_dim(X, merge(dimsA, dimsB))
    broadcast_in_dim(splat, _) -> constant(splat)
    compare(X, X, [EQ,GE,LE]) -> true
    compare(X, X, [NE,GT,LT]) -> false
    compare(cst, X, comparator) -> compare(X, cst, inv(comparator))
    complex(real(X), imag(X))) -> X
    concatenate(X) -> X
    concatenate(X, Y, []) -> concatenate(X, Y)
    concatenate(concatenate(X, Y), Z) -> concatenate(X, Y, Z)
    convert(X, [X.type]) -> X
    dynamic_broadcast_in_dim(X, _, _, [all_nonexpanding...]) -> convert(X)
    dynamic_broadcast_in_dim(X, shape_of(X)) -> X
    dynamic_broadcast_in_dim(dynamic_broadcast_in_dim(X, _, [dimsA...]), shape, [dimsB...]) -> dynamic_broadcast_in_dim(X, shape, merge(dimsA, dimsB))
    dynamic_broadcast_in_dim(dynamic_reshape(X, shape), shape) -> dynamic_reshape(X, shape)
    dynamic_gather(x, constant(slice_sizes)) -> gather(x, slice_sizes)
    dynamic_iota(shape, dim) -> dynamic_broadcast_in_dim(dynamic_iota(slice(shape), dim), shape)
    dynamic_pad(X, low, high, interior) -> pad(X, low, high, interior) [if low, high, interior are all constants]
    dynamic_reshape(dynamic_reshape(X, _), shape)) -> dynamic_reshape(X, shape)
    dynamic_reshape(op(dynamic_reshape(X, shape)), shape) -> op(dynamic_reshape(X, shape)) [if op has same operand and result shape]
    dynamic_slice(X, begin, slice_sizes) -> slice(X, begin, slice_sizes)
    dynamic_update_slice(X, update : zero_extent)) -> X
    dynamic_update_slice(X, update, start_indices : zero)) -> update
    gather(X, cst_start_indices) -> slice(X, slice_start, slice_end)
    get_dimension_size(X, i) -> X.shape[i]
    get_tuple_element(tuple(X_0, X_1, ...), i) -> X_i
    imag(complex(R,I)) -> I
    iota(dim) : multi_rank -> broadcast_in_dim(iota(dim) : array, multi_rank)
    iota(dim) : type -> constant(0) : type [if type[dim] == 1]
    max(cst, X) -> max(X, cst)
    minimum(cst, X) -> minimum(X, cst)
    multiply(X, 0i) -> 0i
    multiply(X, 1i) -> X
    multiply(cst, X) -> multiply(X, cst)
    or(X, 0) -> X
    or(X, 1) -> 1
    or(cst, X) -> or(X, cst)
    pad(empty_tensor, _) -> broadcast_in_dim(empty_tensor, _)
    real(complex(R,I)) -> X
    real_dynamic_slice(X, start, limit, strides) -> dynamic_slice(X, start, limit, strides) [if strides, start are constants, limit = start + constant]
    real_dynamic_slice(X, start, limit, strides) -> slice(X, start, limit, strides) [if start, limit, strides are all constants]
    reduce(X..., dims=[], add) -> X...
    reduce(empty_0, empty_1, ...) -> [broadcast_in_dim(empty_i)...]
    reduce(in_1, in_2, _, _) -> reduce(in_1, _, _) [if unused(in_2)]
    reduce[A](_, _, fn:return A) -> A...
    reshape(X, [X.shape]) -> X
    reshape(cst, shape) -> cst
    reshape(reshape(X, _), [shape]) -> reshape(X, [shape])
    select(broadcast(not(p)), t, f) => select(broadcast(p), f, t)
    select(not(p), t, f) => select(p, f, t)
    shape_of(dynamic_reshape(X, shape)) -> shape
    slice(X, [A:A], [B:B], ...) -> X
    slice(concat(X,Y,Z,...),...) -> concat(slice(X),slice(Y),slice(Z))
    sort(X) -> sort(X, dim = N) [when dim can be inferred]
    sort(X,Y) -> sort(X) [if Y unused and unused in comparator]
    subtract(X, 0) -> X
    subtract(X, X) -> 0
    transpose(X, [iota...]) -> X
    transpose(X, [no_mem_layout_change...]) -> reshape(X)
    tuple(get_tuple_element(X, 0), get_tuple_element(X, 1), ...) -> X
    while -> while (loop invariants as implicit captures)
    xor(cst, X) -> xor(X, cst)
    op(X : zero_extent_tensor) -> constant([])
    ```

[33mcommit 27c10815eaaf42d65f0fa60070ccbfa24eb41f4a[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Oct 30 22:39:19 2024

    Bump patch version after integrate 1.8.1 -> 1.8.2 (#2610)

[33mcommit c32f7c215d9bed2d4077a7f884781f02e61df292[m
Author: srcarroll <50210727+srcarroll@users.noreply.github.com>
Date:   Wed Oct 30 16:40:17 2024

    [legalize-linalg] Simple constant propagation for elementwise operands (#2609)
    
    For elementwise operations, infer splat constant operands to avoid
    creating corresponding operands in the converted linalg op. Moreover, to
    facilitate this, the rewrite patterns `PointwiseToLinalgMapConverter`
    and `PointwiseToLinalgConverter` have been refactored to reuse common
    logic.

[33mcommit 9edd9cf09458351aac4615cdacb1ec615784e2a6[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Oct 30 15:06:15 2024

    Add stablehlo-translate interpreter arg passing, complex types in reference APIs (#2600)
    
    Two main changes:
    1. Support complex types in Reference APIs
    2. Add support for passing arguments when using `stablehlo-translate`
    
    To avoid inventing anything fancy, I've leveraged existing MLIR assembly
    printing/parsing:
    
    ```
    stablehlo-translate myfile.mlir --interpret --args="[dense<1> : tensor<2xi32>, dense<2> : tensor<2xi32>]"
    ```

[33mcommit 7920ed06053fbfd83b82c49a6bfd24680d2df376[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Tue Oct 29 21:06:50 2024

    Bump WEEK_12 to v1.5.0, WEEK_4 to 1.7.5 (#2599)
    
    WEEK_4: v1.7.5 2024-09-26 28 days ago
    WEEK_12: v1.5.0 2024-08-01 84 days ago
    
    
    [get_compatibility_versions.py](https://gist.github.com/apivovarov/e5f949983feff1fc378d93f3008a9900)

[33mcommit de5103d9613143d754519745673ea17b1cc78437[m[33m ([m[1;33mtag: [m[1;33mv1.8.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Oct 29 16:24:36 2024

    Integrate LLVM at llvm/llvm-project@ac4bd74190fe (#2606)

[33mcommit acc379abbdcf1bab70b9abf259698bb4b5778bb3[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Oct 28 23:55:54 2024

    Bump patch version after integrate 1.8.0 -> 1.8.1 (#2605)

[33mcommit 6e403b1aa6a71f5eaa09cc720e4ad42f692745e6[m[33m ([m[1;33mtag: [m[1;33mv1.8.0[m[33m)[m
Author: Max191 <44243577+Max191@users.noreply.github.com>
Date:   Fri Oct 25 19:19:28 2024

    Integrate LLVM at llvm/llvm-project@6c64c8a6f3f7 (#2602)

[33mcommit 5ae7716ab0c7a01174d80ad114a0672604e30102[m
Author: Sergey Kozub <skozub@nvidia.com>
Date:   Thu Oct 24 16:17:12 2024

    Add MX floating point types (f4E2M1FN, f6E2M3FN, f6E3M2FN, f8E8M0FNU) (#2582)
    
    This PR is adding MX (microscaling) floating point types to StableHLO.
    
    The RFC is created in https://github.com/openxla/stablehlo/pull/2581
    
    Bumps VHLO version to 1.8.0

[33mcommit 7c06dd2defe406f6a2a4ba360a3d91b1a2cfcf60[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Thu Oct 24 16:16:14 2024

    Bump WEEK_12 to v1.4.2, WEEK_4 to 1.7.3 (#2596)
    
    WEEK_4: v1.7.3 2024-09-23 28 days ago
    WEEK_12: v1.4.2 2024-07-25 88 days ago
    
    
    [get_compatibility_versions.py](https://gist.github.com/apivovarov/e5f949983feff1fc378d93f3008a9900)
    
    BTW, WEEK_12 will be v1.5.0 in 3 days on Oct 24

[33mcommit 7ce6c1c562a3446cb7147ead44aace12c3d276e7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Oct 23 21:50:50 2024

    Fix issue when targeting a patch version for serialization (#2598)
    
    We want to allow targeting patch version because we allow
    `-target=current` so it makes sense that current can resolve to a patch
    version.
    
    That said we write our compat passes against patch v0 since all opset
    changes require a minor version bump and therefore have patch v0.
    
    This PR implements "version flooring" inside the vhlo-to-version pass.

[33mcommit 2952108ddcd07ecafa3831598206fdbbeffe8a7a[m
Author: Sergey Kozub <skozub@nvidia.com>
Date:   Wed Oct 23 19:38:47 2024

    [RFC] Microscaling data types (f4E2M1FN, f6E2M3FN, f6E3M2FN, f8E8M0FNU) (#2581)
    
    This is a proposal to add MX (microscaling) floating point types to
    StableHLO.
    
    Related links:
    - StableHLO [PR#2582](https://github.com/openxla/stablehlo/pull/2582)
    Add MX floating point types (f4E2M1FN, f6E2M3FN, f6E3M2FN, f8E8M0FNU)
    - LLVM [PR#95392](https://github.com/llvm/llvm-project/pull/95392)
    [APFloat] Add APFloat support for FP4 data type
    - LLVM [PR#94735](https://github.com/llvm/llvm-project/pull/94735)
    [APFloat] Add APFloat support for FP6 data types
    - LLVM [PR#107127](https://github.com/llvm/llvm-project/pull/107127)
    [APFloat] Add APFloat support for E8M0 type
    - LLVM [PR#108877](https://github.com/llvm/llvm-project/pull/108877)
    [MLIR] Add f4E2M1FN type
    - LLVM [PR#107999](https://github.com/llvm/llvm-project/pull/107999)
    [MLIR] Add f6E2M3FN type
    - LLVM [PR#105573](https://github.com/llvm/llvm-project/pull/105573)
    [MLIR] Add f6E3M2FN type
    - LLVM [PR#111028](https://github.com/llvm/llvm-project/pull/111028)
    [MLIR] Add f8E8M0FNU type
    - JAX-ML [PR#181](https://github.com/jax-ml/ml_dtypes/pull/181) Add
    sub-byte data types: float4_e2m1fn, float6_e2m3fn, float6_e3m2fn
    - JAX-ML [PR#166](https://github.com/jax-ml/ml_dtypes/pull/181) Add
    float8_e8m0_fnu (E8M0) OCP MX scale format

[33mcommit 1c0b606503aac4f8e01f5511b0a10418bf93e7a6[m[33m ([m[1;33mtag: [m[1;33mv1.7.9[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Oct 21 17:49:29 2024

    Integrate LLVM at llvm/llvm-project@12bcea3292a1 (#2595)

[33mcommit a3e6892312ff721f0b111772de04a1ce6bcc6e78[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Oct 18 16:44:17 2024

    Revamp StableHLO simplification paterns. (#2586)
    
    Starting to look into porting new simplification patterns from MHLO to
    StableHLO and wanted to clean things up in the process to make StableHLO
    simplification/folders an overall better place to innovate:
    
    - Leverage DRR when possible. This is usually possible when the pattern
    isn't super complicated or the op being built doesn't have a region.
    - Order simplification patterns by op alphabetically, order tests
    alphabetically, ensure a test per simplification pattern.
    - Add code comments for each pattern of the format `// Pattern: ...`
    which can be used in the future for generating documentation.
    - Move patterns which fold constants to folder pass.

[33mcommit 0e7fbe0c5ab03cbd8c5078a4ab1fe7c137a899f1[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Wed Oct 16 19:04:34 2024

    Bump WEEK_12=>v1.4.1, WEEK_4=>1.7.1 (#2589)
    
    Bump version in `Version::fromCompatibilityRequirement`
    - WEEK_12 => v1.4.1 (84 days ago)
    - WEEK_4 => 1.7.1 (35 days ago)

[33mcommit b9ca2387eccc53f7e8f87feebd800897ca2c41d2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Oct 16 19:04:13 2024

    Bump patch version after integrate 1.7.8 -> 1.7.9 (#2591)

[33mcommit c49487b26835b15ac21508c857df8ecf0ccf23e3[m
Author: Scott Todd <scott.todd0@gmail.com>
Date:   Wed Oct 16 18:10:48 2024

    Set `/bigobj` option on MSVC to fix object limit error. (#2594)
    
    This fixes downstream build errors tracked at
    https://github.com/iree-org/iree/issues/18785.
    
    Sample error message when building on Windows with MSVC:
    ```
    FAILED: llvm-external-projects/stablehlo/stablehlo/dialect/CMakeFiles/obj.VhloOps.dir/VhloOps.cpp.obj
    C:\ProgramData\Chocolatey\bin\ccache C:\PROGRA~1\MICROS~2\2022\ENTERP~1\VC\Tools\MSVC\1441~1.341\bin\Hostx64\x64\cl.exe  /nologo /TP -DGTEST_HAS_RTTI=0 -D_HAS_EXCEPTIONS=0 -D_USE_MATH_DEFINES -ID:\a\iree\iree\third_party\llvm-project\llvm\include -ID:\a\iree\iree\build-windows\llvm-project\include -ID:\a\iree\iree\third_party\llvm-project\mlir\include -ID:\a\iree\iree\third_party\stablehlo -ID:\a\iree\iree\build-windows\llvm-external-projects\stablehlo -ID:\a\iree\iree\third_party\llvm-project\lld\include -ID:\a\iree\iree\build-windows\llvm-project\tools\lld\include -external:I\..\mlir\include -external:ID:\a\iree\iree\build-windows\llvm-project\tools\mlir\include -external:W0 /DWIN32 /D_WINDOWS /EHsc /Z7 /O2 /Ob1  -std:c++17 -MD  /EHs-c- /GR- /showIncludes /Follvm-external-projects\stablehlo\stablehlo\dialect\CMakeFiles\obj.VhloOps.dir\VhloOps.cpp.obj /Fdllvm-external-projects\stablehlo\stablehlo\dialect\CMakeFiles\obj.VhloOps.dir\ /FS -c D:\a\iree\iree\third_party\stablehlo\stablehlo\dialect\VhloOps.cpp
    cl : Command line warning D9025 : overriding '/EHs' with '/EHs-'
    cl : Command line warning D9025 : overriding '/EHc' with '/EHc-'
    D:\a\iree\iree\third_party\stablehlo\stablehlo\dialect\VhloOps.cpp : fatal error C1128: number of sections exceeded object file format limit: compile with /bigobj
    ```
    
    I think our downstream Windows builds worked with
    https://github.com/openxla/stablehlo/pull/2573 but they do not work with
    https://github.com/openxla/stablehlo/pull/2575, without this change.
    Hooray for compiler options whack-a-mole.

[33mcommit e44958c8ac3178df0d69d46804960df8e92580b1[m[33m ([m[1;33mtag: [m[1;33mv1.7.8[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Oct 14 17:36:07 2024

    Integrate LLVM at llvm/llvm-project@efcfa6e71168 (#2587)

[33mcommit e0c426fbf9cc565cb11aa72e86b21b599e004312[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Fri Oct 11 15:05:03 2024

    Bump WEEK_12 compat to v1.3.0 (#2584)
    
    Bump 12W compat to v1.3.0 released on July 16, 2024
    
    Tags: https://github.com/openxla/stablehlo/tags
    
    12W is used in `xla::GetDefaultStablehloVersion()` which is used by
    `PjRtCApiCompiler::Compile()` xla/pjrt/pjrt_c_api_client.cc:2317

[33mcommit 15324c4d313556d0d1324a33ce66e9172534fd1d[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Wed Oct 9 14:17:00 2024

    Add EvalTranspose pattern to StablehloAggressiveFolder (#2570)
    
    This patch folds `stablehlo.transpose` operation with constant operand
    into `stablehlo.constant`.
    
    I have considered doing this by iterating over source index space, i.e.
    ```
    auto initialValue = *std::begin(data);
    SmallVector<ElementType> result(resultType.getNumElements(), initialValue);
    
    for (int64_t i = 0; i < operandType.getNumElements(); ++i) {
      auto srcDimIndex = delinearize(i, operandStrides);
      auto dstDimIndex = applyPermutation(srcDimIndex, permutation);
      auto dstLinearIndex = linearize(dstDimIndex, resultStrides);
      result[dstLinearIndex] = data[i];
    }
    ```
    but that requires preinitialization of result vector with some value,
    which is twice as slow on simple case:
    ```
    func.func @eval_transpose() -> (tensor<5000x80x30xi32>) {
      %0 = stablehlo.iota dim = 0 : tensor<30x80x5000xi32>
      %1 = stablehlo.transpose %0, dims = [2, 1, 0] : (tensor<30x80x5000xi32>) -> tensor<5000x80x30xi32>
      func.return %1 : tensor<5000x80x30xi32>
    }
    ```

[33mcommit 46d146850e1b5ae701f493c2d92875c757b85a7c[m
Author: Mike <FruitClover@gmail.com>
Date:   Tue Oct 8 21:41:46 2024

    Fix BroadcastInDimOps linalg lowering with compatible dim=1 (#2583)
    
    For the case of simple broadcast check that we don't have to extend
    dimentions, so skip convertion to BroadcastOp when only one of them is
    1.
    
    Error before:
    
    test.mlir:5:8: error: 'linalg.broadcast' op input dim 1 should match
    init dim 2. input: 1, init: 5
    %0 = stablehlo.broadcast_in_dim %arg, dims = [1, 2, 3] :
    (tensor<3x1x1xf32>) -> tensor<1x3x5x7xf32>
               ^
        test.mlir:5:8: note: see current operation:
        %1 = "linalg.broadcast"(%arg0, %0) <{dimensions = array<i64: 0>}> ({
        ^bb0(%arg1: f32, %arg2: f32):
          "linalg.yield"(%arg1) : (f32) -> ()
        }) : (tensor<3x1x1xf32>, tensor<1x3x5x7xf32>) -> tensor<1x3x5x7xf32>

[33mcommit 8c7d87b6f4d3e1d692afb94494530ed993b6eb59[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Oct 4 23:45:56 2024

    Bump patch version after integrate 1.7.7 -> 1.7.8 (#2580)

[33mcommit 44a9acf2de746ac1c6c885ba11b414f1d38637d5[m[33m ([m[1;33mtag: [m[1;33mv1.7.7[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Oct 4 16:38:38 2024

    Bump 4w compat to v1.7.0 release Sept 05, 2024 (#2579)
    
    fixes https://github.com/openxla/stablehlo/issues/2578
    
    It is safe to bump 4w compat to `1.6.0`. It is more that 4 weeks since
    it's release date (Aug 16, 2024).

[33mcommit c4959572749b3438e4a0c292dda71f13732dfee2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Oct 3 23:59:51 2024

    Cleanup: Remove duplicate quantization checks (#2566)
    
    The recent upstream
    [change](https://github.com/llvm/llvm-project/pull/100667) have
    introduced quantization checks that are
    already present in the StableHLO core library. This commit removes these
    duplicate
    checks to avoid redundancy and potential inconsistencies.
    
    
    |Checks proposed to be removed| StableHLO Code | Upstream MLIR |
    |-|-|-|
    | `channel-axis >= 0`|
    [cs](https://github.com/openxla/stablehlo/blob/1c0547f391dff5ac71d36dc20a916260afa78c61/stablehlo/dialect/Base.cpp#L795)
    |
    [cs](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantTypes.cpp#L399)
    |
    | scale within smallest and largest finite numbers determined by
    `expressed_type`|
    [cs](https://github.com/openxla/stablehlo/blob/1c0547f391dff5ac71d36dc20a916260afa78c61/stablehlo/dialect/Base.cpp#L765)
    |
    [cs1](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantTypes.cpp#L327)
    [cs2](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantTypes.cpp#L393C9-L393C45)
    |
    
    
    Note that StableHLO has checks like `quantization_dimension <
    rank(self)` and
    `dim(self, quantization_dimension) = size(scales)` implemented at
    [cs](https://github.com/openxla/stablehlo/blob/1c0547f391dff5ac71d36dc20a916260afa78c61/stablehlo/dialect/Base.cpp#L795).
    In upstream MLIR similar checks
    [cs](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantOps.cpp#L51)
    are encoded as part of
    [dcast](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantOps.cpp#L110)
    and
    [qcast](https://github.com/llvm/llvm-project/blob/96f37ae45310885e09195be09d9c05e1c1dff86b/mlir/lib/Dialect/Quant/IR/QuantOps.cpp#L139)
    ops and hence cannot be claimed as duplicate.
    
    related upstream clean-up
    https://github.com/llvm/llvm-project/pull/110604

[33mcommit 8dd667a74ea1a7330390a0ba343d7c4c6f2d834d[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Oct 3 23:11:04 2024

    Integrate LLVM at llvm/llvm-project@00128a20eec2 (#2577)

[33mcommit d40285ef3db0687e3f1e2bb0d716d748485a9739[m[33m ([m[1;32mllvm_revision_bump[m[33m)[m
Author: Scott Todd <scott.todd0@gmail.com>
Date:   Thu Oct 3 20:16:34 2024

    Fix MSVC build by defining `_USE_MATH_DEFINES` earlier. (#2576)
    
    The fix in https://github.com/openxla/stablehlo/pull/2254 is not
    sufficient because the first file that includes `<cmath>` is what
    determines which macros are defined.
    
    Downstream Windows builds broke at some point due to this:
    https://github.com/iree-org/iree/actions/runs/11159458848/job/31017884277#step:7:6258
    ```
    [5387/8614] Building CXX object llvm-external-projects\stablehlo\stablehlo\transforms\CMakeFiles\obj.StablehloPasses.dir\ChloLegalizeToStablehlo.cpp.obj
    FAILED: llvm-external-projects/stablehlo/stablehlo/transforms/CMakeFiles/obj.StablehloPasses.dir/ChloLegalizeToStablehlo.cpp.obj
    C:\ProgramData\Chocolatey\bin\ccache C:\PROGRA~1\MICROS~2\2022\ENTERP~1\VC\Tools\MSVC\1441~1.341\bin\Hostx64\x64\cl.exe  /nologo /TP -DGTEST_HAS_RTTI=0 -DUNICODE -D_CRT_NONSTDC_NO_DEPRECATE -D_CRT_NONSTDC_NO_WARNINGS -D_CRT_SECURE_NO_DEPRECATE -D_CRT_SECURE_NO_WARNINGS -D_HAS_EXCEPTIONS=0 -D_SCL_SECURE_NO_DEPRECATE -D_SCL_SECURE_NO_WARNINGS -D_UNICODE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -ID:\a\iree\iree\third_party\llvm-project\llvm\include -ID:\a\iree\iree\build-windows\llvm-project\include -ID:\a\iree\iree\third_party\llvm-project\mlir\include -ID:\a\iree\iree\third_party\stablehlo -ID:\a\iree\iree\build-windows\llvm-external-projects\stablehlo -ID:\a\iree\iree\third_party\llvm-project\lld\include -ID:\a\iree\iree\build-windows\llvm-project\tools\lld\include -external:I\..\mlir\include -external:ID:\a\iree\iree\build-windows\llvm-project\tools\mlir\include -external:W0 /DWIN32 /D_WINDOWS   /Zc:inline /Zc:preprocessor /Zc:__cplusplus /Oi /bigobj /permissive- -wd4141 -wd4146 -wd4244 -wd4267 -wd4291 -wd4351 -wd4456 -wd4457 -wd4458 -wd4459 -wd4503 -wd4624 -wd4722 -wd4100 -wd4127 -wd4512 -wd4505 -wd4610 -wd4510 -wd4702 -wd4245 -wd4706 -wd4310 -wd4701 -wd4703 -wd4389 -wd4611 -wd4805 -wd4204 -wd4577 -wd4091 -wd4592 -wd4319 -wd4709 -wd5105 -wd4324 -wd4251 -wd4275 -w14062 -we4238 /Gw /Z7 /O2 /Ob1  -std:c++17 -MD  /EHs-c- /GR- /showIncludes /Follvm-external-projects\stablehlo\stablehlo\transforms\CMakeFiles\obj.StablehloPasses.dir\ChloLegalizeToStablehlo.cpp.obj /Fdllvm-external-projects\stablehlo\stablehlo\transforms\CMakeFiles\obj.StablehloPasses.dir\ /FS -c D:\a\iree\iree\third_party\stablehlo\stablehlo\transforms\ChloLegalizeToStablehlo.cpp
    D:\a\iree\iree\third_party\stablehlo\stablehlo\transforms\ChloLegalizeToStablehlo.cpp(1324): error C2065: 'M_PI': undeclared identifier
    D:\a\iree\iree\third_party\stablehlo\stablehlo\transforms\ChloLegalizeToStablehlo.cpp(1324): error C2660: 'mlir::stablehlo::getConstantLike': function does not take 3 arguments
    D:\a\iree\iree\third_party\stablehlo\stablehlo/transforms/PassUtils.h(51): note: see declaration of 'mlir::stablehlo::getConstantLike'
    ```

[33mcommit 5f3c287dee4553907161e7a828131cbae3671083[m
Author: Benoit Jacob <jacob.benoit.1@gmail.com>
Date:   Thu Oct 3 19:54:01 2024

    CMake: Do not specify `LLVMSupport` in `LINK_LIBS`. (#2575)
    
    This is a new take on https://github.com/openxla/stablehlo/pull/2573.
    
    There, I was trying to fix CMake errors in the external build, with the
    error message saying that `LINK_COMPONENTS` should be used instead of
    `LINK_LIBS` for LLVM libraries. That change fixed my external build, but
    broke the standalone build on CI.
    
    Reading the `AddMLIR.cmake` code generating the error, I came across
    this line:
    
    https://github.com/llvm/llvm-project/blob/ee4dd147baff8f971f3ec5aad5a216ca9837a732/mlir/cmake/modules/AddMLIR.cmake#L287-L288
    
    ```cmake
      # MLIR libraries uniformly depend on LLVMSupport.  Just specify it once here.
      list(APPEND ARG_LINK_COMPONENTS Support)
    ```
    
    This looks like hardcoding always depending on `LLVMSupport` (though I
    wasn't quite sure about the nuance between `Support` and `LLVMSupport`).
    If that's correct, then we never needed specifying `LLVMSupport` in the
    first place. So I tried just omitting it, and that seems to work. WDYT?
    
    ---------
    
    Signed-off-by: Benoit Jacob <jacob.benoit.1@gmail.com>

[33mcommit 24a3664c70148a5abc6ded4845d1e291b922a004[m
Author: Benoit Jacob <jacob.benoit.1@gmail.com>
Date:   Thu Oct 3 14:16:06 2024

    Always include LLVM CMake modules. (#2572)
    
    These LLVM CMake modules used to be unconditionally included, but #2430
    changed that to only conditionally including them in the standalone and
    embedded builds, leaving out the external-project build.
    
    That broke integration in https://github.com/iree-org/iree where
    Stablehlo is an external project. [Error log
    here](https://gist.github.com/bjacob/17f86fb5f07007e06b94f3a9347747a1).
    These are linking errors when building with Clang or GCC, with
    unresolved symbols for `typeinfo` for various types, referenced from
    Stablehlo symbols. `typeinfo` is a RTTI feature, and all LLVM-derived
    projects normally disable RTTI, which is why these symbols are not
    defined, and why that is normally not a problem. What changed with #2430
    was that suddenly, the disabling of RTTI was itself disabled in our
    external build of Stablehlo. [Here is the
    code](https://github.com/llvm/llvm-project/blob/09ba83be0ac178851e3c9c9c8fefddbdd4d8353f/llvm/cmake/modules/AddLLVM.cmake#L63-L64)
    that is responsible for disabling RTTI. It was not kicking in anymore,
    because `LLVM_COMPILER_IS_GCC_COMPATIBLE` was not defined. It is
    normally defined by this `include` [just
    above](https://github.com/llvm/llvm-project/blob/09ba83be0ac178851e3c9c9c8fefddbdd4d8353f/llvm/cmake/modules/AddLLVM.cmake#L5)
    in that file.
    
    But as of #2430, the `include(AddLLVM)` itself was no longer done in the
    Stablehlo project, instead the Stablehlo external build was relying on
    `include(AddLLVM)` to have been done in the parent project, which it
    was. The problem was that while the parent-project `include(AddLLVM)`
    did define all the functions provided by that module, the CMake variable
    definitions such as `LLVM_COMPILER_IS_GCC_COMPATIBLE` were made on the
    parent project and were not visible to the child project.
    
    Summary of the above: in their current shape, LLVM CMake modules such as
    `AddLLVM` (and `AddMLIR`, which Stablehlo uses, and which itself relies
    on `AddLLVM`) must be included by any child project that uses them, even
    if the parent project already included them.
    
    Signed-off-by: Benoit Jacob <jacob.benoit.1@gmail.com>

[33mcommit 139e61fc4012fe33020a443c1ae8901fd3327b1e[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Oct 2 22:48:59 2024

    Bump patch version after integrate 1.7.6 -> 1.7.7 (#2574)

[33mcommit 7b13f60e59adc17abed6624ab5f13c75a31284ce[m[33m ([m[1;32mversion-bump-1002[m[33m, [m[1;32mversion-bump[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Oct 1 22:30:29 2024

    Rename  pass testfile stablehlo_create_compat....mlir to stablehlo_compat....mlir   (#2571)
    
    pass is renamed

[33mcommit f7f8e4e35296deeff2e12e39421ac8d9599ba340[m[33m ([m[1;33mtag: [m[1;33mv1.7.6[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Oct 1 21:00:52 2024

    apply patch for compatibility expander to support gather/scatter batc‚Ä¶ (#2568)
    
    ‚Ä¶h dims
    
    ---------
    
    Co-authored-by: Tom Natan <tomnatan@google.com>

[33mcommit 9e407a9464b7d022ea886e0b194198e7df1e53f9[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Oct 1 19:35:29 2024

    Integrate LLVM at llvm/llvm-project@ac2a2816e3fe (#2567)

[33mcommit c6c158323db8075b1b06921ee5c6a2cc1fb5c78b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Oct 1 18:50:39 2024

    Improve forward incompatibility error messaging (#2569)
    
    Now failure to `--deserialize` a portable artifact will include:
    1. The op that failed.
    2. The version that the portable artifact was serialized for.
    3. The current version of StableHLO.
    
    This should help debugging compat issues at a glance.
    
    Added a forward incompatible hypothetical feature `vhlo.constant_v99`,
    serialized for `StableHLO_v2.0.0`, which emulates a current version of
    StableHLO trying to parse a future operation that doesn't currently
    exist.
    
    ```
    $ stablehlo-translate --deserialize file.mlirbc
    unregistered operation 'vhlo.constant_v99' found in dialect ('vhlo') that does not allow unknown operations
    note: in bytecode version 6 produced by: StableHLO_v2.0.0
    failed to deserialize portable artifact using StableHLO_v1.7.5
    ```

[33mcommit dd94939fb770a716759c2d303bd1bc5ea946d8b3[m[33m ([m[1;32malltogether[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Oct 1 17:49:41 2024

    Add ide.md to TOC (#2565)
    
    The page exists at: https://openxla.org/stablehlo/ide
    
    But it isn't mentioned in `_toc.yaml` so it is missing the side bar on
    the page, and it doesn't show up in the typical nav bar.

[33mcommit 1c0547f391dff5ac71d36dc20a916260afa78c61[m[33m ([m[1;32mmlir_bump[m[33m)[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Mon Sep 30 16:48:28 2024

    Support all static tensor shapes in EvalSlice folder (#2557)
    
    This patch enhances the EvalSlice constant folder to support all
    operator configurations on static tensor shapes, including 0-rank
    scalars.

[33mcommit 75bac79130c8ff2339b51f38bf4c396dbc389965[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 27 19:45:28 2024

    Add documentation on dynamism in StableHLO (#2564)
    
    Added `dynamism.md` with information on dynamic shape handling in
    StableHLO.
    
    A few other cleanups found while adding documentation:
    - Added examples to argument refinement and canonicalize dynamism.
    - Ordered passes alphabetically
    - Rename `StablehloCreateCompatibilityExpanderPatterns` ->
    `StablehloCompatibilityExpanderPatterns`

[33mcommit ff13c96e56b73c62dcbb5b34b69f5ece9e71322f[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Sep 26 22:51:27 2024

    Bump patch version after integrate 1.7.5 -> 1.7.6 (#2563)

[33mcommit e398b224ccdf578ee144b45d7d422b53e008d4aa[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 26 18:41:57 2024

    Add StableHLO Landing Image (#2561)
    
    Some images I made for a video we recorded. Thought it would be a nice
    addition to the landing readme.
    
    Preview:
    
    https://github.com/openxla/stablehlo/blob/f77e72531bc1264e23ef18e12e7ba3bd29bb1cfb/README.md

[33mcommit 9d9290dc2308c1850cea69ea05f8c94017e484ee[m[33m ([m[1;33mtag: [m[1;33mv1.7.5[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Sep 26 17:53:41 2024

    Integrate LLVM at llvm/llvm-project@29b92d07746f (#2562)

[33mcommit 49fd792dcd789fea6d13e3dd721e1b00de11804c[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Sep 25 21:58:47 2024

    Bump patch version after integrate 1.7.4 -> 1.7.5 (#2560)

[33mcommit ca13d31b5ed0b2053dde0a624480ad765e219ebf[m[33m ([m[1;33mtag: [m[1;33mv1.7.4[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Sep 24 21:20:51 2024

    Integrate LLVM at llvm/llvm-project@42b696d7b994 (#2558)

[33mcommit 165cd60cfa1a154c1c10aea0cbaa2f6595d72054[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Sep 23 17:12:54 2024

    Bump patch version after integrate 1.7.3 -> 1.7.4 (#2556)

[33mcommit 9bb28f84c281795783639364b727e4398dcec570[m[33m ([m[1;33mtag: [m[1;33mv1.7.3[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Sep 20 22:32:14 2024

    Add an expander pattern for GatherOp/ScatterOp with batching dims (#2555)
    
    porting to gh
    https://github.com/openxla/xla/commit/10f3a172a9973846e45d193ac9098d40b0ca156f

[33mcommit 728d62586f8c061e2dd8180fe5fc27081c117fe7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 20 19:59:38 2024

    Integrate LLVM at llvm/llvm-project@94c024adedcb (#2554)

[33mcommit 376dbf3a5967c9dd1d6999254accf88bb443cb04[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Sep 19 00:14:38 2024

    Clean-up duplicate functions from pass dir (#2553)
    
    use PassUtil instead

[33mcommit 449d2763197ef772ef246e9a9940593a4508e8bb[m
Author: srcarroll <50210727+srcarroll@users.noreply.github.com>
Date:   Wed Sep 18 18:09:58 2024

    Enable `stablehlo.concatenate` to `tensor.concat` conversion (#2552)
    
    When `enable-primitive-ops` is true, emit `tensor.concat`.

[33mcommit 1cd2d260bd230d4122e4b53c33e39fed1542013e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 18 00:05:24 2024

    Bump patch version after integrate 1.7.2 -> 1.7.3 (#2550)

[33mcommit 78c753ad13ad8205cacc5fcc12418c1ac97276c7[m[33m ([m[1;33mtag: [m[1;33mv1.7.2[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 17 00:16:21 2024

    Integrate LLVM at llvm/llvm-project@36adf8ecedb6 (#2548)

[33mcommit 81e2cffcf47cae59b8e108e4fb57a949597f80c4[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Sep 16 21:20:53 2024

    Fix typo from a comment for WEEK_4 compat requirement (#2549)

[33mcommit e05a320906c23313e7d9150cd27c15543591031e[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Sep 16 18:21:54 2024

    Add "PassUtils" for common functions to avoid duplicate definitions (#2547)
    
    [back-porting to
    gh](https://github.com/openxla/xla/commit/5a07f58bb626dec8ad17c6a7998d5410401d6d93)

[33mcommit 7f88b35fe1e914e9b0726835439659f3047edc71[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Sep 16 17:01:31 2024

    Bump 4w and 12w version numbers (#2545)
    
    For 4w compat, enables use of TanOp. For 12w, enables use of
    gather/scatter batch dims.
    
    Co-authored-by: Tom Natan <tomnatan@google.com>

[33mcommit 7e89e4cbbcbbc12dfdaffceebb230a7258e1f44e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 12 21:41:38 2024

    Fix Quant grammar (#2541)
    
    fixes https://github.com/openxla/stablehlo/issues/2539

[33mcommit 5790920cd636a5094d1c650d64c65c64a037dc2d[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Sep 12 18:26:30 2024

    Fix typo:  known_non_expanding_dimensions -> known_nonexpanding_dimensions (#2540)
    
    fixes https://github.com/openxla/stablehlo/issues/2535

[33mcommit c15b1e9025708bf690190fae32074d5f6686b000[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 12 18:24:15 2024

    Delete empty testdata files (#2536)
    
    I'm not sure why these were generated as empty files, but they're not
    useful so lets delete them. I'll look into how these were generated
    separately.
    
    Notified of these as @jtristan applies the
    https://github.com/leanprover/SHerLOC project to testdata files.
    
    Fixes https://github.com/openxla/stablehlo/issues/2537, confirmed that
    there are 38 lines in that issue and 38 files removed in this PR.

[33mcommit 73b0862c56facf3b6ba10a5f00ef074261a86e54[m
Author: Tai Ly <tai.ly@arm.com>
Date:   Thu Sep 12 00:56:57 2024

    Enhance stablehlo-quant-legalize-to-tosa-rescale (#2534)
    
    by adding support for following quantized operators:
      -  abs
      -  multiply and divide
      -  maximum and minimum
      -  compare
    
    ---------
    
    Signed-off-by: Tai Ly <tai.ly@arm.com>

[33mcommit 5290683f3ac4ffedebe3c7c6b53f02bcbf9465a6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Sep 9 17:05:36 2024

    Bump patch version after integrate 1.7.1 -> 1.7.2 (#2533)

[33mcommit 252c1a9aa044be64ca7c5f9047cf6269b6281584[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Sep 6 23:48:47 2024

    Update VhloToVersionPass  description (#2532)
    
    fixes https://github.com/openxla/stablehlo/issues/2524
    discussed with Kevin,  don't need to support `'target=current'`

[33mcommit e51fd95e5b2c28861f22dc9d609fb2a7f002124e[m[33m ([m[1;33mtag: [m[1;33mv1.7.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Sep 6 22:27:44 2024

    Integrate LLVM at llvm/llvm-project@eaa95a1c2bd3 (#2531)

[33mcommit c6670894e07eab91a875501f2931bcbf5fae519a[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Sep 6 22:09:32 2024

    Fix JAX windows ci build error : missing stablehlo C API symbols (#2530)
    
    Issue: JAX windows build is failing because of missing stablehlo C API
    symbols
    https://github.com/google/jax/actions/runs/10739809804/job/29786443538
    
    root cause: `StablehloApi.h` defs has function overloads. Compiler does
    name mangling (decorating function names with additional information).
    These symbols are missing in JAX APIs, JAX only allowlist symbol exports
    for symbols starting with `stablehlo`, but the mangled c++ names don't
    have that property.
    
    fix:
    1. add `extern "C" ` around stablehloapi.h declarations. extern "C"
    instructs the compiler to suppress the mangling.
    2. rename functions to avoid function overloading
    
    
    thank you @hawkinsp  for help with root causing and validating the fix.

[33mcommit 3cc3767e0a3c12b72d2e81305f0f1799420215ec[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Sep 5 22:04:17 2024

    Bump patch version after integrate 1.7.0 -> 1.7.1 (#2529)

[33mcommit 1313d379e4d4a174a1828ebb4b177dd8c2121d9a[m
Author: Peter Hawkins <hawkinsp@cs.stanford.edu>
Date:   Thu Sep 5 17:33:53 2024

    Fix stablehlo_capi_objects to depend on :CAPIIRObjects, not :CAPIIR. (#2528)
    
    Depending on :CAPIIR from a capi_objects target causes duplicate symbol
    problems in JAX on Windows.

[33mcommit 932b32f619fed74431d32df2478af4039fc4873f[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Thu Sep 5 17:12:23 2024

    Improve the accuracy of complex atanh and atan (#2513)
    
    As in the title.
    
    The accuracy improvements are as follows (using ca 1000000 samples over
    the entire complex plane):
    
    - complex64 atanh:
      ```
                               current main  -> this PR
      ULP difference == 0 count is 257275    -> 868955
      ULP difference == 1 count is 24034     -> 130206
      ULP difference == 2 count is 1280      -> 2832
      ULP difference == 3 count is 2903      -> 12
      ULP difference >= 4 count is 716513    -> 0
      ```
    - complex64 atan:
      ```
      ULP difference == 0 count is 3326      -> 868953
      ULP difference == 1 count is 6970      -> 130204
      ULP difference == 2 count is 3384      -> 2832
      ULP difference == 3 count is 3418      -> 12
      ULP difference >= 4 count is 984903    -> 0
      ```
    - complex128 atanh:
      ```
      ULP difference == 0 count is 239712    -> 941283
      ULP difference == 1 count is 2853      -> 60322
      ULP difference == 2 count is 616       -> 400
      ULP difference == 3 count is 8         -> 0
      ULP difference >= 4 count is 758816    -> 0
      ```
    
    This PR requires functional_algorithms 0.10.1 or newer.

[33mcommit 4f31b2e7632d91704c2dae0fb8c1c9e108289409[m[33m ([m[1;33mtag: [m[1;33mv1.7.0[m[33m)[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Thu Sep 5 01:28:12 2024

    Fix some warnings (#2527)
    
    This PR fixes the following warnings:
    ```
    stablehlo/stablehlo/dialect/Version.cpp:89:1:
      warning: control reaches end of non-void function [-Wreturn-type]
    ```
    
    ```
    stablehlo/stablehlo/transforms/StablehloAggressiveFolder.cpp:315:59:
      warning: 'result' may be used uninitialized in this function [-Wmaybe-uninitialized]
      315 |       return getAPSInt(resultType.getElementType(), result);
    ```

[33mcommit 974ba7b7c2a787ecd4e5eb52ea9355f3eea5db2b[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Sep 4 21:16:00 2024

    Integrate LLVM at llvm/llvm-project@6b4b8dc4a45d (#2526)

[33mcommit ed8c91ea8395cc0db707301f6bf953b1029656e4[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Wed Sep 4 14:25:19 2024

    Add f8E4M3 and f8E3M4 types support (#2482)
    
    This PR adds f8E4M3 and f8E3M4 types support.
    
    f8E4M3 and f8E3M4 types follow IEEE 754 convention.
    
    ```c
    f8E4M3 (IEEE 754)
    - Exponent bias: 7
    - Maximum stored exponent value: 14 (binary 1110)
    - Maximum unbiased exponent value: 14 - 7 = 7
    - Minimum stored exponent value: 1 (binary 0001)
    - Minimum unbiased exponent value: 1 ‚àí 7 = ‚àí6
    - Precision specifies the total number of bits used for the significand (mantisa),
        including implicit leading integer bit = 3 + 1 = 4
    - Follows IEEE 754 conventions for representation of special values
    - Has Positive and Negative zero
    - Has Positive and Negative infinity
    - Has NaNs
    
    Additional details:
    - Max exp (unbiased): 7
    - Min exp (unbiased): -6
    - Infinities (+/-): S.1111.000
    - Zeros (+/-): S.0000.000
    - NaNs: S.1111.{001, 010, 011, 100, 101, 110, 111}
    - Max normal number: S.1110.111 = +/-2^(7) x (1 + 0.875) = +/-240
    - Min normal number: S.0001.000 = +/-2^(-6)
    - Max subnormal number: S.0000.111 = +/-2^(-6) x 0.875 = +/-2^(-9) x 7
    - Min subnormal number: S.0000.001 = +/-2^(-6) x 0.125 = +/-2^(-9)
    ```
    
    ```c
    f8E3M4 (IEEE 754)
    - Exponent bias: 3
    - Maximum stored exponent value: 6 (binary 110)
    - Maximum unbiased exponent value: 6 - 3 = 3
    - Minimum stored exponent value: 1 (binary 001)
    - Minimum unbiased exponent value: 1 ‚àí 3 = ‚àí2
    - Precision specifies the total number of bits used for the significand (mantissa),
        including implicit leading integer bit = 4 + 1 = 5
    - Follows IEEE 754 conventions for representation of special values
    - Has Positive and Negative zero
    - Has Positive and Negative infinity
    - Has NaNs
    
    Additional details:
    - Max exp (unbiased): 3
    - Min exp (unbiased): -2
    - Infinities (+/-): S.111.0000
    - Zeros (+/-): S.000.0000
    - NaNs: S.111.{0,1}‚Å¥ except S.111.0000
    - Max normal number: S.110.1111 = +/-2^(6-3) x (1 + 15/16) = +/-2^3 x 31 x 2^(-4) = +/-15.5
    - Min normal number: S.001.0000 = +/-2^(1-3) x (1 + 0) = +/-2^(-2)
    - Max subnormal number: S.000.1111 = +/-2^(-2) x 15/16 = +/-2^(-2) x 15 x 2^(-4) = +/-15 x 2^(-6)
    - Min subnormal number: S.000.0001 = +/-2^(-2) x 1/16 =  +/-2^(-2) x 2^(-4) = +/-2^(-6)
    ```
    
    Related PRs:
    - LLVM [PR-97179](https://github.com/llvm/llvm-project/pull/97179)
    [APFloat] Add support for f8E4M3 IEEE 754 type (Merged)
    - LLVM [PR-97118](https://github.com/llvm/llvm-project/pull/97118)
    [MLIR] Add f8E4M3 IEEE 754 type (Merged)
    - LLVM [PR-99698](https://github.com/llvm/llvm-project/pull/99698)
    [APFloat] Add support for f8E3M4 IEEE 754 type (Merged)
    - LLVM [PR-101230](https://github.com/llvm/llvm-project/pull/101230)
    [MLIR] Add f8E3M4 IEEE 754 type (Merged)
    - StableHLO [PR-2486](https://github.com/openxla/stablehlo/pull/2486)
    [RFC] Add f8E4M3 and f8E3M4 types support
    - ml_dtypes [PR-161](https://github.com/jax-ml/ml_dtypes/pull/161/) Add
    float8_e4m3 (Merged)
    - ml_dtypes [PR-171](https://github.com/jax-ml/ml_dtypes/pull/171/) Add
    float8_e3m4 (Merged)
    - XLA [PR-16585](https://github.com/openxla/xla/pull/16585) Add support
    for float8_e4m3

[33mcommit dd97e7acf3a1e2f8921e38588b59404116aa4707[m[33m ([m[1;32mpass-updates[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Sep 4 00:33:21 2024

    Move StableHLO  passes test files to a dedicated ./transforms dir (#2523)

[33mcommit 5cd228d87a85cd8ab7bec05951094774a683944c[m[33m ([m[1;32mvhlo_to_pass[m[33m)[m
Author: Christopher Bate <cbate@nvidia.com>
Date:   Tue Sep 3 23:43:24 2024

    [cmake] Remove dependence on LLVM 'Core' component in the CMake build (#2522)
    
    This change removes two spurious 'LINK_COMPONENTS Core' declarations in
    the CMake build. Depending on the 'Core' component indicates that the
    libraries depend on the code under 'llvm-project/llvm/lib/IR' (e.g.
    'LLVMCore'
    library), which is not true for the StablehloLinalgTransforms and
    StablehloTOSATransforms libraries.

[33mcommit d68ab0727fd2af96201146a14b09d72f9b7ad7f6[m[33m ([m[1;32mpass_enhace[m[33m)[m
Author: Alexander Pivovarov <pivovaa@amazon.com>
Date:   Tue Sep 3 19:20:58 2024

    [RFC] Add f8E4M3 and f8E3M4 types support (#2486)
    
    ### Summary
    This is a proposal to add `Float8E4M3` and `Float8E3M4` floating point
    types to StableHLO.
    Feedback welcome, see [RFC: Float8E4M3 and
    Float8E3M4](https://github.com/apivovarov/stablehlo/blob/rfc_f8E4M3_f8E3M4/rfcs/20240808-f8E4M3_f8E3M4.md)
    for more details.
    
    ### References and Links
    - LLVM [PR-97179](https://github.com/llvm/llvm-project/pull/97179)
    [APFloat] Add support for f8E4M3 IEEE 754 type (Merged)
    - LLVM [PR-97118](https://github.com/llvm/llvm-project/pull/97118)
    [MLIR] Add f8E4M3 IEEE 754 type (Merged)
    - LLVM [PR-99698](https://github.com/llvm/llvm-project/pull/99698)
    [APFloat] Add support for f8E3M4 IEEE 754 type (Merged)
    - LLVM [PR-101230](https://github.com/llvm/llvm-project/pull/101230)
    [MLIR] Add f8E3M4 IEEE 754 type (Merged)
    - [RFC: FP8 in
    StableHLO](https://github.com/openxla/stablehlo/blob/main/rfcs/20221031-fp8.md)
    - [RFC: Float8E4M3FNUZ and
    Float8E5M2FNUZ](https://github.com/openxla/stablehlo/blob/main/rfcs/20230321-fp8_fnuz.md)
    - StableHLO [PR-2482](https://github.com/openxla/stablehlo/pull/2482)
    Add f8E4M3 and f8E3M4 types support
    - [Amazon EC2 Trn1
    Instances](https://aws.amazon.com/ec2/instance-types/trn1/)
    - ml_dtypes [PR-161](https://github.com/jax-ml/ml_dtypes/pull/161/) Add
    float8_e4m3 (Merged)
    - ml_dtypes [PR-171](https://github.com/jax-ml/ml_dtypes/pull/171/) Add
    float8_e3m4 (Merged)
    - XLA [PR-16585](https://github.com/openxla/xla/pull/16585) Add support
    for float8_e4m3

[33mcommit 1456dfa1e1a83aab0cc717714ba3695886f60302[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 3 18:47:13 2024

    Add DotAlgorithm to StableHLO Python API (#2521)

[33mcommit 21dcdd2caeb4e6b282a62b8920d8454303abbf8b[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Aug 30 23:23:49 2024

    Bump patch version after integrate 1.6.3 -> 1.6.4 (#2519)

[33mcommit 6d24f7c0445d68c0b61c4d754d0ae279abaf7b71[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Aug 30 19:57:41 2024

    Remove deprecated serialization APIs (#2517)

[33mcommit 2be7164ac6253f403b813a14dc87defd1591f511[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 30 18:37:10 2024

    Add StableHLOCreateCompatibilityExpanderPass (#2518)
    
    Backporting from g3..
    
    This pass expands backward compatibility with older StableHLO versions
    by decomposing newer StableHLO operations into equivalent operations
    supported by those older versions.

[33mcommit 54aa1a57178251981da616b877dda1a88d840d11[m[33m ([m[1;33mtag: [m[1;33mv1.6.3[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Aug 29 17:51:02 2024

    Integrate LLVM at llvm/llvm-project@f142f8afe21b (#2516)

[33mcommit cce025c8c1978ac99ed48735c9e9b9583cc50c8e[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Aug 28 22:42:30 2024

    Bump patch version after integrate 1.6.2 -> 1.6.3 (#2515)

[33mcommit 44d54f67fee7028e76427218610b3adb3d068c00[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Aug 28 22:24:49 2024

    Move python bindings to portable apis that use C-API datatypes. (#2514)
    
    Add `stablehlo.get_version_from_compatibility_requirement` API.

[33mcommit 371678eeb3995cf21b13c15947774ffaf02ab84a[m[33m ([m[1;33mtag: [m[1;33mv1.6.2[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Aug 27 22:27:38 2024

    Integrate LLVM at llvm/llvm-project@65281570afd7 (#2510)

[33mcommit 3bb40f72f39ca3822931ca2082cbc6991dd7f12b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 27 16:55:35 2024

    Add best effort validation for known dot algorithms (#2511)
    
    This is somewhat of ecosystem implementation details leaking into
    StableHLO, but that seems to be safer than allowing any arbitrary
    combination and enforcing that the user get some lucky combination
    correct.

[33mcommit 8602e098be0aaba038868b943963dd23141eb605[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Aug 26 23:32:10 2024

    StableHLO Testdata Refresh (#2441)
    
    # StableHLO Testdata Refresh
    
    _Note: Careful attempting to review.. this is all generated by a JAX
    script to dump their parameterized JAX primitive tests to StableHLO test
    cases, so the majority of this PR isn't interesting. I'll highlight some
    interesting bits below. The testgen script will remain an internal tool
    until it's cleaned up / validated a bit then we can upload it to
    build_tools._
    
    I noticed that we didn't have any CHLO tests to capture precision
    changes as we go about making CHLO decompositions more precise.
    
    ## High level overview
    
    ### Better names
    
    Better is relative.. but these names are now shorter and more readable,
    and shouldn't break any users with "max file length too long" anymore.
    
    ```
    $ ls stablehlo/testdata/
    abs_bfloat16_20_20.mlir
    abs_complex128_20_20.mlir
    abs_complex64_20_20.mlir
    abs_float16_20_20.mlir
    abs_float32_20_20.mlir
    abs_float64_20_20.mlir
    abs_int16_20_20.mlir
    abs_int32_20_20.mlir
    abs_int64_20_20.mlir
    abs_int8_20_20.mlir
    acos_bfloat16_20_20.mlir
    ...
    ```
    
    ### CustomCalls more closely model Check dialect ops
    
    Pretty much all the same, just no longer do the custom_call's return
    anything. This will allow us to write a trivial pass to convert to/from
    check ops, by more closely modeling check ops.
    
    ```
    // RUN: stablehlo-opt -inline %s | stablehlo-translate --interpret
    // RUN: stablehlo-translate --serialize --target=current %s | stablehlo-translate --deserialize | stablehlo-opt > %t.0
    // RUN: stablehlo-opt %s > %t.1
    // RUN: diff %t.0 %t.1
    
    module @jit_main attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
      func.func public @main() -> (tensor<20x20xbf16> {jax.result_info = "", mhlo.layout_mode = "default"}) {
        %0 = call @inputs() : () -> tensor<20x20xbf16>
        %1 = call @expected() : () -> tensor<20x20xbf16>
        %2 = stablehlo.abs %0 : tensor<20x20xbf16>
        stablehlo.custom_call @check.expect_close(%2, %1) {has_side_effect = true} : (tensor<20x20xbf16>, tensor<20x20xbf16>) -> ()
        return %2 : tensor<20x20xbf16>
      }
      func.func private @inputs() -> (tensor<20x20xbf16> {mhlo.layout_mode = "default"}) {
        %cst = stablehlo.constant dense<"__elided__"> : tensor<20x20xbf16>
        return %cst : tensor<20x20xbf16>
      }
      func.func private @expected() -> (tensor<20x20xbf16> {mhlo.layout_mode = "default"}) {
        %cst = stablehlo.constant dense<"__elided__"> : tensor<20x20xbf16>
        return %cst : tensor<20x20xbf16>
      }
    }
    ```
    
    ### New CHLO tests
    
    Now we have tests that leverage CHLO decompositions and compare them to
    reference semantics from JAX/XLA.
    
    ```
    $ cat stablehlo/testdata/acosh_bfloat16_20_20_chlo.mlir
    
    // RUN: stablehlo-opt --chlo-pre-serialization-pipeline -inline %s | stablehlo-translate --interpret
    ...
    module @jit_main attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
      func.func public @main() -> (tensor<20x20xbf16> {jax.result_info = "", mhlo.layout_mode = "default"}) {
        %0 = call @inputs() : () -> tensor<20x20xbf16>
        %1 = call @expected() : () -> tensor<20x20xbf16>
        %2 = chlo.acosh %0 : tensor<20x20xbf16> -> tensor<20x20xbf16>
        stablehlo.custom_call @check.expect_close(%2, %1) {has_side_effect = true} : (tensor<20x20xbf16>, tensor<20x20xbf16>) -> ()
        return %2 : tensor<20x20xbf16>
      }
      // ...
    }
    ```
    
    ## New bugs discovered ( üòÑ )
    
    Filtering a few sort-related bugs on #2440. _Note: All disabled tests
    contain `RUN-DISABLED(reason)`_
    
    ```
    $ grep -rnwl stablehlo/testdata -e 'RUN-DISABLED(#2440)'
    stablehlo/testdata/top_k_float32_5_chlo.mlir
    stablehlo/testdata/top_k_float64_5_3_chlo.mlir
    stablehlo/testdata/sort_int8_5_7.mlir
    stablehlo/testdata/top_k_uint8_5_3_chlo.mlir
    stablehlo/testdata/sort_uint64_5_7.mlir
    stablehlo/testdata/sort_float32_5_7.mlir
    stablehlo/testdata/top_k_bool_5_3_chlo.mlir
    stablehlo/testdata/top_k_int8_5_3_chlo.mlir
    stablehlo/testdata/sort_bfloat16_5_7.mlir
    stablehlo/testdata/sort_uint8_5_7.mlir
    stablehlo/testdata/sort_int64_5_7.mlir
    stablehlo/testdata/sort_complex64_5_7.mlir
    stablehlo/testdata/top_k_uint16_5_3_chlo.mlir
    stablehlo/testdata/sort_bool_5_7.mlir
    stablehlo/testdata/top_k_int16_5_3_chlo.mlir
    stablehlo/testdata/top_k_uint64_5_3_chlo.mlir
    stablehlo/testdata/sort_int32_100_int32_100.mlir
    stablehlo/testdata/top_k_int32_6_chlo.mlir
    stablehlo/testdata/top_k_int32_5_3_chlo.mlir
    stablehlo/testdata/top_k_float32_5_3_chlo.mlir
    stablehlo/testdata/top_k_float16_5_3_chlo.mlir
    stablehlo/testdata/sort_float64_5_7.mlir
    stablehlo/testdata/sort_uint16_5_7.mlir
    stablehlo/testdata/sort_int32_100_int32_100_float32_100.mlir
    stablehlo/testdata/sort_int16_5_7.mlir
    stablehlo/testdata/sort_float32_5.mlir
    stablehlo/testdata/top_k_uint32_5_3_chlo.mlir
    stablehlo/testdata/top_k_bfloat16_5_3_chlo.mlir
    stablehlo/testdata/sort_complex128_5_7.mlir
    stablehlo/testdata/top_k_int64_5_3_chlo.mlir
    stablehlo/testdata/sort_float16_5_7.mlir
    stablehlo/testdata/sort_int32_5_7.mlir
    stablehlo/testdata/sort_uint32_5_7.mlir
    ```
    
    ## Accuracy filters
    
    ```
    $ grep -rnwl stablehlo/testdata -e 'RUN-DISABLED(inaccurate)'
    stablehlo/testdata/dot_general_uint32_4_3_bfloat16_3_6.mlir
    stablehlo/testdata/pow_complex64_20_30_complex64_20_30.mlir
    stablehlo/testdata/dot_general_uint16_4_3_bfloat16_3_6.mlir
    stablehlo/testdata/dot_general_int8_4_3_float16_3_6.mlir
    stablehlo/testdata/dot_general_int32_4_3_float16_3_6.mlir
    stablehlo/testdata/sin_complex64_20_20.mlir
    stablehlo/testdata/tan_complex64_20_20_chlo.mlir
    stablehlo/testdata/cumprod_complex64_8_9.mlir
    stablehlo/testdata/dot_general_uint64_4_3_float16_3_6.mlir
    stablehlo/testdata/dot_general_uint32_4_3_float16_3_6.mlir
    ```
    
    ## Future work
    
    ### OSS the test generation script
    
    Hopefully can get to this later in the week. Just need to figure out how
    to go about testing it. At a minimum I'll put it in an gist that may go
    stale at any time.
    
    ### Make accuracy constraints tighter
    
    We don't have many accuracy guarantees today, and some of these have
    fairly loose tolerances. Would be good to make this more strict over
    time.

[33mcommit b4f1fd8ef551f7e4e644bbf08b1d6e106267f705[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 26 21:29:52 2024

    Bump patch version after integrate 1.6.1 -> 1.6.2 (#2509)

[33mcommit 4c5d9d25ba3b125e6eb52ddf49ffd73f77525fae[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 23 23:16:03 2024

    Minor change : remove extra print() (#2508)
    
    came across it during integration review.

[33mcommit 9ef4af15b8ad1938ceb205076d183de2b41861c6[m[33m ([m[1;33mtag: [m[1;33mv1.6.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 23 21:45:33 2024

    Quantized Test Data files : RoundOp (#2507)
    
    refer parent PR https://github.com/openxla/stablehlo/pull/2404

[33mcommit b31ea9d6806bdddf13d83e79030ae55abb2f73ca[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 23 21:13:12 2024

    Quantized Test Data files : DotGeneralOp (#2500)
    
    refer parent PR https://github.com/openxla/stablehlo/pull/2404
    
    for 6 test files  zero_point of DotGeneralOp `rhs` is not 0.

[33mcommit b98a00fca14e535ec8793ab15933bf0935f34c9b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 23 19:31:08 2024

    Move exported files behind filegroups (#2504)
    
    It is problematic to include StableHLO source files from non-StableHLO
    projects. It makes renaming files and dependencies much more difficult.
    
    Hiding file names behind a target that lives in StableHLO should help
    with the file renames case. There isn't a good way to keep a dependency
    list in StableHLO, but the only dependency of python bindings should be
    CAPI so that shouldn't be too problematic.

[33mcommit 13bf01ca53816c7f9d030b8423ea763d8016f5e6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 23 17:18:48 2024

    Quantized Test Data files : ConvolutionOp (#2499)
    
    refer parent PR https://github.com/openxla/stablehlo/pull/2404

[33mcommit eda7bf75fe60ade79455413ec1d28395b9d6188e[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 23 16:58:34 2024

    Quantized Test Data files : GatherOp (#2502)
    
    refer parent PR https://github.com/openxla/stablehlo/pull/2404

[33mcommit b075e948092d8a27ed0be48f4f8dbaa6df7e2e3e[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Aug 22 16:24:00 2024

    Add a test for non quantized type (DotGeneralOp mismatched operand and result types scenario)  (#2506)
    
    Test at `/testdata/quantized/` is for Quantized type (ref PR
    https://github.com/openxla/stablehlo/pull/2503). Adding a test for non
    quantized types.

[33mcommit c8ef9cc3f3ce655c9ab43c20f1c0122bc7c9163b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Aug 22 00:52:13 2024

    Update testdata-generator to avoid returning custom-call results (#2505)
    
    https://github.com/openxla/stablehlo/pull/2441 proposes a new format of
    testdata where the custom_call value should not be return from the
    testdata function. This PR updated the generator to impose the new
    format.

[33mcommit 8c4bade5d71aa158ac14f9f7ab5b670b547772b8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Aug 21 23:27:13 2024

    Integrate LLVM at llvm/llvm-project@f9031f00f2c9 (#2498)

[33mcommit e175ca184b2cf9cab5366504dcabc22709cd71dc[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 21 19:03:06 2024

    Add round-trippable printing to VHLO per axis type (#2447)

[33mcommit f8fed843bbb44c2a57f943cc11d0024f177247ba[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Wed Aug 21 19:02:33 2024

    Refactor complex asin/acos/asinh/acosh to use asin_acos_kernel. Improve real acos accuracy. (#2496)
    
    As in the title.
    
    This PR introduces asin_acos_kernel operation that implements modified
    Hull et al algorithm for evaluating complex asin, acos, asinh, and acosh
    operations. This task corresponds to the refactor request in
    https://github.com/openxla/stablehlo/pull/2411#discussion_r1662810017
    and it resolves
    https://github.com/pearu/functional_algorithms/issues/15.
    
    In addition, the PR also improves the real acos accuracy as a fix to
    https://github.com/openxla/stablehlo/issues/2452 . As a result, the
    accuracy of single-precision real acos improved as follows (using 1
    million samples):
    ```
                                 main       this PR
    ULP difference == 0 count is 971713  -> 999045
    ULP difference == 1 count is 28158   ->    962
    ULP difference == 2 count is 54      ->      0
    ULP difference == 3 count is 20      ->      0
    ULP difference >= 4 count is 62      ->      0
    ```
    
    
    The cause of the revert by PR
    https://github.com/openxla/stablehlo/pull/2449 (`acos(-1) -> 0` while
    expecting `pi`) is fixed in functional_algorithms which now also
    generates extra samples that correspond to limiting values of real acos.
    For instance, `acos(-1)->pi`, `acos(nextafter(-1, -inf))->nan`, and
    `acos(nextafter(-1, int)-> approx pi` are now included in the tests.
    
    
    The required functional_algorithms version is now 0.9 which includes a
    fix to `fa.utils.real_samples` to return samples that are distributed
    uniformly with respect to ULP differences of neighboring samples. This
    fix lead to updates of `stablehlo/tests/math/*.mlir`.

[33mcommit c49838a06ace7ea840c32ba1772e7a41577f0fac[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Aug 21 16:36:03 2024

    DotGeneralOp interpreter updates (#2503)
    
    1. earlier code assumed result element type is float : added
    `convertUsingZeroValue` to handle supported types.
    
    2. `result` element type and `lhs` and `rhs` element types can be
    different. Convert the `lhs``rhs` to `result` element type before
    computation to avoid errors like `LLVM ERROR: Element types don't match:
    i32 vs i8`
    
    Tests:
    - existing stablehlo tests
    - Validated against new `dot_general` tests being added at
    https://github.com/openxla/stablehlo/pull/2500

[33mcommit d1b9d1ef7648b224d0211aa12dc11ad6ef427def[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Aug 20 00:22:06 2024

    Python APIs for testdata generation  (#2446)
    
    This PR exposes some key APIs to auto-generate StableHLO test programs
    in testdata format, which are leveraged in
    https://github.com/openxla/stablehlo/pull/2404.
    
    
    ## Proposed API
    
    ```python
    def testdata_generator(
        module: ir.Module, args: Sequence[np.ndarray] = []
    ) -> ir.Module:
    ```
    
    -   `module`: The StableHLO module to generate test data for.
    
    - `args`: (Optional) A sequence of NumPy arrays representing input
    values for
    the module. If not provided, the function will attempt to extract input
        values from the module itself.
    
    ## Example
    
    ```python
    # Input (module_str)
    module_str = """
    module {
      func.func @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {
        %0 = stablehlo.add %arg0, %arg1 : tensor<2xf32>
        return %0 : tensor<2xf32>
      }
    }
    """
    
    # Input (args)
    args = [
        np.array([1.0, 2.0], dtype=np.float32),
        np.array([3.0, 4.0], dtype=np.float32)
    ]
    
    # Generate test data
    module_output = testdata_generator(module, args)
    
    # Output (module_output)
    module_output_str = """
    module {
      func.func @main() -> tensor<i1> {
        %cst = stablehlo.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>
        %cst_0 = stablehlo.constant dense<[3.000000e+00, 4.000000e+00]> : tensor<2xf32>
        %cst_1 = stablehlo.constant dense<[4.000000e+00, 6.000000e+00]> : tensor<2xf32>
        %0 = stablehlo.add %cst, %cst_0 : tensor<2xf32>
        %1 = stablehlo.custom_call @check.eq(%cst_1, %0) : (tensor<2xf32>, tensor<2xf32>) -> tensor<i1>
        return %1 : tensor<i1>
      }
    }
    """
    ```
    
    Note to reviewer: The current PR is based on
    https://github.com/openxla/stablehlo/pull/2445, so please review that
    first.

[33mcommit 691c6765186c6a8507c5132e080de568bfa32a90[m[33m ([m[1;32mtestdata_gather[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Aug 19 16:14:05 2024

    Add StableHLO CAPI and get_version_from_compat_requirement API (#2494)
    
    This PR introduces proper CAPI bindings for popular APIs (not all, will
    get to that). Testing internally before external PR, wanted to share for
    knowledge's sake.
    
    This also introduces a new StableHLO API and python binding for getting
    target versions based on compatibility requirements. Currently
    supporting the following values:
    
    ```
    CompatRequirement ::= None | 1mo | 3mo | Max
    
    Version fromCompatibilityRequirement(CompatRequirement);
    ```
    
    Anything more fine-grained didn't work well because of our integrate
    cadence: Merge to StableHLO, then export to openxla/xla / TF /
    elsewhere. Potentially 2 different dates with the same meaning depending
    on how a project depends on StableHLO.
    
    More compat requirements can be added at any time, on a per-use-case
    basis and versions that compat requirements map to can be modified as
    needed, as long as the updated version satisfies the requirement
    constraint.
    
    Closes #2170
    Closes #2350

[33mcommit 591f2e360f61b5b5a7fc7e610b9967fad2a0f366[m[33m ([m[1;32mqtestdata1[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 16 21:17:28 2024

    Bump patch version after integrate 1.6.0 -> 1.6.1 (#2495)

[33mcommit 23d3e1414b0be1c1b5256f0949520dc4f0a0705c[m[33m ([m[1;33mtag: [m[1;33mv1.6.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 14 19:02:18 2024

    Integrate LLVM at llvm/llvm-project@c8b5d30f7077 (#2492)

[33mcommit c376038dbb9f7c3ad21af2879e2fe99e921bf48c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 14 18:59:31 2024

    Add version log notice to VhloDialect.td (#2493)

[33mcommit 401749bd1874546b90d9aa4212e0b94a730e7456[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 14 18:25:34 2024

    Compatibility impl for DotAlgorithmAttr (#2491)
    
    Added VHLO support for DotAlgorithmAttr:
    - Added VHLO NoneType
    - Added VHLO TF32 (only supported in dot alg attr)
    - Upgrade/downgrades to DotGeneralV1/V2
    - Updated parsing/printing (two sequential optional atts don't behave
    well)

[33mcommit 3d71fd57447a2fab0147ceb26b4d2ce480833bf6[m
Author: Tai Ly <tai.ly@arm.com>
Date:   Wed Aug 14 16:16:18 2024

    Add passes to lower quantized operations to TOSA rescale (#2476)
    
    This adds two passes that help lower quantized stablehlo operations to
    integer operations.
    
    1. The pass --stablehlo-quant-legalize-to-tosa-rescale rewrites
    stablehlo quantized operations to integer operations by inserted TOSA
    rescale operations at the inputs and output of the integer operations.
    
    Initially, this only supports stablehlo add and subtract operations
    
    2. The pass, --tosa-rescale-legalize-to-stablehlo rewrites TOSA rescale
    operations to stablehlo operations that implement the rescale
    operations.
    
    Initially, this only supports TOSA rescales for signed inputs/outputs,
    with attributes scale32=true, double_round=false, and per_channel=false.
    
    ---------
    
    Signed-off-by: Tai Ly <tai.ly@arm.com>

[33mcommit 266d8845a45a4696b4018ab6651349a53237b58e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 13 18:24:00 2024

    Implement Dot Algorithms RFC (#2467)
    
    Implementation of https://github.com/openxla/stablehlo/pull/2422.
    
    Done:
    - Attribute impl
    - Spec updates
    - Test cases
    
    To-do:
    - Create an XLA doc on supported algorithm pairs to link to from spec.
    - Compatibility impl, will do this in a child CL to keep review
    simplified.
    
    To consider:
    - Should we verify algorithms more strictly, so that only algs supported
    on _some_ hardware are allowed in StableHLO. Expanding support will be
    trivial, no RFC required, but StableHLO should try to statically enforce
    as much as possible, why allow nonsense pairs.
    - Should dot_general on integers support floating point precision types?
    
    Other cleanup:
    - PrecisionConfig is no longer implicitly optional, but all uses are
    wrapped in `OptionalAttr` now.

[33mcommit d8ad016f03ef334d85517ae605c8299b7d349501[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Aug 12 22:48:41 2024

    New StablehloLegalizeQDQToQuantizedOpPass (#2478)
    
    `StablehloLegalizeQDQToQuantizedOpPass` composes StableHLO quantized op
    using uniform quantize/dequantize ops.
    
        ```mlir
    func.func @add(%arg0: tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>) ->
    tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>> {
    %0 = stablehlo.uniform_dequantize %arg0 :
    (tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>) -> tensor<16x16xf32>
          %1 = stablehlo.abs %0 : tensor<16x16xf32>
    %2 = stablehlo.uniform_quantize %1 : (tensor<16x16xf32>) ->
    tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>
          func.return %2 : tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>
        }
        ```
    
        Will become:
    
        ```mlir
    func.func @add(%arg0: tensor<16x16x!quant.uniform<u8:f32,
    3.400000e+01:16>>) -> tensor<16x16x!quant.uniform<u8:f32,
    3.400000e+01:16>> {
    %0 = stablehlo.abs %arg0 : tensor<16x16x!quant.uniform<u8:f32,
    3.400000e+01:16>>
          return %0 : tensor<16x16x!quant.uniform<u8:f32, 3.400000e+01:16>>
        }
        ```
    
    created https://github.com/openxla/stablehlo/issues/2485 to track
    extending this pass to support more patterns

[33mcommit 65b184cd9ff5399318f9a4c04136165533869aa7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 12 17:50:07 2024

    Integrate LLVM at llvm/llvm-project@4c5ef6690040 (#2490)

[33mcommit 726ac03ae34faaf26535a66db892fa90e7d93ce4[m[33m ([m[1;32mquantized_testdata[m[33m, [m[1;32mpass_imrpovements[m[33m, [m[1;32mpass_improv[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 9 17:51:25 2024

    Bump patch version after integrate 1.5.2 -> 1.5.3 (#2484)

[33mcommit d4405ad84b43c9255b504726105c3ee9434c12d5[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Aug 8 23:24:49 2024

    Collectives Ops : Match example from the StableHLO op description comment with example from the spec (#2481)
    
    as per
    https://github.com/openxla/stablehlo/blob/main/docs/reference_checklist.md#after-implementing-the-op.

[33mcommit 3ec554684732a497123a21057ac831180d24d3a7[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Aug 8 21:51:20 2024

    Minor spec fix : correct variadic result notation for all_gather and all_reduce ops (#2480)

[33mcommit 24d1807a9a3e0df81103a0be9be7ad28ee34c85a[m[33m ([m[1;33mtag: [m[1;33mv1.5.2[m[33m, [m[1;32msmallvector[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Aug 8 16:15:35 2024

    Integrate LLVM at llvm/llvm-project@0c25f85e5b88 (#2479)

[33mcommit 451d80fc0f9adad2882e221aa4b24a0bed58ab26[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 7 17:37:54 2024

    Fix cpp file includes using build cleaner (#2477)
    
    Ran a build cleaner which clears unused imports and makes all imports
    explicit.
    
    Note, this is only done for CPP files for now, will do header files in a
    followup

[33mcommit 604a1ecaa1d010113e45f7e69348ec23e3d977a1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 5 18:59:08 2024

    Fix SortOp interpreter (#2474)
    
    fixes https://github.com/openxla/stablehlo/issues/2440. The source and
    destination indices were not in correct order.

[33mcommit 0d6806b65b3341257ab5bcc4f356e1501432be0b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 5 18:58:14 2024

    Document availability of interpreter for quantized ops (#2470)

[33mcommit 3b38a18f7a9cb8ff8b853db175afd11dfbeed4a3[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Sat Aug 3 00:05:04 2024

    Bump patch version after integrate 1.5.1 -> 1.5.2 (#2475)

[33mcommit 2fcbae0c933b5cc2735523bab2de880a3a9c5e46[m[33m ([m[1;33mtag: [m[1;33mv1.5.1[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 2 17:49:52 2024

    [Back-porting from the Integrate] Portable API version bump 7 -> 8 (#2473)

[33mcommit 92203a9612dbdd6681ccd3e65bc61586c4290df1[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Aug 2 16:21:42 2024

    Integrate LLVM at llvm/llvm-project@60a7d33106d3 (#2472)

[33mcommit 0209ea49ea552decc08d4933d3cf51ae227e98c1[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Aug 1 21:10:30 2024

    Bump patch version after integrate 1.5.0 -> 1.5.1 (#2469)

[33mcommit cc9d35e909c54aaa0dcfea1b2bc0304a198674b1[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Aug 1 15:13:14 2024

    ExpectCloseOp  : Keep only one `optional-group` in custom assembly (#2468)
    
    After recent LLVM integrate, optional attributes that have a default
    value, will not be printed anymore.
    This exposed `ExpectCloseOp` custom assembly parser limitation. It can't
    handle a scenario of - if `max_ulp_difference` is not printed.
    
    ```
    if (::mlir::succeeded(parser.parseOptionalComma())) {
      if (parser.parseKeyword("max_ulp_difference"))
        return ::mlir::failure();
      if (parser.parseEqual()) return ::mlir::failure();
    ...
    }
    
    ```
    
    existing parser expects `max_ulp_difference` to be present after `,`
    which is not the case anymore for test
    `check.expect_close %3, %4, max_ulp_difference = 1, min_ulp_difference =
    1 : tensor<9xf64>, tensor<9xf64>`
    To address this, we will need nested `optional-group`s which results in
    an error `attribute 'min_ulp_difference' is already bound`
    
    fix- keep only one `optional-group` in custom assembly.
    
    Reviewed, none other stablehlo op has multiple `optional-group`s in
    custom assembly.

[33mcommit fb18ee251b81a43fd9a0f45f2f34770389e916d0[m[33m ([m[1;33mtag: [m[1;33mv1.5.0[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Jul 29 17:57:28 2024

    Integrate LLVM at llvm/llvm-project@99bb9a719cec (#2466)

[33mcommit c85f4274b04b945a543eacc1872196015dc371d2[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Jul 29 17:25:15 2024

    Make Collectives (all_reduce, all_gather, all_to_all) Ops Variadic (#2450)
    
    RFC : https://github.com/openxla/stablehlo/pull/2099
    
    * updated the op spec and relevant tests
    * updated interpreter
    * required VHLO changes

[33mcommit fbcf2942d29bb65f1c155fdc0fda7e867d50b754[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 26 16:26:25 2024

    Remove the `qunt-to-math` pass limitation of Dot/Conv op result type (#2462)
    
    [ParentPR](https://github.com/openxla/stablehlo/pull/2461)
    
    `quant-to-math` pass
    [assumes](https://github.com/openxla/stablehlo/blob/eba821aa1c54a21d70331d7926dfc8b929f988f3/stablehlo/transforms/StablehloLegalizeQuantToInt.cpp#L984)
    that the return type of quantized dot_general or convolution is always
    having `i32` as storage type.
    With that the following program with the result storage type of `i8`fail
    to materialize all the intermediate converted values.
    
    
    ```
    func.func @dot_general_per_tensor_quantization(%arg0: tensor<2x3x4x!quant.uniform<i8:f32, 1.0:17>>, %arg1: tensor<2x3x5x!quant.uniform<i8:f32, 1.0:0>>) -> tensor<2x4x5x!quant.uniform<i8:f32, 1.0:17>> {
      // expected-error@+1 {{failed to legalize operation 'stablehlo.dot_general' that was explicitly marked illegal}}
      %0 = "stablehlo.dot_general"(%arg0, %arg1) {
        dot_dimension_numbers = #stablehlo.dot<
          lhs_batching_dimensions = [0],
          rhs_batching_dimensions = [0],
          lhs_contracting_dimensions = [1],
          rhs_contracting_dimensions = [1]
        >
      } : (tensor<2x3x4x!quant.uniform<i8:f32, 1.0:17>>, tensor<2x3x5x!quant.uniform<i8:f32, 1.0:0>>) -> tensor<2x4x5x!quant.uniform<i8:f32, 1.0:17>>
      func.return %0 : tensor<2x4x5x!quant.uniform<i8:f32, 1.0:17>>
    }
    ```
    
    One option to fix this to provode source/target materialization
    [link](https://mlir.llvm.org/docs/DialectConversion/#type-converter),
    but we found that for other Ops
    [e.g.](https://github.com/openxla/stablehlo/blob/eba821aa1c54a21d70331d7926dfc8b929f988f3/stablehlo/transforms/StablehloLegalizeQuantToInt.cpp#L510),
    there is a precedent on how to convert the math computed in `i32` is
    converted back to result type. The PR implements the missing conversion.
    
    Note to the reviewers: To may just focus on the very last commit of the
    chain. The rest is coming from parent PR.

[33mcommit 0673dd2839cc047b1e88abda754a6be48feef5f0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 26 05:06:30 2024

    Support qdq decomposition of DotGeneralOp and ConvolutionOp (#2461)
    
    [ParentPR](https://github.com/openxla/stablehlo/pull/2460)
    
    The PR
    1. adds patterns for qdq decomposition patterns for `DotGeneralOp` and
    `ConvolutionOp`.
    1. Updates the tests in
    stablehlo/tests/stablehlo_legalize_quant_to_int.mlir updating only
    negatives tests, which are previously unhanded by
    stablehlo-legalize-quant-to-int. The current
    stablehlo-legalize-quant-to-math uses the fallback to handle these
    cases.
    
    Note to the reviewers: To may just focus on the very last commit of the
    chain. The rest is coming from parent PR.
    
    [childPR](https://github.com/openxla/stablehlo/pull/2462)

[33mcommit 9655fab22b318681898dbfa2ae07a93f17824419[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 26 02:40:49 2024

    Remove type-inference dependency while creating qdq pattarns (#2460)
    
    [ParentPR](https://github.com/openxla/stablehlo/pull/2459)
    
    Previously while creating the QDQ pattern we use `create` API without
    using the result type and hence reply on the type inference to derive
    the result type. That works for most of the element-wise operations,
    however, for dot_general and convolution the result type can might be
    infeasible to infer in the presense of input quantize types.
    
    The PR fixes that.
    
    Note to the reviewers: To may just focus on the very last commit of the
    chain. The rest is coming from parent PR.
    
    [ChildPR](https://github.com/openxla/stablehlo/pull/2461)

[33mcommit 4286b80151d68d28baf1f2cafb586e060dfbbd63[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 26 01:56:21 2024

    Support qdq decomposition of TanOp (#2459)
    
    [parent PR](https://github.com/openxla/stablehlo/pull/2458)
    
    The PR adds patterns for qdq decomposition patterns for newly added
    `TanOp`
    
    Note to the reviewers: To may just focus on the very last commit of the
    chain. The rest is coming from parent PR.
    
    [childPR](https://github.com/openxla/stablehlo/pull/2460)

[33mcommit 29802595d17f6541991d34f2c7418d1d02808ae1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 26 00:54:12 2024

    Merge qdq and quant-to-int passes (#2458)
    
    This PR does the followings:
    1. Merges `stablehlo-legalize-quantized-op-to-qdq` into
    `stablehlo-legalize-quant-to-int`.
    1. Rename `stablehlo-legalize-quant-to-int` to
    `stablehlo-legalize-quant-to-math`. This is to clarify for scenario when
    the fallback `qdq` is used and sull integer quantized program cannot be
    generated.
    1. Removes `stablehlo-legalize-quantized-op-to-qdq` pass and replace its
    uses with `stablehlo-legalize-quant-to-math`.
    1. Remove QDQ lit checks from
    `stablehlo/tests/ops_stablehlo_quantized.mlir` and merges the tests
    added for qdq pass in
    `stablehlo/tests/stablehlo_legalize_quant_to_int.mlir`
    1. Updates the tests in
    `stablehlo/tests/stablehlo_legalize_quant_to_int.mlir` updating __only__
    negatives tests, which are previously unhanded by
    `stablehlo-legalize-quant-to-int`. The current
    `stablehlo-legalize-quant-to-math` uses the fallback to handle these
    cases.
    1. About the pass    `stablehlo-legalize-quant-to-math`
    - It uses `Patternbenefit` to assign highest priority (`benefit=10`) to
    pattern which has specialized handling in
    `stablehlo-legalize-quant-to-int`. Next in priority (`benefit=0`) are
    the QDQ patterns.
    
    With that the following program, which `stablehlo-legalize-quant-to-int`
    has specialized handling, will avoid the fallback path.
    ```
    func.func @max_per_tensor_same_quant_parameters(
        %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>
      ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>> {
      %0 = "stablehlo.maximum"(%arg0, %arg0) : (
        tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,
        tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>
      ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>
      return %0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>
    }
    ```
    
    whereas the following, which is not supported in
    `stablehlo-legalize-quant-to-int` will choose the fallback path.
    ```
    func.func @max_per_tensor_diff_quant_parameters(%arg0: tensor<!quant.uniform<i8:f32,1.0:0>>, %arg1: tensor<!quant.uniform<i8:f32,2.0:1>>) ->  tensor<!quant.uniform<i8:f32,3.0:2>> {
      %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<!quant.uniform<i8:f32,1.0:0>>, tensor<!quant.uniform<i8:f32,2.0:1>>) -> tensor<!quant.uniform<i8:f32,3.0:2>>
      func.return %0 : tensor<!quant.uniform<i8:f32,3.0:2>>
    }
    ```
    
    
    - Currently handles qdq fallback for AddOp and a bunch of `GenericOps`
    op
    [cs](https://github.com/openxla/stablehlo/blob/eba821aa1c54a21d70331d7926dfc8b929f988f3/stablehlo/transforms/StablehloLegalizeQuantToInt.cpp#L1239).
    qdq fallback for `dot_general` and `convolution` will be handled in a
    follow up PR. What this means is we will still see quantized
    dot_gneral/convolution program, which are currently unsupported by
    `stablehlo-legalize-quant-to-int`, error out.
    
    
    [childPR](https://github.com/openxla/stablehlo/pull/2459)

[33mcommit 0bfc5362b3a2897bc08b7d3f437d8fb762fc0dd9[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Jul 26 00:16:30 2024

    Minor fix : add missing space in log message (#2465)

[33mcommit 193ce72619456dcb5afdc474075a654bf57d9c4d[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Jul 25 23:15:25 2024

    Bump patch version after integrate 1.4.2 -> 1.4.3 (#2464)

[33mcommit 8555db77763fadbd6be83df0a5532828bc419cba[m[33m ([m[1;33mtag: [m[1;33mv1.4.2[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Jul 25 18:34:28 2024

    Integrate LLVM at llvm/llvm-project@84658fb82b67 (#2463)

[33mcommit b056d3da25c18169c14018a1e953112852a53453[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Jul 24 16:46:07 2024

    Update all_to_all op interpreter to fix issue#2433 (#2457)
    
    fixes: https://github.com/openxla/stablehlo/issues/2433
    
    
    ```
    scattered_parts...@receiver = [split_parts...@sender[receiver_index] for sender in process_group] where receiver_index = process_group.index(receiver).
    ```
    each `sender/process` from `process_group` contributes a `split_part` to
    the `scattered_parts`. This PR ensures the order of `split_part` in
    `scattered_parts` matches with the order of the `sender/processes`
    within the `process_group`.

[33mcommit eba821aa1c54a21d70331d7926dfc8b929f988f3[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jul 23 16:07:58 2024

    Python bindings for registering check dialect (#2445)
    
    Check dialect is an auxiliary dialect used in StableHLO repository for
    validation of StableHLO program evaluation. Currently there is no
    cleaner way to parse a module containing check dialect operations or to
    create them. One way to go around this is to textually modify the check
    dialect ops to normalize them to generic MLIR text and allow
    unregistered dialect to pars that text. However, this PR provodes a
    cleaner way to process check dialect.
    
    This PR prepares for open sourcing some of the utilities, leveraged in
    https://github.com/openxla/stablehlo/pull/2404, for auto-generating
    testdata formatted test files.

[33mcommit edef22aec278adec623dce33792d969e6dbedebe[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Jul 22 22:36:42 2024

    Bump patch version after integrate 1.4.1 -> 1.4.2 (#2455)

[33mcommit 840c41ceb0d13800d286a9d76d8ad00d97838d9e[m[33m ([m[1;33mtag: [m[1;33mv1.4.1[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Jul 22 19:45:48 2024

    Integrate LLVM at llvm/llvm-project@dd7d81ea49bf (#2454)

[33mcommit 50cdc03b3e1840e316960fda5f31378a834ba2f7[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Jul 22 18:13:37 2024

    Make "Rendezvous" variadic (#2443)
    
    The PR allows each Process from the ProcessGrid to contribute more than
    one tensor at `ProcessGrid::rendezvous`.
    This is the first set of change, will be followed by interpreter updates
    to collectives.
    
    Note: The PR does **not** make collectives interpreter variadic.
    
    Tested:
    1. No existing test failures indicate no change in behavior for ops
    using the `rendezvous`
    2. The diff is tested with new variadic interpreter for `all_reduce` op.
    Will upload the PR soon.
    
    
    ref: https://github.com/openxla/stablehlo/pull/2099

[33mcommit 70c210d661594b9b35c148d1289a2b3668699978[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jul 22 14:55:30 2024

    Revert acos changes for non complex numbers (#2449)
    
    It looks like something may be broken in the `chlo.acos` lowering from
    functional algorithms. The complex lowering works just fine (has
    `SelectOp`).
    
    Also I'm wondering how we didn't catch this / could have caught this.
    Perhaps we should have special cases for ops with limits..somehow? Open
    to iterating on this, either by dev policy on these CLs (ensure total
    code coverage) or if there's a way to auto generate cases would be good
    too.
    
    Current behavior prior to revert: `chlo.acos(-1) --> 0`, expected
    behavior is `pi`.

[33mcommit c28d55e91b4a5daaff18a33ce7e9bbd0f171256a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Jul 20 00:39:48 2024

    Bump patch version after integrate 1.4.0 -> 1.4.1 (#2451)

[33mcommit 7f336b6a7462c40e8086ea8f909836405a11ca70[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jul 19 18:27:12 2024

    MInor lint fix (#2448)

[33mcommit 531816f07e0db010a676c23fc66fe0a1a2e2d648[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jul 18 01:26:28 2024

    Integrate LLVM at llvm/llvm-project@0913547d0e39 (#2444)

[33mcommit 2f89787c8f663e39db3b47a161be98f0f2e72ced[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 16 23:51:05 2024

    Add CHLO decomposition pipeline and new interpreter check calls (#2439)
    
    These are changes in preparation for refreshing all of `testdata` and
    adding CHLO-based testdata files.
    
    - Add more check ops to supported custom_calls in translate --
    `@check.expect_close/almost_equal/check_eq`
    - Add `--chlo-pre-serialization-pipeline` to be run on programs with
    CHLO ops to decompose them along with their shape computation to
    StableHLO ops.
    
    Note: This is based on https://github.com/openxla/stablehlo/pull/2438
    for the changes to CheckOps in that PR.

[33mcommit a30db72f387c4faaa8456d797ee703c5c88304d1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 16 23:40:22 2024

    Add better printing in reference interpreter, add tolerance to check almost eq (#2438)
    
    Background: I was running interpreter testcases and wanted some syntax I
    could copy-paste into python. Now tensors in the interpreter print in a
    python-array-compatible syntax, wrapped in a `tensor{...}` descriptor.
    
    ```
    $ cat /tmp/t.mlir
    func.func @main() -> tensor<2x2xi8> {
      %matrix = stablehlo.constant dense<[[1, 2], [3, 4]]> : tensor<2x2xi8>
      func.return %matrix : tensor<2x2xi8>
    }
    
    $ stablehlo-translate --interpret /tmp/t.mlir
    tensor<2x2xi8> {
      [
        [1, 2],
        [3, 4]
      ]
    }
    ```
    
    Also as a part of this work I needed to modify some testcase tolerances.

[33mcommit eff200ceb69e4447b1470a3fc38d223ecdf40522[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Jul 16 15:41:41 2024

    Add TanOp to StableHLO Opset (#2177)
    
    RFC: https://github.com/openxla/stablehlo/pull/2101
    
    This PR include
    * OP def
    * reference implementation, test cases
    * VHLO change and test cases

[33mcommit 7e749c84f933e092872ac0bcc2ecab40abf24f5d[m[33m ([m[1;33mtag: [m[1;33mv1.4.0[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jul 15 21:21:55 2024

    Integrate LLVM at llvm/llvm-project@9ddfe62f5c11 (#2436)

[33mcommit 443a56f17a57abc82d3df3c2e0997d4549137bd3[m[33m ([m[1;33mtag: [m[1;33mv1.3.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jul 15 20:23:07 2024

    Bump patch version after integrate 1.3.0 -> 1.3.1 (#2437)

[33mcommit 283f41ce1f2a623e99b73131debfcc76a5b7da16[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Jul 12 17:33:06 2024

    [RFC] Add TanOp to the StableHLO specification (#2101)
    
    This RFC proposed adding TanOp to the StableHLO specification
    
    Please review and provide you feedback.

[33mcommit 289eadfa06745f7f195f357b7d8bba98dcb95ad1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jul 11 14:49:06 2024

    Add convenience method to CustomCallOp for checking empty backend config (#2432)
    
    This logic is used many times in openxla/xla, so decided to turn it into
    a method

[33mcommit 8817ff1d7105b027f1f50e671a52121c7cf51ea7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jul 10 23:28:21 2024

    Integrate LLVM at llvm/llvm-project@0a95f2f7fe3a (#2431)

[33mcommit 92676645416d218dc86a18e7546ef08ce105dd51[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jul 10 20:28:41 2024

    Refactor CMakeList to work for all build types (#2430)
    
    This setup is based on:
    [How_to_Build_your_own_MLIR_Dialect.pdf](https://www.google.com/url?sa=D&q=https%3A%2F%2Farchive.fosdem.org%2F2023%2Fschedule%2Fevent%2Fmlirdialect%2Fattachments%2Fslides%2F5740%2Fexport%2Fevents%2Fattachments%2Fmlirdialect%2Fslides%2F5740%2FHow_to_Build_your_own_MLIR_Dialect.pdf)
    
    ```
    # There are 3 build modes, one will be set to ON
    #  - Standalone: Build MLIR as a part of StableHLO
    #  - External: StableHLO built as an external LLVM project (XLA/MHLO uses this)
    #  - Embedded: StableHLO built as a part of another MLIR project (torch-mlir uses this)
    #
    # If building as part of another project, let it handle the MLIR dependency.
    # The dependent project might use a bundled version of MLIR instead of installing.
    ```
    
    Tested using `mlir-hlo` repo. No changes to the `EMBEDDED` build so
    should not impact torch-mlir.

[33mcommit a5f8840f145e1d542567bcd8e999f0f26b30e889[m
Author: Sagar Shelke <shelkesagar29@yahoo.com>
Date:   Wed Jul 10 19:18:51 2024

    Don't apply `stablehlo-legalize-to-linalg` pass on ModuelOp. (#2429)
    
    Anchoring `stablehlo-legalize-to-linalg` on ModuleOp doesn't serve any
    specific purpose at this point. However, this makes it difficult to use
    this pass in any way for downstream stablehlo users. This change removes
    ModuleOp anchor from `stablehlo-legalize-to-linalg` pass.
    
    Co-authored-by: Sagar Shelke <sagshelke@nvidia.com>

[33mcommit 2f4d7eab2d677c3b80d0ba13d00ec87b8870360a[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Jul 10 16:43:41 2024

    Extend CustomCallOp backend_config to take a DictionaryAttr (#2415)
    
    * updated custom_call op def
    * Added logic to fail a downgrade from 1.3.0 to previous versions, if
    the `custom_call` op's `backend_config` is a `DictionaryAttr.`

[33mcommit 8a78ebb89bc6c5608f6f27a7a9ce687b3578b79d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jul 9 17:11:25 2024

    Fix quant verification labels (#2428)
    
    The verification labels used in code comments is out of place for some
    quantization relation constraints. Fixing those.

[33mcommit 3033979958c7be016f9f68395689155dcbfa0157[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Jul 8 19:41:22 2024

    Bump patch version after integrate 1.2.0 -> 1.2.1 (#2427)

[33mcommit 922385479464af5c5356c7c88b66b3846ceae686[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Fri Jul 5 13:55:43 2024

    Improve the accuracy of complex asinh and the efficiency of real asinh. (#2402)
    
    - The accuracy of complex asinh is improved by using the relation
      ```
      asinh(z) = -I * asin(I * z)
      ```
    and `asin` from #2357. As a result, the counts of ULP differences
    between `asinh` evaluation and expected values, reduce as follows:
      ```
                             current main  -> this PR
      ULP difference == 0 count is 143574  -> 824063
      ULP difference == 1 count is 12110   -> 176898
      ULP difference == 2 count is 4359    -> 1028
      ULP difference == 3 count is 3312    -> 12
      ULP difference >= 4 count is 838646  -> 0
      ```
    - The efficiency of real asinh is improved (read: the number of
    instructions in real asinh op is reduced) by eliminating a branch for
    `abs(x) < 1` because the formula
      ```
      asinh(x) = log1p(x + x ** 2 / (1 + hypot(1, x)))
      ```
      is accurate for all `x: abs(x) < sqrt(largest)`

[33mcommit aea942ea85b6769b057372ee798f1ad06b18e013[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Wed Jul 3 22:46:52 2024

    Improve accuracy of complex and real acosh (#2411)
    
    - Improve the accuracy of complex acosh as follows (complex64):
      ```
                             current main  -> this PR
      ULP difference == 0 count is 141180  -> 839342
      ULP difference == 1 count is 10145   -> 162239
      ULP difference == 2 count is 1929    -> 420
      ULP difference == 3 count is 826     -> 0
      ULP difference >= 4 count is 847921  -> 0
      ```
    
    - Improve the accuracy of real acosh as follows (float32):
      ```
                               current main  -> this PR
      ULP difference == 0 count is 966912    -> 993686
      ULP difference == 1 count is 33008     -> 6303
      ULP difference == 2 count is 49        -> 12
      ULP difference == 3 count is 13        -> 0
      ULP difference >= 4 count is 19        -> 0
      ```
    
    This PR is created on top of
    https://github.com/openxla/stablehlo/pull/2409

[33mcommit 1b08c4c0e8c893d202bd33c2735562fa0ccc849f[m[33m ([m[1;33mtag: [m[1;33mv1.2.0[m[33m)[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Jul 3 16:47:08 2024

    Integrate LLVM at llvm/llvm-project@6461b921fd06 (#2426)

[33mcommit d44dbf853ddd2bb656ea7a463d4e7b8c403b2ac9[m
Author: Sergei Lebedev <185856+superbobry@users.noreply.github.com>
Date:   Wed Jul 3 13:46:49 2024

    Add int2 support (#2424)
    
    As proposed in RFC: int2 in StableHLO (#2403), this PR adds support for
    these types to StableHLO.

[33mcommit 4e36997aa74e96dd97d3451355631256afd106f5[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Jul 2 21:22:29 2024

    Bump patch version after integrate 1.1.7 -> 1.1.8 (#2423)

[33mcommit f372def1c14e1b7f3a95627857d5220e3cadd22f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 2 16:55:48 2024

    Dot algoritms RFC: Lint fix and status update (#2422)

[33mcommit 5f52ca32c02970026a626309b21b4f81c1414ecc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jul 2 16:55:03 2024

    Adding multi-operation quantization tests (#2413)
    
    Adds multi-operation quantized test cases
    
    There aren't many multi-operation testdata files in
    https://github.com/openxla/stablehlo/tree/main/stablehlo/testdata
    satisfying
    1. The file should cover ops from the list `cbrt, ceil, cosine, divide,
    exponential, exponential_minus_one, floor, log, log_plus_one, logistic,
    multiply, negate, sign, sine, sqrt, subtract, tanh,`
    2. The file should test only `f32` type
    
    
    Going forward will will be expanding on the op/type coverage.

[33mcommit 00da85dad6d17bbe3fade40311793a4554b9b4f3[m
Author: tdanyluk <tdanyluk@google.com>
Date:   Tue Jul 2 16:41:45 2024

    [RFC] Add algorithm to dot_general's parameters in the StableHLO specification (#2096)

[33mcommit 03cd35383237cd84c1124272715a634d314cb29d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jul 2 16:28:25 2024

    Python binding for stablehlo.register_passes (#2401)
    
    Implements python bindings to register stablehlo specific passes.

[33mcommit 8eb0ad3de6530b52f83dbad3281d955682a591ca[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jul 1 21:37:58 2024

    Cleanup StableHLO bytecode patches used for temporary pjrt compatibility (#2420)
    
    Several hacks/patches were put in place to preserve StableHLO
    compatibility for PJRT plugins.
    
    Recently PJRT was switched to send VHLO, meaning we can clean up all
    these patch fixes.
    
    Closes #2216

[33mcommit 2a6ae6e1d8aac9514cbe393898d383a1d9f8fc9f[m[33m ([m[1;33mtag: [m[1;33mv1.1.7[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Jul 1 20:43:06 2024

    Integrate LLVM at llvm/llvm-project@8598bcb9934d (#2421)

[33mcommit 5c630e11a8721ff63024e0c0688e7d35f3a5f929[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jul 1 17:39:01 2024

    Update RFC status on approved RFCs (#2418)

[33mcommit ed406d89191bd056a662674d95f9d931dcaac2a6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Jun 27 00:50:13 2024

    Update all_to_all op spec example to reflect variadic operand, result (#2419)

[33mcommit 007c0599f2b26488251f54956f37ae0d37d7c789[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Jun 26 16:56:35 2024

    Use ConfinedAttr to verify attr range is positve or nonnegative (#2416)
    
    I recently came across [confining
    attributes](https://mlir.llvm.org/docs/DefiningDialects/Operations/#confining-attributes),
    and it's a nice feature that we should leverage to maximize ODS
    strengths.
    
    I'm still looking into whether we can use them for array attributes. If
    possible, we may be able to remove tons of manual verification.

[33mcommit 6b69e214ec7b5bd74fffb506d0ec0d8bbc455683[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed Jun 26 15:40:41 2024

    [RFC] Standardize Collective (AllGatherOp, AllReduceOp, AllToAllOp) ops to support variadic operand/result (#2099)
    
    This RFC proposes standardizing collective (AllGatherOp, AllReduceOp,
    AllToAllOp) ops to enable support for `multi-operand` and `multi-result`
    
    Please review and provide you feedback.

[33mcommit 20667011f256458782678293e9a352eb3153b103[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 26 15:38:01 2024

    [RFC] Add support for int2 in StableHLO (#2403)
    
    To be shared to OpenXLA Discuss shortly.
    
    ---------
    
    Co-authored-by: Sergei Lebedev <slebedev@google.com>

[33mcommit aad3d9d3b8118ad019db9c18f387675adb95f1fe[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Jun 25 23:41:04 2024

    Remove spurious symbol on lit error checks (#2417)
    
    Our repo uses `expected-error@+1` rather than `@expected-error@+1` for
    majority of the tests, so let's consolidate it to one style.

[33mcommit 59b7fd28a848fa4e5db399e3f163ce0d36c22b08[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Tue Jun 25 22:08:46 2024

    Align canonicalization of stablehlo.broadcast_in_dim with XLA (#2371)
    
    This patch is a two-stage:
    1) change `isIotaRange` constraint to `llvm::sorted` in order to enable
    the rewrite from `broadcast_in_dim` -> `reshape`:
    ```
      %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x6xi32>) -> tensor<1x3x6xi32>
    ```
    is transformed into:
    ```
      %0 = stablehlo.reshape %arg0 : (tensor<3x6xi32>) -> tensor<1x3x6xi32>
    ```
    (hlo_ops.cpp: BroadcastInDimSimplifier uses same approach)
    
    2) `broadcast_in_dim` operator can be quite complex in its semantics,
    sometimes functioning as a combination of `transpose` and `broadcast`.
    In order to simplify analysis and optimization, it can be beneficial to
    fully expand operand dimensions by ensuring that the size of
    broadcast_dimensions matches the rank of the result. This can be
    achieved by reshaping the input operand while preserving the constraint
    `size(broadcast_dimensions) = rank(operand)` that the size of
    broadcast_dimensions matches the rank of the operand.
    
    ```
        %3 = stablehlo.broadcast_in_dim %arg3, dims = [2, 1, 4] : (tensor<32x64x16xf32>) -> tensor<4x64x32x2x16xf32>
    ```
    is transformed into:
    ```
      %2 = stablehlo.reshape %arg3 : (tensor<32x64x16xf32>) -> tensor<1x32x64x1x16xf32>
      %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 1, 3, 4] : (tensor<1x32x64x1x16xf32>) -> tensor<4x64x32x2x16xf32>
    ```
    
    Later it can be simplified even more by extracting transpose logic into
    standalone operation, something like this:
    ```
      %2 = stablehlo.reshape %arg3 : (tensor<32x64x16xf32>) -> tensor<1x32x64x1x16xf32>
      %3 = stablehlo.transpose %2, dims = [0, 2, 1, 3, 4] : (tensor<1x32x64x1x16xf32>) -> tensor<1x64x32x1x16xf32>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1, 2, 3, 4] : (tensor<1x64x32x1x16xf32>) -> tensor<4x64x32x2x16xf32>
    ```
    Since `broadcast` is on the way out from stablehlo.
    
    UPD: only patch 1 is accepted.

[33mcommit d182a1c4685becee124d87a2352a2d33d8a72ff7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 25 18:12:19 2024

    Fix operand order for a few quantized tests (#2414)
    
    This is to fix the order of operands in a few generated output testdata
    files.
    
    For example, for the generated quantized testdata file
    `stablehlo/testdata/quantized/div_dtypes_lhs_float32_2__rhs_float32_2_qi8.mlir`,
    the corresponding
    https://github.com/openxla/stablehlo/blob/a710fd72e3fa7e74b25d34d7f7d0bbe18cd408ef/stablehlo/testdata/div_dtypes_lhs_float32_2__rhs_float32_2.mlir#L4
    the intents to divide `%0 = stablehlo.constant dense<[-1.51913178,
    -1.08529949]> : tensor<2xf32>` by `%1 = stablehlo.constant
    dense<[2.67503524, -1.21969211]> : tensor<2xf32>`. But in the quantized
    testdtat file `div_dtypes_lhs_float32_2__rhs_float32_2_qi8.mlir` the
    order is wrong.
    
    Same with other files. (You can find the input file by dropping the
    suffix `_qi8` from the quantized testsdata file name.

[33mcommit 3a0acd356879d202e56725a0d1f87c1df78a5531[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Jun 25 17:53:05 2024

    [RFC] Standardize CustomCallOp to extend backend_config to take a DictionaryAttr (#2097)
    
    This RFC proposes standardizing CustomCallOp to extend backend_config to
    take a DictionaryAttr.
    
    Please review and provide you feedback.

[33mcommit a710fd72e3fa7e74b25d34d7f7d0bbe18cd408ef[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Jun 25 00:48:13 2024

    Bump patch version after integrate 1.1.6 -> 1.1.7 (#2412)

[33mcommit b6129dedc3799fa7714a22dc03b645db7b46486b[m[33m ([m[1;33mtag: [m[1;33mv1.1.6[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Jun 24 18:33:02 2024

    Integrate LLVM at llvm/llvm-project@5cd0ba30f53d (#2410)

[33mcommit 7bd4e772b07867de074c3306e1b23cdbc95bb859[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jun 21 23:56:30 2024

    Bump patch version after integrate 1.1.5 -> 1.1.6 (#2408)

[33mcommit 61826746d640f6342856af5ac71dabe6fbf37ff5[m[33m ([m[1;33mtag: [m[1;33mv1.1.5[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jun 21 22:04:23 2024

    Integrate LLVM at llvm/llvm-project@c07be08df573 (#2407)

[33mcommit 34507173d5b55284666ae9ba9c11e0192405e906[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Jun 21 21:44:00 2024

    Use built-in function to check for emptiness (#2406)

[33mcommit 57d16b101fb8b71bee13fa93e320d0cccef87f52[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 20 20:38:16 2024

    Add testdata files for quantized stablohlo programs (#2404)
    
    Add quantized tests for the following stablehlo operations
    ```
    abs
    cbrt
    ceil
    cosine
    divide
    exponential
    exponential_minus_one
    floor
    log
    log_plus_one
    logistic
    multiply
    negate
    sign
    sine
    sqrt
    subtract
    tanh
    ```
    
    ## A few details
    - These tests are all auto-generated by a `testdata-generator` tool
    which takes an input a non-quantized program file from
    https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret
    or https://github.com/openxla/stablehlo/tree/main/stablehlo/testdata and
    generates a quantized program file in the same format as the input file.
    - The test file names have 1-1- correspondence with its input file name
    appended with `_qi8` appended, suggesting that all the quantized program
    involves `i8` as storage type.
    - The constant argument fed to each test file is the same as that fed to
    the input program.
    
    
    ## A few limitations
    1. Currently we are using the [Stablehlo
    Quantizer](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/quantization/stablehlo)
    to generate quantized program which currently supports quantization of
    `f32` floating point programs only. In future this case be improved.
    - **Corollary**: If the input test files have non-f32 based pograms,
    then it will be preserved unquantized in the output file. That is why we
    can see some of the generated files have non-quantized programs as well.
    3. While scraping the input programs from
    https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret
    or https://github.com/openxla/stablehlo/tree/main/stablehlo/testdata we
    choose only those programs which tests single stablehlo operations. This
    what the `testdata-generator` tool currently supports, but in very near
    future we can expect the tool to work on input test files with mutiple
    operations being tested.
    
    
    Keep an eye on more quantized tests to be populated!

[33mcommit 68020161e915d7947c6bcad6c275269908308fcc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 20 19:11:02 2024

    Interpreter support for quantized type (#2388)
    
    fixes https://github.com/openxla/stablehlo/issues/2373
    
    The PR is rebased on top on
    https://github.com/openxla/stablehlo/pull/2383 and cherry-pick changes
    from https://github.com/openxla/stablehlo/pull/2384.
    
    ### Direction to reviewer
    
    Please review the commit
    https://github.com/openxla/stablehlo/pull/2388/commits/4d7dc1ae715ba0f0fb3671404441df80902dadcd
    **excluding** the following files
      - docs/generated/stablehlo_passes.md
      - stablehlo/transforms/Passes.td
      - stablehlo/transforms/ShapeLegalizeToStablehlo.cpp

[33mcommit 91ed649613e5a51926045b4fd88575ab7a20a33e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 19 00:24:15 2024

    Bump patch version after integrate 1.1.4 -> 1.1.5 (#2400)

[33mcommit d41390c3a731ba038e6363f75fcd135e6f727039[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 18 20:38:15 2024

    Lint fixes driven by internal intregation (#2399)

[33mcommit f1f49945f3862a46ecd6c7fc111e9d7843c2b7da[m[33m ([m[1;33mtag: [m[1;33mv1.1.4[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 18 19:21:39 2024

    Integrate LLVM at llvm/llvm-project@52d87de7a42d (#2398)

[33mcommit 3295a5e130563d18e6cb17a4a08e07cbab04913a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 18 17:17:20 2024

    Legalize quantized stablehlo operation using uniform_quantize/uniform_dequantize (#2394)
    
    This PR provides with a pass to decompose StableHLO quantized programs
    using uniform quantize/dequantize
        operations. For example, the following program
    
        ```mlir
    func.func @add(%arg0: tensor<!quant.uniform<i8:f32,1.0:0>>, %arg1:
    tensor<!quant.uniform<i8:f32,2.0:1>>) ->
    tensor<!quant.uniform<i8:f32,3.0:2>> {
    %0 = "stablehlo.add"(%arg0, %arg1) :
    (tensor<!quant.uniform<i8:f32,1.0:0>>,
    tensor<!quant.uniform<i8:f32,2.0:1>>) ->
    tensor<!quant.uniform<i8:f32,3.0:2>>
          func.return %0 : tensor<!quant.uniform<i8:f32,3.0:2>>
        }
        ```
    
        Will become:
    
        ```mlir
    func.func @add(%arg0: tensor<!quant.uniform<i8:f32, 1.000000e+00>>,
    %arg1: tensor<!quant.uniform<i8:f32, 2.000000e+00:1>>) ->
    tensor<!quant.uniform<i8:f32, 3.000000e+00:2>> {
    %0 = stablehlo.uniform_dequantize %arg0 : (tensor<!quant.uniform<i8:f32,
    1.000000e+00>>) -> tensor<f32>
    %1 = stablehlo.uniform_dequantize %arg1 : (tensor<!quant.uniform<i8:f32,
    2.000000e+00:1>>) -> tensor<f32>
          %2 = stablehlo.add %0, %1 : tensor<f32>
    %3 = stablehlo.uniform_quantize %2 : (tensor<f32>) ->
    tensor<!quant.uniform<i8:f32, 3.000000e+00:2>>
          return %3 : tensor<!quant.uniform<i8:f32, 3.000000e+00:2>>
        }
    
    
    
    Per the
    [docs/spec.md](https://github.com/openxla/stablehlo/blob/main/docs/spec.md),
    following is the exhaustive list of ops which can be interpreted `using
    dq-op-q` strategy. The current PR handles all these ops except for
    un-bolded ones (`DotGeneralOp`, `ConvolutionOp`, `DynamicConvOp`, and
    `AddOp`) which are already lowered to integer match using
    `--stablehlo-legalize-quant-to-int`
    [pass](https://github.com/openxla/stablehlo/blob/main/stablehlo/transforms/StablehloLegalizeQuantToInt.cpp).
    
    1. **AbsOp**
    1. AddOp
    1. **Atan2Op**
    1. **BatchNormGradOp**
    1. **BatchNormInferenceOp**
    1. **BatchNormTrainingOp**
    1. **CbrtOp**
    1. **CeilOp**
    1. **CholeskyOp**
    1. **ClampOp**
    1. **CompareOp**
    1. ConvolutionOp
    1. **CosineOp**
    1. **DivOp**
    1. DotGeneralOp
    1. DynamicConvOp
    1. **Expm1Op**
    1. **ExpOp**
    1. **FloorOp**
    1. **Log1pOp**
    1. **LogisticOp**
    1. **LogOp**
    1. **MaxOp**
    1. **MinOp**
    1. **MulOp**
    1. **NegOp**
    1. **PowOp**
    1. **ReducePrecisionOp**
    1. **RemOp**
    1. **RoundOp**
    1. **RoundNearestEvenOp**
    1. **RsqrtOp**
    1. **SelectOp**
    1. **SignOp**
    1. **SineOp**
    1. **SqrtOp**
    1. **SubtractOp**
    1. **TanhOp**
    1. **TriangularSolveOp**

[33mcommit 03ddb9a3237b53950b605dd2d7b07211d97146bc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 18 16:07:54 2024

    Fix legalization of quantized Min/Max op to Int ops  (#2395)
    
    ## Problem
    
    Consider the following quantized stablehlo max operation
    
    ```
    func.func @max_per_tensor_diff_quant_parameters(%arg0: tensor<!quant.uniform<i8:f32,1.0:0>>, %arg1: tensor<!quant.uniform<i8:f32,2.0:1>>) ->  tensor<!quant.uniform<i8:f32,3.0:2>> {
      %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<!quant.uniform<i8:f32,1.0:0>>, tensor<!quant.uniform<i8:f32,2.0:1>>) -> tensor<!quant.uniform<i8:f32,3.0:2>>
      func.return %0 : tensor<!quant.uniform<i8:f32,3.0:2>>
    }
    ```
    
    Note that the quantization parameters (scales and zps) for operands and
    results are different.
    
    Currently
    [--stablehlo-legalize-quant-to-int](https://github.com/openxla/stablehlo/blob/main/stablehlo/transforms/StablehloLegalizeQuantToInt.cpp)
    legalizes this as follows:
    
    ```
    func.func @max_per_tensor_diff_quant_parameters(%arg0: tensor<i8>, %arg1:  tensor<i8>) ->   tensor<i8> {
      %0 = stablehlo.maximum %arg0, %arg1:  tensor<i8>
      func.return %0 :  tensor<i8>
    ```
    
    which only makes sense when the quantization parameters is same for all
    operands and results, otherwise it is buggy ([why](reason)).
    
    ## Proposed solution
    
    The PR adds a check to allow the legalization only when "the
    quantization parameters is same for all operands and results". Else
    error out.
    
    ### Appendix: Reason that it is a problem
    Consider `a` and `b` are the input quantized values with scale/ zp as
    `[s1, z1]` and `[s2, z2]` resp, and `c` is the output with scal/zp as
    `[s3, z3]`
    
    
    we have from the semantics of the operation
    ```
    (c - z3)*s3 = max[ (a-z1)*s1, (a-z2)*s2 ]
    ```
    
    Only if `z1=z2=z3`, and `s1=s2=s3`, we can simplify the above as `c =
    Max(a,b)` which is what the current legalization is doing.

[33mcommit fab6b632a26942ca1413689b75b81a5984722712[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 17 21:09:52 2024

    Integrate LLVM at llvm/llvm-project@e83adfe59632 (#2397)

[33mcommit 7bee4a2c60778a5ea636d239a8c5c7f279a0cd87[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Mon Jun 17 15:42:32 2024

    support get_smaller_version in python binding (#2396)
    
    Support `get_smaller_version(version1, version2)` in stablehlo's python
    binding.
    Example usage:
    ```python
    version = get_smaller_version(frontend_compiler.get_current_version(), backend_compiler.get_current_version())
    serialize_portable_artifact(module, version)
    ```

[33mcommit 3ce6c32f862a4657464089f0f8aa64f5121d9578[m
Author: Pearu Peterson <pearu.peterson@gmail.com>
Date:   Mon Jun 17 15:40:17 2024

    Improve the accuracy of asin for complex inputs using modified Hull et al algorithm (#2357)
    
    This PR implements the modified [Hull et al
    algorithm](https://dl.acm.org/doi/10.1145/275323.275324) for evaluating
    `asin` on complex inputs as a CHLO function.
    
    The modified Hull et al algorithm template is defined in
    [functional_algorithms:
    asin](https://github.com/pearu/functional_algorithms/blob/9ab746d01538249cdee15a266ffca5cd0283f91d/functional_algorithms/algorithms.py#L215)
    from which `stablehlo/transforms/ChloDecompositionPatternsMath.td` file
    is generated using a provided Python script.
    
    In addition, test files `stablehlo/tests/math/asin*.mlir` are also
    generated that include complex and float samples that magnitudes vary
    from minimal to maximal values of the corresponding floating point
    systems plus complex infinities. The generating Python scripts require
    [MPMath](https://docs.python.org/3/library/math.html) and
    [functional_algorithms](https://github.com/pearu/functional_algorithms)
    which both are available via `pip install` and `conda install`.
    
    Notice that stablehlo itself does not need to depend on `mpmath` or
    `functional_algorithms`. These will be needed only when adding other
    math functions to the list of these generator scripts.
    
    This PR introduces `check.expect_is_close_to_reference` to check if the
    asin results are close to reference values. The closeness is measured in
    terms of ULP differences of `actual` and `reference` values that is
    defined as the number of all floating point values that are larger than
    or equal to `min(actual, reference)` and smaller than `max(actual,
    reference)`.
    
    Before and after this PR, evaluating asin on 1 002 001 samples
    ULP-equispaced over all complex plane or real line, the closeness to the
    corresponding reference values has the following statistics:
    ```
                     current main     ---> this PR
    complex64:
    ULP diff == 0 count is 95493      ---> 817723
    ULP diff == 1 count is 9436       ---> 182678
    ULP diff == 2 count is 2936       ---> 1584
    ULP diff == 3 count is 2527       ---> 16
    ULP diff >= 4 count is 891609     ---> 0
    
    complex128:
    ULP diff == 0 count is 115337     ---> 856449
    ULP diff == 1 count is 2276       ---> 144708
    ULP diff == 2 count is 116        ---> 836
    ULP diff == 3 count is 1026       ---> 8
    ULP diff >= 4 count is 883246     ---> 0
    ```
    that is, before this PR, there is about 90 % chance that `asin` produces
    incorrect or inaccurate results when considering all possible floating
    point complex values. This PR reduces this chance to 0 % with the
    maximal ULP difference=3.

[33mcommit f2a94734bd8a33f3772031d9aa103df8e00cca85[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jun 14 15:59:32 2024

    Bump patch version after integrate 1.1.3 -> 1.1.4 (#2393)

[33mcommit e17bfd45228838fccbe41fdb28e48c2f13c5eef1[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Jun 13 17:47:46 2024

    Bump API version (#2391)

[33mcommit dd48ec58d3bb8d674adf56715d4394102538fa84[m[33m ([m[1;33mtag: [m[1;33mv1.1.3[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 12 22:52:36 2024

    Integrate LLVM at llvm/llvm-project@c012e487b724 (#2392)

[33mcommit 966375dd19fed10ec32bd4cb6fa4c6cba5f35138[m
Author: Yan Xu <45385219+Connor-XY@users.noreply.github.com>
Date:   Tue Jun 11 00:37:58 2024

    fix linalg_passes link (#2389)
    
    As a follow-up on my previous
    [PR](https://github.com/openxla/stablehlo/pull/2387), The linalg_passes
    also needs to link `ChloOps` and `StablehloOps`. They are already
    included in
    [`BUILD.bazel`](https://github.com/openxla/stablehlo/blob/main/BUILD.bazel#L477,L479)
    so only the `CMake` file needs modification.
    
    ---------
    
    Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>

[33mcommit cca707378f03c3646d075907371cdb6a2806258f[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 10 23:31:39 2024

    Fix quant type handling in stablehlo-legalize-quant-to-int pass (#2385)
    
    Originally brought up at
    https://github.com/openxla/stablehlo/pull/2383#discussion_r1630510134
    
    The `stablehlo-legalize-quant-to-int` pass propagated from mho (#2383 )
    uses std::variant to handle per-tensor and per-axis quantized type.
    However, the base class quant::QuantizedType provides a better
    polymorphic way to accomplish the same.
    
    ## Direction to reviewers
    
    Please review the change only in the following commit:
    https://github.com/openxla/stablehlo/pull/2385/commits/ee8b499fa691bbb7b6b3948bdc581dce05abaa39
    
    Reason: This PR is based on
    https://github.com/openxla/stablehlo/pull/2383 which is not merged yet
    and hence has many files which are already reviewed as part of #2383.

[33mcommit 6ebc59d53b02e7e1618c8701f0de058ec0597131[m
Author: Yan Xu <45385219+Connor-XY@users.noreply.github.com>
Date:   Mon Jun 10 19:02:52 2024

    fix StablehloConvertToSignless linkage (#2387)
    
    `StablehloPasses` needs to link `StablehloLinalgTransforms` since
    `StablehloConvertToSignless` needs `RemoveSignTypeConverter` from
    `linalg/transforms/TypeConversion`

[33mcommit 72feb5b539a528a45c7594e55ac9c3ba121c88e6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 10 17:55:54 2024

    Port mhlo-quant-legalize-to-int pass to StableHLO (#2383)
    
    This is the port of
    [mhlo-quant-legalize-to-int](https://github.com/openxla/xla/blob/9449b0851c855ae3f17e8675d95d1f1b68104abd/xla/mlir_hlo/mhlo/transforms/mhlo_passes.td#L378)
    pass to Stablehlo.
    
    
    
    A bunch of tests needs to be updates based on:
    - Different error message because of the differences in quantization
    verification for mhlo and stablehlo.
    - Update check directive to accommodate pretty printing in stablehlo vs
    not pretty printed ops in mhlo (like dot_general)
    - Modify tests with boradcast_in_dim to update the syntax of
    broadcast_dimension (from `DenseIntElementsAttr` to `I64ArrayAttr`)
    - Convert a test with per-axis stablehlo.max with per-tensor
    stablehlo.max as max only supports per-tensor. The intent of the tests
    is preserved.
    
    Also, updated the original code to address creation of attributes of
    type `I64ArrayAttr` instead of `DenseIntElementsAttr`.

[33mcommit 58721dbb1212d82b9b2e256fa8650856b334b33f[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jun 7 19:40:46 2024

    Add verifier for missing constraint for UniformQuantize Op (#2386)
    
    Here is the constraint this PR is adding verifier for
    ```
    (C2) expressed_type(result) = is_float(operand) ? element_type(operand) : expressed_type(operand).
    ```
    
    
    [link](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#constraints-96)
    
    Note: Per the guidelines [(P3) Maintain verification code in verifiers
    and shape functions
    
    ](https://github.com/openxla/stablehlo/blob/main/docs/type_inference.md#p3-maintain-verification-code-in-verifiers-and-shape-functions),
    we are adding the logic in the verifier (not in the type inference) as
    the element type for this op cannot be inferred.

[33mcommit e33721819188b217c852f45017eabc6e6b21967a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 6 19:17:06 2024

    Bump patch version after integrate 1.1.2 -> 1.1.3 (#2382)

[33mcommit 0b26dc6d5df05082ecc1ff398f15cfcacc534df3[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 5 19:49:19 2024

    Move interpreter probe pass to reference (#2381)
    
    Part of #2255.
    
    Currently the `stablehlo_passes` target bundles the reference
    interpreter, which isn't necessary or intuitive. This splits off the
    interpreter probe op pass to remove the dependency from stablehlo
    transforms.

[33mcommit b8ec14541bc700a59ee67a4ddb46467ac18bc442[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 5 17:06:54 2024

    Add diff patch to doc ci error message (#2372)
    
    Will help with fixing doc CI issues without need for locally running the
    CI script:
    
    Note the issue from https://github.com/openxla/stablehlo/pull/2125 where
    this CI script had trouble running on certain linux distros.

[33mcommit 1f4d613cc0531e15ff56fe1b65dcd0bf79edcafc[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jun 5 17:06:43 2024

    Update project stewards (#2380)

[33mcommit d08d0ae2239b7bbdb05f3443de0ceac4108b5f3b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jun 5 01:24:01 2024

    Tweak spec for special case when split_dim = concat_dim in all_to_all (#2379)
    
    This was pointed out by Yuyou on the StableHLO Discord:
    https://discord.com/channels/999073994483433573/999074539138990131/1246199149905449060

[33mcommit 14e2323f0ee3d308c1384fdb806dc6d0c98b16ca[m[33m ([m[1;33mtag: [m[1;33mv1.1.2[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 4 18:19:34 2024

    Fix the return type of python generator function (#2378)
    
    The return type of 'List[tensorflow.TensorSpec]' for generator function
    `StableHLOToTFSavedModel._make_input_signatures` is error-ed out during
    internal testing.
    
    The fix is to annotating the return type as Iterator as all generators
    are basically iterators.

[33mcommit 42bf96fc59ace2382afae27f9a286992bc0404f8[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jun 4 17:57:46 2024

    Fix custom call deserialization bug (#2377)
    
    Discovered an issue when converting PJRT to take VHLO -- when
    deserializing custom_calls with empty operand/result layouts get washed
    away and fail verification, since these two attrs are tied together.
    Also this is potentially bad verification which is another potential
    fix.

[33mcommit 3c6a0673ee69e3ef0917b640ddd2ccc801e38a5c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 3 22:37:08 2024

    Integrate LLVM at llvm/llvm-project@765206e05045 (#2375)

[33mcommit 392108abbf386d42b42c6cab1f7bc0ec7adf757e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jun 3 22:36:14 2024

    Revert API bump (#2376)
    
    PR was not ready to be submitted:
    https://github.com/openxla/stablehlo/pull/2366

[33mcommit 4e9a41ba618e5474748c2128ef3be9e3ae280d74[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jun 3 22:14:51 2024

    Revert bytecode changes from #2259 (#2367)
    
    The bytecode changes are causing issues due to some StableHLO consumers
    using StableHLO directly rather than VHLO.
    
    Temporarily remove the relevant roundtrip tests as well since this
    breaks round-tripping.

[33mcommit 86d8989292699b91c95a755484528851cebcfa9e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 3 22:13:33 2024

    Bump patch version after integrate 1.1.1 -> 1.1.2 (#2374)

[33mcommit 7fd23c5926df4c3c247ed9cd8158aff56c71e767[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Mon Jun 3 17:38:53 2024

    Take rank into account when checking if a DotGeneralOp is simple (#2370)
    
    When the rank of either `lhs` or `rhs` is greater than 2, `isSimpleDot`
    may produce an incorrect result because of how
    `getDefaultDotDimensionNumbers` is implemented.

[33mcommit 21ff94c4c1a366d23f8e18bf45b809172af899ec[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Mon Jun 3 16:41:25 2024

    Add support of float type in specialization of ConvertOp (#2125)
    
    This adds support of following specializations in ConvertOp:
    
    Integer -> Integer (old)
    Integer -> FloatType (new)
    FloatType -> FloatType (new)
    FloatType -> Integer (new)
    
    I have considered approach used in `convertElementsAttr` from
    `mlir-hlo/utils/convert_op_folder.cc`, but
    `DenseIntOrFPElementsAttr::mapValues` doesn't give an option to stop
    conversion in case of failure.
    
    This implementation works for my case:
    
    ```
    %cst = arith.constant dense<1.000000e+00> : tensor<1xf64>
    %cst_0 = arith.constant dense<128> : tensor<1xi64>
    %cst_1 = arith.constant dense<9.9999999999999998E-13> : tensor<1xf64>
    %61 = stablehlo.constant dense<8.000000e+00> : tensor<f64>
    %73 = stablehlo.constant dense<-3.4028234663852886E+38> : tensor<f64>
    %76 = stablehlo.constant dense<1> : tensor<2x128xi32>
    ...
    %86 = stablehlo.convert %76 : (tensor<2x128xi32>) -> tensor<2x128xf32>
    ...
    %90 = stablehlo.convert %cst : (tensor<1xf64>) -> tensor<1xf32>
    ...
    %94 = stablehlo.convert %73 : (tensor<f64>) -> tensor<f32>
    ...
    %106 = stablehlo.convert %cst_0 : (tensor<1xi64>) -> tensor<1xf32>
    ...
    %117 = stablehlo.convert %cst_1 : (tensor<1xf64>) -> tensor<1xf32>
    ...
    %151 = stablehlo.convert %61 : (tensor<f64>) -> tensor<f32>
    ```
    
    ```
    // other constants are equivalent/folded completely
    %73 => stablehlo.constant dense<-3.40282347E+38> : tensor<2x1x1x128xf32>
    %cst_1 => stablehlo.constant dense<9.99999996E-13> : tensor<2x128x1xf32>
    ```
    
    but i am not sure how sound it is with [operator
    specification](https://github.com/tensorflow/mlir-hlo/blob/master/stablehlo/docs/spec.md#convert),
    mainly how conversion should be performed.

[33mcommit 2f97b6ccf5c72618d8c391fd734aafb5cc3b31a7[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jun 3 15:08:48 2024

    Bump API version (#2366)

[33mcommit 2073b377c7291c732a02c40a5da801b5c4ae959a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri May 31 20:23:00 2024

    Make target-version required arg for `stablehlo_to_tf_saved_model` API (#2369)

[33mcommit 28304274610ddc76ecfcfebeefb88d6880d1c37a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 31 19:19:20 2024

    Add Python 3.12 to supported version matrix (#2368)
    
    Some folks are asking for 3.12 support:
    https://discord.com/channels/999073994483433573/999074539138990131/1246155487758520423

[33mcommit 82bfdd489013e9aa85a0b5a5c346c850bb4672af[m[33m ([m[1;33mtag: [m[1;33mv1.1.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu May 30 18:00:28 2024

    Integrate LLVM at llvm/llvm-project@9b79acedd689 (#2365)

[33mcommit 4f5192599ea5dfdc7c0bfdf27ab2ffedc48f5270[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 30 16:09:49 2024

    Bump patch version after integrate 1.1.0 -> 1.1.1 (#2364)

[33mcommit b9346c7adf18d227e22dd38fc41a2fdeb153d5be[m
Author: tomnatan30 <130450079+tomnatan30@users.noreply.github.com>
Date:   Wed May 29 17:50:32 2024

    Add support for batching dims when legalizing `stablehlo.gather` to linalg (#2363)

[33mcommit 25d237f6273361bb29e8436349c7067ee559dca2[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue May 28 18:46:36 2024

    Fix formatting in Python file (#2360)

[33mcommit 982fc0a313df246daa2f16987ced59cb24c3854a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue May 28 18:46:17 2024

    Fix typos in tests involving dynamic_gather (#2362)
    
    Specifically:
    - Some tests specify a slice_sizes attribute on dynamic_gather ops.
    There is no registered attribute by that name, instead slice_sizes are
    provided as an operand.
    - Some tests use dynamic_gather in the name but are in fact concerned
    with gather.

[33mcommit 0f67d83c565b3059d013b8b14bfa32d3156749ca[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Tue May 28 18:30:45 2024

    migrate convert-to-signless pass from mhlo (#2359)
    
    as title, close https://github.com/openxla/stablehlo/issues/2356

[33mcommit f6fa783d046d15d43302b45833395aae8aee685f[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Thu May 23 19:23:43 2024

    Add EvalIotaOp pattern to StablehloAggressiveFolder (#2132)
    
    This PR adds support of IotaOp specialization:
    
    ```
    func.func @eval_iota() -> tensor<3x4x5xi32> {
      %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
      func.return %1 : tensor<3x4x5xi32>
    }
    ```
    is transformed into:
    ```
    func.func @eval_iota() -> tensor<3x4x5xi32> {
      %0 = stablehlo.constant dense<
                [[[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
                 [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
                 [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>
      func.return %0 : tensor<3x4x5xi32>
    }
    ```

[33mcommit c44d9af8d4879adccf1054cb61a53377ae5898cb[m[33m ([m[1;33mtag: [m[1;33mv1.1.0[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon May 20 20:45:23 2024

    Integrate LLVM at llvm/llvm-project@1e5f29af81a5 (#2358)
    
    Had to update the TOSA tests since all floating point types are now
    supported: https://github.com/llvm/llvm-project/pull/91745

[33mcommit d5a22dd393e685ffeeed952e906136f0f860e74c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 17 23:06:21 2024

    Use == instead of equals method (deprecated) (#2355)

[33mcommit 879807058681a06401d1c13847aa5d6ea27cdf2b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 17 15:07:58 2024

    Don't mark dev-wheel as latest release (#2353)
    
    Dev wheels should not be marked as latest release, v1.0 should show up
    as the latest public release.

[33mcommit 3dce5fdfef144a8d9eff63a2f8adf0f4e5e045d6[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu May 16 17:11:49 2024

    Regenerated pass documentation to fix bazel build failure at head (#2352)
    
    documentation is outdated after
    https://github.com/openxla/stablehlo/pull/2253
    
    Validated that  the change actually fixes the failure
    
    
    Follow up: why https://github.com/openxla/stablehlo/pull/2253 CI check
    did not complain?

[33mcommit fe1b3a190411ead0821b1333662d96c802013f0d[m
Author: tomnatan30 <130450079+tomnatan30@users.noreply.github.com>
Date:   Thu May 16 14:58:24 2024

    Small nit fix in gather/scatter batching index computation in evaluator (#2351)
    
    Use `startIndicesIndex` in Ops.cpp for computing gather/scatter batching
    index instead of `batchIndex`

[33mcommit 488749da83d1b3008a1750292f6d4d7449975261[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Thu May 16 02:47:54 2024

    Extract folders from StablehloRefineShapes into new StablehloAggressiveFolder pass (#2253)
    
    Extract folders from StablehloRefineShapes into new
    StablehloAggressiveFolder pass.
    
    This patch addresses the problem of adding new folders and allowing
    potentially lossy computations in folding patterns. It preserves the
    behavior of existing models and workflows that rely on shape refinement
    to fold shapes while assuming non-shape-related computations remain
    unchanged.

[33mcommit b217c4628be6b5f336e889458323fa24a017201f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 15 21:03:00 2024

    Use snake_case instead of PascalCase for generated docs (#2349)
    
    In an effort to be consistent with the rest of the documentation.

[33mcommit e0304c12ed83b7da9351e7e4f9e70aa7d61eda5f[m
Author: tomnatan30 <130450079+tomnatan30@users.noreply.github.com>
Date:   Wed May 15 20:59:19 2024

    [stablehlo] Add batching dims to `stablehlo.gather` and `stablehlo.scatter` (#2259)
    
    Add `operand_batching_dims` and `start_indices_batching_dims` attributes
    to `stablehlo.gather`. `operand_batching_dims` refers to the dimensions
    of the `operand` that are treated as batch.
    `start_indices_batching_dims` refers to the dimensions of the
    `start_indices` that are treated as batch. The corresponding dimension
    sizes must be equal. The semantics is equivalent to concatenating the
    outputs of the gather with each slices of `operand` and `start_indices`.
    
    Similarly, add `input_batching_dims` and `scatter_indices_batching_dims`
    attributes to `stablehlo.scatter`. `input_batching_dims` refers to the
    dimensions of each tensor in `inputs` that are treated as batch.
    `scatter_indices_batching_dims` refers to the dimensions of the
    `scatter_indices` that are treated as batch.
    
    See https://github.com/openxla/stablehlo/pull/2084 for more information
    
    ---------
    
    Co-authored-by: Kevin Gleason <gleasonk@google.com>
    Co-authored-by: Gunhyun Park <gunhyun@google.com>
    Co-authored-by: Gunhyun Park <ghpvnist28@gmail.com>
    Co-authored-by: Abhinav Gunjal <agunjal@google.com>
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit 2d294ae07bab5ad197e393461d4038fef404a037[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 15 18:29:30 2024

    Make clang-format errors more helpful (#2330)
    
    Specifically:
    - Provide a command that can be used to fix formatting problems
    automatically
    - Make it obvious what version of clang-format to use
    
    Also:
    - Don't specify --style=google because we have a .clang-format file that
    already specifies that.
    - Only show the version if there are formatting failures
    
    Closes https://github.com/openxla/stablehlo/issues/2223.

[33mcommit a77d850072f68d087cecd4a4e73e07ef814991cc[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 15 18:28:30 2024

    Add ToC entries for generated pass docs (#2348)

[33mcommit 91a76bc89587197deabe6f5dddba18966642c412[m
Author: Bart Chrzaszcz <bart.chr@gmail.com>
Date:   Wed May 15 16:22:28 2024

    Delete `<optional>` and add `<stdint.h>` from `StablehloAttributes.h` (#2344)
    
    This is for C builds which doesn't have `optional` defined. And include
    `<stdint.h>` as that is what defines `int64_t`.

[33mcommit 14edd9d26d25e7d020df47c41eae0708d7f092ab[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed May 15 00:47:07 2024

    Bump patch version after integrate 1.0.0 -> 1.0.1 (#2345)

[33mcommit 162050be754078c108d6ffc88d7ba603671e15a3[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 15 00:30:11 2024

    Exit after printing usage (#2347)
    
    I implemented this in https://github.com/openxla/stablehlo/pull/2310 for
    a new script and I think it is good to apply this pattern to other
    scripts as well.

[33mcommit b4e4b05bb20f6deaa929969683bc77e308b8f9d5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 15 00:25:56 2024

    Add script to generate pass documentation (#2310)
    
    Ideally, we would have some fully automated process to generate the
    documentation, which would run as a GitHub action on submit. However,
    since we currently store all our docs in the repo itself (and they are
    mirrored on the OpenXLA website by some external process), I think it
    makes sense to use a manually-run script to generate the docs and check
    that the docs are up to date in CI. Indeed, we rarely add new passes or
    update existing pass documentation, so this shouldn't be too onerous.
    
    Fixes https://github.com/openxla/stablehlo/issues/2053

[33mcommit 6051bcdfc057bba9a572da01b413106b4bfa3161[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue May 14 18:56:00 2024

    Update constraint comment for consistency (#2343)

[33mcommit 797bee217e1a041e9aac22cad4db207274596d94[m[33m ([m[1;33mtag: [m[1;33mv1.0.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 14 03:46:36 2024

    Add StableHLO v1.0 Compatibility Updates (#2342)
    
    Update compatibility.md to include the latest compatibility window for
    StableHLO v1.0. Also include link on readme to StableHLO's on-device use
    which utilizes these longer compatibility guarantees.

[33mcommit 8235e9e44a36aee5098c089c0708eec54a97a6c7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 14 03:30:31 2024

    Add featured declated as 'to remove' in spec (#2341)
    
    Was flagged to me that there were a few things explicitly denoted as "to
    be removed" in the spec. I've closed a few of these issues, and added
    the ones that are likely beneficial to the community to the list of
    deprecations.

[33mcommit 72722be7055caf3abf660cea3a9e8f1bed5f6de2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 14 02:11:16 2024

    Integrate LLVM at llvm/llvm-project@a6d7828f4c50 (#2338)

[33mcommit cb8a3a13a6ef6ecd91e2ce4ab0e85aaa3b91aa8a[m
Author: Zhihao Wang <101526713+xiuhu17@users.noreply.github.com>
Date:   Tue May 14 02:11:07 2024

    Update TypeInference.cpp (#2328)
    
    add static cast to int64_t type
    
    Context: I was trying to setup the environment of XLA, and find there
    was a syntax error which caused by the non consistent type of parameters
    in std::max() function.
    
    There was an error on Macos Sonoma 14.3 , using LLVM 17.0.6.

[33mcommit 3cbe8f0f54e480cd51ae54758b838148f7629dee[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 14 02:04:37 2024

    Make dynamic conv operand require static dim sizes, fix padding name (#2337)
    
    Changes:
    
    - `d_padding --> padding`
    - `HLO_2DIntTensor:$d_padding --> HLO_Static2DIntTensor:$padding`
    
    And cleanup for the two changes.

[33mcommit 06c8fdb83e8a0d20125630fe432a51050202cd8b[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue May 14 01:57:35 2024

    Remove obsolete comments (#2339)

[33mcommit 24536ab626cb17fd72c14a91039f38d02afeb036[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue May 14 01:57:20 2024

    Bump StableHLO to 1.0.0 (#2329)
    
    This was announced on openxla-discuss about a month ago. See the post
    for details:
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/CPuSw-zSse8
    
    All items from the StableHLO v1.0 Release project have been addressed:
    https://github.com/orgs/openxla/projects/23

[33mcommit f822bea6d3d8245c9aaa5f0507a0dc907dd25ffd[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue May 14 01:48:32 2024

    Inline `StableHLO_ConvolutionAttributes` to tablegen (#2335)

[33mcommit a2b434536659090319cf50713f4aee2e2efdf1e5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon May 13 23:36:58 2024

    Avoiding linking to specific line in VhloDialect.td (#2336)
    
    The line the log is at is not super likely to change, but the file is
    small and the log is easy to locate anyway.

[33mcommit 966e4fba5fcf5268c13232ff2d9ad09d8aee9b0c[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon May 13 22:18:55 2024

    Address remaining feedbacks from #2312 (#2327)

[33mcommit 2d35f55a7f5580ee74788096fefa27e8e065a319[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 13 21:15:08 2024

    Add versioning note to spec (#2333)

[33mcommit 195a448f06c3346c3168b65608cb9dfea386e974[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon May 13 21:09:34 2024

    Split out VHLO contribution guidelines into their own file (#2332)
    
    This may help with making sure we don't miss any items when making
    changes to VHLO. The main vhlo.md file should focus on what VHLO is.

[33mcommit f606a36ffba7a68aeab87a525aee28d5099c1bc9[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon May 13 21:01:11 2024

    Fix incorrect reference for DynamicGather spec (#2334)

[33mcommit 06bcb0de3bd03c0592398470757fe0a87c8ebb7c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 13 20:36:27 2024

    [RFC] StableHLO v1.0 Opset Deprecations & Cleanups (#2283)
    
    A proposal to remove redundant operations from StableHLO before
    long-term compatibility guarantees go into place.
    
    High level summary:
    - Remove `CreateTokenOp`, `TraceOp`, `BroadcastOp`, `DotOp`,
    `UnaryEinsumOp`, `RealDynamicSliceOp`.
    - Enhance `DynamicSliceOp`.
    - Move `CrossReplicaSumOp` to CHLO.
    - Hopefully remove/move to CHLO (need feedback) `MapOp`, `RngOp`,
    `EinsumOp`, `TorchIndexSelectOp`, `GetTupleElementOp`, `tuple` and `tuple` type.
    
    OpenXLA Discuss post:
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/sBAkvnd2bcA
    
    Related tickets: #2176, #3

[33mcommit 9b3820595e765dd4ffef2d919cee63cb1b746912[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon May 13 19:56:38 2024

    Add vhlo version log for v0.20.0 (#2331)
    
    Thanks @mlevesquedion for the catch!

[33mcommit bbda3a8805eb7737ccef532e37a9a39191908c08[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon May 13 17:05:21 2024

    Integrate LLVM at llvm/llvm-project@3a8316216807 (#2318)
    
    This PR bumps LLVM version to the latest LLVM integration commit.
    
    (backported from g3)
    1. Tosa dialect op change form div to int_div
    2. CHECK test expected log change
    
    (manually updated CHECK test expected logs)
    3. stablehlo_legalize_to_vhlo.0_20_0.mlir test file, it is newly
    introduced
    
    (to fix asan cmake build failure)
    3. disabled allow_user_poisoning. Created tracker
    https://github.com/openxla/stablehlo/issues/2326 to reenable it. Thanks
    @mlevesquedion for the workaround.

[33mcommit 12fd0a9e7b3c6f3dea3defc513870c962e62726d[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Sat May 11 00:57:09 2024

    dynamic_broadcast_in_dim  followup cleanup (#2325)
    
    Separate constraints are not required, C8 is taking care of C9, C10.
    Same verifier check is applicable.

[33mcommit 532a0ef44f9fdacc878874ae47d8fa79c8cf4160[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Sat May 11 00:32:55 2024

    Address post-submit comments on #2306 (#2324)
    
    Thanks for taking the time to review the code!
    
    FYI:
    * I didn't quite understand this comment:
    https://github.com/openxla/stablehlo/pull/2306#discussion_r1597278405
    * I didn't implement this suggestion (I think the existing code is
    correct):
    https://github.com/openxla/stablehlo/pull/2306#discussion_r1597281592

[33mcommit 306b7e9373a6476a34afc3ce6b5292664c34a46e[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri May 10 23:57:21 2024

    dynamic_broadcast_in_dim op spec (#2315)
    
    Added Spec, Updated Verifier, ODS def and interpreter for the op.
    
    
    Testing:
    ```
    I1.  operand is type tensor or quantized tensor : ODS
    I2. output_dimensions is  1-dimensional tensor of integer type : ODS
    I3. broadcast_dimensions is 1-dimensional constant tensor of integer type : ODS
    I4. known_expanding_dimensions is 1-dimensional constant tensor of integer type : ODS
    I5. known_non_expanding_dimensions is 1-dimensional constant tensor of integer type : ODS
    C1.  element_type(result) = element_type(operand)  with special handling for per axis tensor : Verifier, test added
    C2. size(broadcast_dimensions) = rank(operand) : Verifier, test added
    C3.  0 <= broadcast_dimensions < rank(result) :  Verifier, test present
    C4.  is_unique(broadcast_dimensions) : verifier, test added
    C5.  ... : Verifier, Tests present
    C6. ... :  Verifier, Test added
    C7. .  size(output_dimensions) = rank(result) : Verifier, test present
    C8, C9, C10. is_unique(known_expanding_dimensions + known_non_expanding_dimensions) :  Verifier, test added
    C11, C12. 0 <= known_expanding_dimensions, known_non_expanding_dimensions  < rank(operand) : Verifier, test added
    ```
    
    Interpreter:
    Supports static shapes only.

[33mcommit 1f107e0907cfd2046693d3dc53ba6aac409374b6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 10 19:00:40 2024

    Revert "Fix asan build issue (#2108)" (#2323)
    
    This reverts commit 538b1e11ca83fd5f14390e73a54c41af8f7ee87d.
    
    The underlying issue was fixed in upstream in
    https://github.com/actions/runner-images/pull/9513
    (https://github.com/actions/runner-images/issues/9491)

[33mcommit 94b6ffbba4bb96ef2073f19369e3bc175f1484a9[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 10 18:06:49 2024

    Derive adaptor from OpTy in linalg dot lowering template (#2322)
    
    Follow-up to
    https://github.com/openxla/stablehlo/pull/2319#discussion_r1597007257

[33mcommit af7ce72c5328050102a6484a89cf9580ef3e4123[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 10 18:06:32 2024

    Use BroadcastOp linalg lowerings for simple BroadcastInDimOps (#2319)
    
    In a similar vein as #2314, this PR eases the process of deprecating
    BroadcastOp by reusing the BroadcastOp lowerings for BroadcastInDimOp
    ops derived from BroadcastOp ops.
    
    Unfortunately, BroadcastInDimOp requires a static result type, so not
    all Broadcast ops can be converted through the deprecated op
    legalization.
    
    #2311

[33mcommit 1431e1c26eb2a6b2fd8cdcb4f7dc5ce6936cf527[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri May 10 17:30:54 2024

    Add spec, interpreter, and verifier for DynamicConvOp (#2312)

[33mcommit 93d088195e75809808c0fa121ec24abd580c164c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri May 10 17:02:01 2024

    Fix typo on dynamic_gather (#2321)

[33mcommit 13fc15576f12e275f39135b3c887fa1aa58c6d79[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri May 10 17:01:53 2024

    dynamic_gather::slice_size cannot be of Index type (#2320)

[33mcommit 1d9391536eca04b884ef56ca574a7740419b80c7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 10 00:22:12 2024

    Remove unused dotop lowering from linalg (#2317)
    
    Many of these map to scalar patterns are used in linalg, but using a
    code search, this DotOp pattern is never initialized. There is another
    dot pattern which is used instead.

[33mcommit e53212b655ba4a3c2e2754e0c1f30a14185ed4e4[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 10 00:22:04 2024

    Migrate TOSA support from Dot to DotGeneral (#2316)
    
    This first phase adds support for DotGeneralOp's that are simple dots
    (i.e. same semantics as DotOp).

[33mcommit 2cc4a3cb8ddd8ca38b6bc367ac96350133d9a84d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri May 10 00:15:25 2024

    Spec dynamic_gather (#2309)
    
    fixes https://github.com/openxla/stablehlo/issues/2290

[33mcommit e5b9c99c94dd25b75345de576fc11a31ec46c7ff[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 9 23:40:57 2024

    Remove TraceOp from StableHLO (#2295)
    
    Part of #2283. This op doesn't seem to have any uses in frameworks or
    compilers. It shouldn't cause any large issues to remove this op all
    together, also since this op was never specced, it is exempt from
    compatibility guarantees.
    
    In the case that it is used somewhere, I would recommend migration to a
    custom_call.

[33mcommit 9d4f27d700b86f384a4ace67d2da103466888295[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 9 22:54:27 2024

    Use DotOp linalg lowerings for simple DotGeneral ops (#2314)
    
    We are in the process of deprecating DotOp (#2296).
    
    To ease the deprecation, we want DotGeneral ops that are equivalent to
    simple DotOp ops to be lowered to linalg in the same way.
    
    In this PR:
    - Add a "isSimpleDot" method to DotGeneralOp which determines if a
    DotGeneralOp can be expressed as a DotOp.
    - Reuse the DotOp linalg lowerings for DotGeneralOp when isSimpleDot.
    - Remove "DotOperationType" as it is redundant with the linalg op types.
    
    https://github.com/openxla/stablehlo/issues/2311

[33mcommit c31ba3d4d275343e7d8225dee0b9bfc22d2b4b40[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 9 21:31:36 2024

    Remove redundant rank checks in linalg lowerings (#2313)
    
    Operands and results are enforced to be ranked by ODS.

[33mcommit 4d66368c6819dc61dd489025f5f9afa5f1c445d6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 9 20:49:09 2024

    Accumulate shape product in int64 to prevent overflow (#2308)
    
    Co-authored-by: Tom Ward <tomward@google.com>

[33mcommit aa576d6d59bd1922a83ee5a806d0928dcab687cb[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 9 17:47:32 2024

    Spec dynamic_pad (#2306)
    
    This includes:
    - Adding partial support to the interpreter (only when the result is
    static)
    - Cleaning up and completing the verifier
    - Updating the ODS
    - Adding relevant tests
    
    I made a few drive-by fixes to related logic/ops as well.
    
    Constraint coverage:
    
    ```
    I1. operand is tensor or per-tensor quantized tensor: ODS
    I2. padding_value is 0-dimensional tensor or per-tensor quantized tensor: ODS
    I3. edge_padding_low is integer tensor: ODS
    I4. edge_padding_high is integer tensor: ODS
    I5. interior_padding is integer tensor: ODS
    C1. element_type(operand) = element_type(padding_value) = element_type(result): ODS
    C2. size(edge_padding_low) = size(edge_padding_high) = size(interior_padding) = rank(operand): ODS + verifier
    C3. 0 <= interior_padding: verifier
    C4. shape(result) = shape(operand) + edge_padding_low + max(shape(operand) - 1, 0) * interior_padding + edge_padding_high: verifier
    ```
    ref https://github.com/openxla/stablehlo/issues/2267
    Fixes #2292

[33mcommit 73c32a33fe228da1f3c72d95b2a669540c40ae1f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 9 17:14:11 2024

    Add initial deprecation upgrade patterns for testing impact radius (#2296)
    
    This pass is intended to be used to test the blast radius of the
    proposed deprecation changes in #2283 and help with migration of passes
    and other tooling.
    
    Still todo - patterns for the following:
    - Einsum pattern
    - TorchIndexSelect pattern
    - RNG pattern (if possible)
    - RealDynamicSliceOp (requires dynamic_slice update first)

[33mcommit 49d93822e1301dc8f29d88f1af0c989ed8da4ee3[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu May 9 16:03:46 2024

    Add link to StableHLO page on official OpenXLA website (#2299)

[33mcommit eb33134d8e8721357270a05b54f5f8fec4fb5f5f[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu May 9 01:18:31 2024

    Bump patch version after integrate 0.19.10 -> 0.19.11 (#2307)

[33mcommit f3835c5c1e1821a7b34450cd643b0301f95ed133[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed May 8 23:48:47 2024

    dynamic_iota op : match ODS description with the spec (#2305)

[33mcommit d5368c92c8ccc4b86af40f94db75a28beb7d726d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 8 21:37:15 2024

    Remove dynamic_{iota,reshape} from list of ops to spec (#2303)
    
    These ops were specced in https://github.com/openxla/stablehlo/pull/2270
    and https://github.com/openxla/stablehlo/pull/2284.

[33mcommit ab780f6990ed63b9b66df44a0c47eae7c1f69d73[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed May 8 19:07:02 2024

    Change uses of deprecated method a.cast to cast<>(a) (#2301)
    
    Back porting from the integrate

[33mcommit bac0cd166d8369f8030a285a148409488dd8c89c[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed May 8 16:55:41 2024

    dynamic_reshape op spec (#2284)
    
    * Constraints in the spec:
    ```
    I1 operand is tensor or quantized tensor
    I2 output_shape is 1-dimensional tensor constant of type si64
    ```
    * Test coverage;
    ```
    I1.1  operand is tensor or quantized tensor.   (covered by ODS)
    I2.1  output_shape is not a 1-dimensional tensor constant of type si64 .   (covered by ODS)
    C1.1 if !is_per_axis_quantized(operand),  element_type(result) = element_type(operand).    (Added a verifier logic and a  test)
    C1.2 if is_per_axis_quantized(operand),  element_type(result) = element_type(operand) except
           quantization_dimension(operand) and quantization_dimension(result) may differ.    (Validated; but test can't be added at this moment because interpreter doesn't support dynamic shapes)
    C2.1  size(operand) = size(result).  (can't be verified at static time for dynamic ops)
    C3.1  quantization constraints.  (verifier logic is common for reshape and dynamic_reshape op. Tests are already present for reshape op. Not needed for dynamic_reshape.)
    C4.1  rank(result) = size(output_shape)   (Already present, validated)
    ```
    
    * Reference interpreter:
    partial implementation to support only static shapes.
    
    
    
    
    ref:#2267
    fixes:https://github.com/openxla/stablehlo/issues/2293

[33mcommit 4213d63bdf4a14d95321f12ad3aaef120b199423[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed May 8 16:32:25 2024

    Update table formatting and fix typos (#2300)
    
    Updates spec for convolution to adhere to 80-char limit and formatting
    table properly.

[33mcommit 8ba7728d3fdc3ea882e893ee7e53255c95ee0e5a[m[33m ([m[1;33mtag: [m[1;33mv0.19.11[m[33m, [m[1;33mtag: [m[1;33mv0.19.10[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue May 7 17:39:26 2024

    Use correct license for LegalizeCompositeToCall (#2297)
    
    This was added with the wrong license a while ago. I think I copied the
    license from StablehloAggressiveSimplication.cpp and changed the
    authorship and year, but didn't realize that the actual license
    designation was wrong.

[33mcommit 800919cc001c83769b813486fd7021b5fd12d0b3[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue May 7 17:07:45 2024

    Delete chlo.minimum_broadcast_shapes op (#2287)
    
    This op is only used by TF's KernelGen, where it can be supported in
    other ways. (Furthermore, use by a single framework/backend is usually
    not sufficient to include an op in CHLO/SHLO).
    
    Context: #1991 (P1) -- This proposal was approved as a part of the
    Dynamism RFC.

[33mcommit 2eb830a3b2fe8472d8e8b26ed626c9642f64392f[m
Author: tomnatan30 <130450079+tomnatan30@users.noreply.github.com>
Date:   Tue May 7 15:24:06 2024

    [RFC] Add batching dims to `stablehlo.gather` and `stablehlo.scatter` specification (#2084)
    
    This RFC proposes adding batching dimensions (across all operands and
    result) to `stablehlo.gather` and `stablehlo.scatter`, similar to
    `lhs_batching_dims` and `rhs_batching_dims` in `stablehlo.dot_general`
    Please provide any feedback you feel is valuable.

[33mcommit fdfab2e4d0837a2307b9e834494a9535d4d5a996[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon May 6 22:35:34 2024

    dynamic_iota op spec (#2270)
    
    Constraints in the spec:
    ```
    I1. output_shape is 1-dimensional tensor constant of type si64
    I2. iota_dimension of type si64
    C1. 0 <= iota_dimension < size(output_shape)
    C2. rank(result) = size(output_shape)
    ```
    Test coverage;
    ```
    I1.1  output_shape is not a 1-dimensional tensor constant of type si64    (covered by ODS) si64 (covered by ODS)
    I2.1  iota_dimension is not of type
    C1.1 0 <= iota_dimension   (Already present)
    C1.2  iota_dimension < size(output_shape)   (Already present)
    C2.1  rank(result) = size(output_shape)   (Already present)
    ```
    
    Reference interpreter:
    partial implementation to support only static shapes.
    
    ref: https://github.com/openxla/stablehlo/issues/2267

[33mcommit b6e3c984b178537f0b17a77ed32ab27b8e209ce0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon May 6 21:02:56 2024

    Verify StableHLO Quantized Types (#2203)
    
    fixes https://github.com/openxla/stablehlo/issues/2074
    
    It seems there are some use-cases, like quantized reduce wanting to
    perform the body computation in a higher bitwidth, which avoids
    constraining and hence is not part of the verification checks.
    
    ```
    (C1) num_bits(storage_type) < num_bits(expressed_type).
    ```
    
    Added a ticket to address it separately
    https://github.com/openxla/stablehlo/issues/2205

[33mcommit a0d3bd85315ad9fb4eec4287ec725bd75f78a342[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon May 6 20:45:32 2024

    Remove unused chlo.dynamic_reshape and downstream ops (#2286)
    
    The downstream ops are stablehlo.compute_reshape_shape and
    stablehlo.cstr_reshapable. These ops were used to implement tf.reshape
    but are no longer needed. (Furthermore, use by a single
    framework/backend is usually not sufficient to include an op in
    CHLO/SHLO).
    
    Context: https://github.com/openxla/stablehlo/issues/1991 (P1) -- This
    proposal was approved as a part of the Dynamism RFC.

[33mcommit 75d2f6c5b3d0d4e0179a2e37e7af58a8213db9ab[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon May 6 17:26:45 2024

    Remove a few more deprecated method uses (#2285)
    
    I think these were added after/during the first round of changes.
    
    Context: https://github.com/openxla/stablehlo/pull/2180

[33mcommit ab92adeda9119a6c3914cd42367b0a2b70765e91[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 3 22:29:01 2024

    Add helper to verify output shape matches result type (#2269)
    
    And use the helper to verify shape operands in DynamicBroadcastInDim,
    DynamicIota and DynamicReshape.
    
    Also, reorganize the isCompatibleForHloTypeInference logic a little bit
    to avoid duplication.
    
    This is a follow-up/generalization of
    https://github.com/openxla/stablehlo/pull/2264.

[33mcommit 40e70d295b6ee6c00a348a008f87f80c83de4f50[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri May 3 19:04:57 2024

    Update test name to match tested element type (#2282)

[33mcommit e0c6d5d4a7a6caa804d4d7bb6f6a6ca8df704a48[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri May 3 16:44:21 2024

    Use map_to_vector instead of map_range ‚àò to_vector (#2281)

[33mcommit c0132de0d56a28df2bf7f65c725b541b8eb62227[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu May 2 23:06:01 2024

    Use free function instead of method for dyn_cast_or_null (#2280)
    
    The method variant is deprecated.

[33mcommit 41e712268dd697b9d6e07834986bbf9532570c0d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 2 21:25:41 2024

    Ensure result shape is static before folding (#2279)
    
    Added test case which crashes before this change due to pattern
    application order.
    
    There's probably some overly safe checks here (like checking that ops
    which cannot have dynamic output shapes - broadcast_in_dim /
    get_dimension_size) but there's no harm, and its safer if we ever add
    any additional constraints on result types that can be folded in the
    future.

[33mcommit e45abc24d0e80714928ed978645809add9ec79c0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu May 2 16:33:04 2024

    Interpreter support for dynamic programs (#2275)
    
    This PR adds a pass-pipeline to refine arguments and shapes of a program
    followed by
    [StablehloCanonicalizeDynamism](https://github.com/openxla/stablehlo/blob/main/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp)
    to replace dynamic StableHLO ops with the corresponding static
    counterparts if applicable* (see note below).
    
    The goal is to interpret dynamic programs after applying the above
    pipeline.
    
    partially-fixes https://github.com/openxla/stablehlo/issues/2202
    
    
    Note:
    
    For a programs like
    
    ```
    func.func @main(%arg0: tensor<?xf32>, %arg1: tensor<2xi64>) -> tensor<?x?xf32> {
        %0 = stablehlo.dynamic_reshape %arg0, %arg1 : (tensor<?xf32>, tensor<2xi64>) -> tensor<?x?xf32>
        return %0 : tensor<?x?xf32>
    }
    ```
    
    when applied through the above pass-pipeline with static input shapes
    like `tensor<6xf32>,tensor<2xi64>`, will not be able to replace
    `stablehlo.dynamic_reshape` with its static counterpart `reshape` as
    [StablehloCanonicalizeDynamism](https://github.com/openxla/stablehlo/blob/main/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp)
    can only do that when the second argument of `stablehlo.dynamic_reshape`
    is constant, which is not true in this case.

[33mcommit de6488a8d88b0c8f954fabd40d0fc75dc1e15765[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu May 2 01:23:19 2024

    Integrate LLVM at llvm/llvm-project@2914a11e3fad (#2266)

[33mcommit a78eb30a7cd3bc65ddf089763c008c0633f43f7c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu May 2 01:20:24 2024

    Bump patch version after integrate 0.19.9 -> 0.19.10 (#2277)

[33mcommit b725ed9fd312db7e775048630df29e24fa9db15d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 1 21:40:14 2024

    Implement ConditionallySpeculatable for remaining dynamic ops (#2242)
    
    Included ops:
    - DynamicPad
    - RealDynamicSlice
    - DynamicConv
    - DynamicGather
    
    I refactored the logic to check speculatability for shaped ops to enable
    reuse and allow ops with more than one shape-related operand to be
    checked.
    
    This is the last change adding new speculatability
    implementations. All the other ops are either done, pure, or
    deprecated.

[33mcommit 0f8e5083c21fda6eb224e7907ec8af6b1395c5fa[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Wed May 1 20:02:37 2024

    iota op spec :  replace "result_index" with "output_index" (#2272)
    
    iota op spec doesn't define `result` but uses `result_index`.

[33mcommit 131b480a3aa9474743947741b3d6eedc9f398ad1[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed May 1 18:49:40 2024

    Use llvm::enumerate to simplify loop (#2276)

[33mcommit e2201e2af69c2b0d32370cfe0deafe2681c7b3e9[m
Author: Doyeon Kim <46413178+doyeonkim0@users.noreply.github.com>
Date:   Wed May 1 15:57:43 2024

    Add spec and verifier for quantized dot_general op (#2252)
    
    This PR adds spec for hybrid quantized op discussed in the
    [RFC](https://github.com/openxla/stablehlo/pull/1792) and implements
    verifier accordingly.
    Please feel free to take a look. Thank you!
    
    cc @sdasgup3 @loganchien @abhigunj

[33mcommit 6d846524098a910f9759af45ad6331fb446e7f65[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed May 1 15:49:08 2024

    [RFC] Increase StableHLO Compatibility Guarantees (#1882)
    
    This PR does not contain an actual RFC - it's merely tracking a
    discussion on openxla-discuss:
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/rfd30zKR9uU.
    Once this RFC reaches an outcome, we'll update the status field in this
    markdown file and merge this PR to document that.
    
    Posting PR on behalf of @zichuan-wei and @nutsiepully who drove this RFC
    on OpenXLA Discuss.
    
    ---------
    
    Co-authored-by: Zichuan Wei <zichuanwei@google.com>
    Co-authored-by: Pulkit Bhuwalka <pulkitb@google.com>

[33mcommit 1c91b78cbadcf9a782169fce4c36737ce291f76a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed May 1 05:41:15 2024

    Fix the bitness of addition/subtraction in quantize op (#2234)
    
    fixes #1627
    
    Requesting the following for review:
    @loganchien
    @paulinesho
    @doyeonkim0

[33mcommit 75f7c0a846d5afb49525d0a27e19d8c3350311db[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed May 1 00:48:14 2024

    Fix formatting for spec table (#2273)

[33mcommit a7904883754a455bfd0c7636cbac1f99c2c24fe5[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Apr 30 22:04:13 2024

    Include command to build StableHLO with python bindings (#2251)
    
    The doc originally stated that the script preceding the instruction may
    have already built MLIR with python bindings, but that isn't true. I've
    included the actual command that builds with python bindings.

[33mcommit 205f405613a14554095d678ebd87c1ced924f7af[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 30 17:38:05 2024

    Add tests for uniform_{de,}quantize speculatability (#2268)
    
    These ops are UnaryElementwise but they were missed in
    https://github.com/openxla/stablehlo/pull/2150

[33mcommit 0a915359d934f16949961381de0c23b180d1b1a1[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 29 20:53:22 2024

    Clean up op speculatability tests (#2265)
    
    Mostly, simplify redundant variable names, remove unnecessary ops in
    nested regions, and move ops around/add whitespace for consistency and readability.

[33mcommit 90d1c3a2a68c1a126333011d531ad6ce02a6d913[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 29 20:52:11 2024

    Include output shape in shape mismatch error message (#2264)
    
    As discussed in
    https://github.com/openxla/stablehlo/pull/2231#discussion_r1571148784,
    this will likely help with debugging shape mismatches.

[33mcommit 72bab32a621c04295d898069c295fd4c09a4c057[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 29 17:49:25 2024

    Implement ConditionallySpeculatable for {Dynamic,}BroadcastInDim (#2233)

[33mcommit ae7fd1523e46d2ff1e7ef0a3ad5a224685b00ea7[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 29 16:54:08 2024

    Add steps for side effects/speculatability to spec_checklist (#2263)

[33mcommit 7d95a16568f44cd0f2f997bfe18345dcaa645ad6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 29 16:37:27 2024

    Fix ConditionalSpeculatability implementation for DynamicReshape (#2261)
    
    Both the input and the shape must be fully known statically. Whether the
    output is dynamic or not does not matter.
    
    Indeed, consider e.g.:
    
    ```
    func.func @foo(%arg0: tensor<?x?xf64>) {
        %constant_shape = stablehlo.constant dense<[2, 3]> : tensor<2xi32>
        %0 = stablehlo.dynamic_reshape %arg0, %constant_shape : (tensor<?x?xf64>, tensor<2xi32>) -> tensor<?x?xf64>
        return
    }
    ```
    
    The input is dynamic, so it could turn out to have e.g. 10 elements
    instead of the expected 6. Similarly, if the shape is unknown:
    
    ```
    func.func @foo(%arg0: tensor<2x3xf64>, %unknown_shape: tensor<2xi32>) {
        %0 = stablehlo.dynamic_reshape %arg0, %unknown_shape : (tensor<2x3xf64>, tensor<2xi32>) -> tensor<?x?xf64>
        return
    }
    ```
    
    Again, the shape could turn out to be e.g. `[2, 5]` at runtime and so
    the reshape's behavior would be undefined.

[33mcommit b6406a43b48b7803f3efdbc235b1fbb5da782449[m[33m ([m[1;33mtag: [m[1;33mv0.19.9[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 26 21:57:05 2024

    Fix extraDeclarations for AllGather (#2262)
    
    Missing the common declarations.

[33mcommit faf995aec3aab212d79a7a5a18f26a4b33095e78[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 26 16:24:01 2024

    Fix deprecated use of dyn_cast method (#2260)
    
    Co-authored-by: Christian Sigg <csigg@google.com>

[33mcommit 3202d37c457e45b1c31ad440339ad28ed56b7a3a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 25 16:39:17 2024

    Remove stray \n in stablehlo_to_tf example (#2258)

[33mcommit 02e0006eed70365cdae6c6e89cdb3323f056e382[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 25 06:47:57 2024

    Update PyTorch example to support 2.3.0 (#2257)
    
    Colab by default ships with outdated versions of `torch` and
    `torchvision` (given that they just released a new version on 4/24/24).
    Adding this explicit upgrade to pip will keep these tutorials more
    stable.
    
    Also it turns out that dynamic shape export wasn't backported into the
    2.3 release cut, so while this does work at nightly, it still won't be
    tested in the 2.3 colab, updated wording to specify 2.4.

[33mcommit c2b7f6bb15b6d4938af1095d881cbee9bbaa5bc0[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 25 06:03:21 2024

    Fix link in JAX tutorial (#2256)

[33mcommit f4d1000595d0186fdcaacd4034094dfcf30f2604[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 25 05:47:57 2024

    Add StableHLO to SavedModel Tutorial (#2250)
    
    [Preview](https://github.com/GleasonK/stablehlo/blob/d8894397b73d19ee93b9509024c02f705b631718/docs/tutorials/savedmodel-embed.ipynb)
    
    Add a tutorial to demonstrate the new StableHLO to SavedModel APIs.

[33mcommit 0b7ecf3e353843746adcbc7763f86348a3d4ed9b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Apr 24 22:00:14 2024

    Add header for M_PI to fix windows build failure on some systems (#2254)
    
    Once we have c++20 support we should flip this to use `std::numbers::pi`
    from the `<numbers>` header.

[33mcommit ae2a1141142204840fb54951102c9ee41f7d1168[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 23 23:02:01 2024

    Make TF tests optional in python CI build script. (#2249)
    
    This should be optional for users relying on these CI scripts to build
    StableHLO python bindings in other projects / locally.

[33mcommit c65a30d12661d1dad5f52a1dab9b6b6f39b0bc17[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Apr 23 22:09:08 2024

    Add a note at type_inference.md for adding custom verifier if return type inference is not feasible (#2158)
    
    if return type of the op is inferable, it can be created using
    `op::build` which does not take result type as input. For such ops it is
    recommended to move verification logic to shape functions to avoid
    missing on any verification during op creation. If return type inference
    is not feasible use custom verifier.

[33mcommit b53ad662cf4a7cd2a1044d060027f4b4f8219008[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 23 21:49:26 2024

    Add TF dependency to wheel build action (#2248)
    
    We want to run all python tests before publishing a wheel. Running all
    tests has a TF dependency to run savedmodel tests.
    
    See:
    
    https://github.com/openxla/stablehlo/actions/runs/8807435054/job/24174472438

[33mcommit 51ec8d1130c4ed61785750d753d0470521ef3134[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Apr 23 20:22:24 2024

    Update conv docs and tests per interpreter guideline (#2239)
    
    This is part 3 of #1964 to implement the remaining parts of #1314.
    
    One notable change in TypeInference.cpp is (C27), whose verification
    differs whether element type is quantized.
    
    We have the following constraints in the spec (excluding
    quantization-related constraints C28-C33):
    
    ```
    (I1) `lhs` tensor.
    (I2) `rhs` tensor.
    (I3) `window_strides` 1-dimensional tensor constant of type `si64`.
    (I4) `padding` 2-dimensional tensor constant of type `si64`.
    (I5) `lhs_dilation` 1-dimensional tensor constant of type `si64`.
    (I6) `rhs_dilation` 1-dimensional tensor constant of type `si64`.
    (I7) `window_reversal` 1-dimensional tensor constant of type `i1`.
    (I8) `input_batch_dimension` constant of type `si64`.
    (I9) `input_feature_dimension` constant of type `si64`.
    (I10) `input_spatial_dimensions` 1-dimensional tensor constant of type `si64`.
    (I11) `kernel_input_feature_dimension` constant of type `si64`.
    (I12) `kernel_output_feature_dimension` constant of type `si64`.
    (I13) `kernel_spatial_dimensions` 1-dimensional tensor constant of type `si64`.
    (I14) `output_batch_dimension` constant of type `si64`.
    (I15) `output_feature_dimension` constant of type `si64`.
    (I16) `output_spatial_dimensions` 1-dimensional tensor constant of type `si64`.
    (I17) `feature_group_count` constant of type `si64`.
    (I18) `batch_group_count` constant of type `si64`.
    (I19) `precision_config` variadic number of enums of `DEFAULT`, `HIGH`, and `HIGHEST`.
    (C1) `N = rank(lhs) = rank(rhs)`.
    (C2) `size(window_strides) = N - 2`.
    (C3) `0 < window_strides`.
    (C4) `shape(padding) = [N - 2, 2]`.
    (C5) `size(lhs_dilation) = N - 2`.
    (C6) `0 < lhs_dilation`.
    (C7) `size(rhs_dilation) = N - 2`.
    (C8) `0 < rhs_dilation`.
    (C9) `size(window_reversal) = N - 2`.
    (C10) `dim(lhs, input_batch_dimension) % batch_group_count = 0`.
    (C11) `dim(lhs, input_feature_dimension) % feature_group_count = 0`.
    (C12) `size(input_spatial_dimensions) = N - 2`.
    (C13) Given `input_dimensions = [input_batch_dimension] +
         input_spatial_dimensions + [input_feature_dimension]`:
    * `is_unique(input_dimensions)`.
    * `0 <= input_dimensions < N`.
    (C14) `dim(rhs, kernel_input_feature_dimension = dim(lhs, input_feature_dimension) / feature_group_count`.
    (C15) `dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0`.
    (C16) `dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0`.
    (C17) `size(kernel_spatial_dimensions) = N - 2`.
    (C18) Given `kernel_dimensions = kernel_spatial_dimensions +
          [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
    * `is_unique(kernel_dimensions)`.
    * `0 <= kernel_dimensions < N`.
    (C19) `size(output_spatial_dimensions) = N - 2`.
    (C20) Given `output_dimensions = [output_batch_dimension] +
          output_spatial_dimensions + [output_feature_dimension]`:
    * `is_unique(output_dimensions)`.
    * `0 <= output_dimensions < N`.
    (C21) `0 < feature_group_count`.
    (C22) `0 < batch_group_count`.
    (C23) `feature_group_count = 1 or batch_group_count = 1`.
    (C24) `size(precision_config) = 2`.
    (C25) `dim(result, result_dim)` is defined as:
    * `dim(lhs, input_batch_dimension) / batch_group_count` if `result_dim = output_batch_dimension`.
    * `dim(rhs, kernel_output_feature_dimension)` if `result_dim = output_feature_dimension`.
    * `num_windows` otherwise, where:
      * `output_spatial_dimensions[spatial_dim] = result_dim`.
      * `lhs_dim = input_spatial_dimensions[spatial_dim]`.
      * `rhs_dim = kernel_spatial_dimensions[spatial_dim]`.
      * `dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1`.
      * `padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]`.
      * `dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1`.
      * `is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]`.
      * `num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1`.
    (C26) `rank(result) = N`.
    (C27) `element_type(lhs) = element_type(rhs) = element_type(result)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `lhs` tensor. (Covered by ODS).
    I2: a) `rhs` tensor. (Covered by ODS).
    I3: a) `window_strides` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`window_strides`) != `si64`. (Covered by ODS).
    I4: a) `padding` is not a 2-dimensional tensor.
        b) element_type(`padding`) != `si64`. (Covered by ODS).
    I5: a) `lhs_dilation` is not a 1-dimensional tensor. (Covered by ODS).
        b)  element_type(`lhs_dilation`) != `si64`. (Covered by ODS).
    I6: a) `rhs_dilation` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`rhs_dilation`) != `si64`. (Covered by ODS).
    I7: a) `window_reversal` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`window_reversal`) != `i1`. (Covered by ODS).
    I8: a) element_type(`input_batch_dimension`) != `si64`. (Covered by ODS).
    I9: a) element_type(`input_feature_dimension`) != `si64`. (Covered by ODS).
    I10: a) `input_spatial_dimensions` is not a 1-dimensional tensor. (Covered by ODS).
         b) element_type(`input_spatial_dimensions`) != `si64`. (Covered by ODS).
    I11: a) element_type(`kernel_input_feature_dimension`) != `si64`. (Covered by ODS).
    I12: a) element_type(`kernel_output_feature_dimension`) != `si64`. (Covered by ODS).
    I13: a) `kernel_spatial_dimensions` is not a 1-dimensional tensor. (Covered by ODS).
         b) element_type(`kernel_spatial_dimensions`) != `si64`. (Covered by ODS).
    I14: a) element_type(`output_batch_dimension`) != `si64`. (Covered by ODS).
    I15: a) element_type(`output_feature_dimension`) != `si64`. (Covered by ODS).
    I16: a) `output_spatial_dimensions` is not a 1-dimensional tensor. (Covered by ODS).
         b) element_type(`output_spatial_dimensions`) != `si64`. (Covered by ODS).
    I17: a) element_type(`feature_group_count`) != `si64`. (Covered by ODS).
    I18: a) element_type(`batch_group_count`) != `si64`. (Covered by ODS).
    I19: a) `precision_config` does not have variadic number of enums of `DEFAULT`, `HIGH`, and `HIGHEST`. (Covered by ODS).
    C1: a) N = rank(`lhs`) != rank(`rhs`).
    C2: a) size(`window_strides`) != N - 2.
    C3: a) `window_strides[i]` <= 0 for any i in [0, size(`window_strides`)).
    C4: a) dim(`padding`, 0) != N - 2.
        b) dim(`padding`, 1) != 2.
    C5: a) size(`lhs_dilation`) != N - 2.
    C6: a) `lhs_dilation[i]` <= 0 for any i in [0, size(`lhs_dilation`)).
    C7: a) size(`rhs_dilation`) != N - 2.
    C8: a) `rhs_dilation[i]` <= 0 for any i in [0, size(`rhs_dilation`)).
    C9: a) size(`window_reversal`) != N - 2.
    C10: a) `dim(lhs, input_batch_dimension) % batch_group_count != 0`.
    C11: a) `dim(lhs, input_feature_dimension) % feature_group_count != 0`.
    C12: a) size(`input_spatial_dimensions`) != N - 2.
    C13: a) Given `input_dimensions = [input_batch_dimension] +
         input_spatial_dimensions + [input_feature_dimension]`:
         * Any dimensions in `input_dimensions` are not unique.
         b) Given `input_dimensions = [input_batch_dimension] +
         input_spatial_dimensions + [input_feature_dimension]`:
         * For any i in `input_dimensions`, i < 0.
         c) Given `input_dimensions = [input_batch_dimension] +
         input_spatial_dimensions + [input_feature_dimension]`:
         * For any i in `input_dimensions`, i >= N.
    C14: a) `dim(rhs, kernel_input_feature_dimension != dim(lhs, input_feature_dimension) / feature_group_count`.
    C15: a) `dim(rhs, kernel_output_feature_dimension) % batch_group_count != 0`.
    C16: a) `dim(rhs, kernel_output_feature_dimension) % feature_group_count != 0`.
    C17: a) size(`kernel_spatial_dimensions`) != N - 2.
    C18: a) Given `kernel_dimensions = kernel_spatial_dimensions +
         [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
         * Any dimensions in `kernel_dimensions` are not unique.
         b) Given `kernel_dimensions = kernel_spatial_dimensions +
         [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
         * For any i in$ `kernel_dimensions`, i < 0.
         c) Given `kernel_dimensions = kernel_spatial_dimensions +
         [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
         * For any i in `kernel_dimensions`, i >= N.
    C19: a) size(`output_spatial_dimensions`) != N - 2.
    C20: a) Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * Any dimensions in `output_dimensions` are not unique.
         b) Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * For any i in `output_dimensions`, i < 0.
         c) Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * For any i in `output_dimensions`, i >= N.
    C21: a) `feature_group_count <= 0`.
    C22: a) `batch_group_count <= 0`.
    C23: a) `feature_group_count` != 1 and `batch_group_count` != 1.
    C24: a) size(`precision_config`) != 2.
    C25: a) For result_dim in [0, N):
            `dim(result, result_dim)` != `dim(lhs, input_batch_dimension) / batch_group_count`, if `result_dim = output_batch_dimension`.
         b) For result_dim in [0, N):
            `dim(result, result_dim)` != `dim(rhs, kernel_output_feature_dimension)`, if `result_dim = output_feature_dimension`.
         c) For result_dim in [0, N):
            `dim(result, result_dim)` != `num_windows` otherwise, where:
           * `output_spatial_dimensions[spatial_dim] = result_dim`.
           * `lhs_dim = input_spatial_dimensions[spatial_dim]`.
           * `rhs_dim = kernel_spatial_dimensions[spatial_dim]`.
           * `dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) == 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1`.
           * `padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]`.
           * `dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) == 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1`.
           * `num_windows = (padded_input_shape[lhs_dim] == 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]) ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1`.
    C26: a) rank(result) != N.
    C27: a) element_type(`lhs`) != element_type(`rhs`).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I4a: `padding` is not a 2-dimensional tensor.
    C1a: rank(`lhs`) != rank(`rhs`) != N.
    C2a: size(`window_strides`) != N - 2.
    C3a: `window_strides[i]` <= 0 for any i in [0, size(`window_strides`)).
    C4a: dim(`padding`, 0) != N - 2.
    C4b: dim(`padding`, 1) != 2.
    C5a: size(`lhs_dilation`) != N - 2.
    C6a: `lhs_dilation[i]` <= 0 for any i in [0, size(`lhs_dilation`)).
    C7a: size(`rhs_dilation`) != N - 2.
    C8a: `rhs_dilation[i]` <= 0 for any i in [0, size(`rhs_dilation`)).
    C9a: size(`window_reversal`) != N - 2.
    C10a: `dim(lhs, input_batch_dimension) % batch_group_count != 0`.
    C11a: `dim(lhs, input_feature_dimension) % feature_group_count != 0`.
    C12a: size(`input_spatial_dimensions`) != N - 2.
    C13a: Given `input_dimensions = [input_batch_dimension] +
          input_spatial_dimensions + [input_feature_dimension]`:
          * Any dimensions in `input_dimensions` are not unique.
    C13b: Given `input_dimensions = [input_batch_dimension] +
          input_spatial_dimensions + [input_feature_dimension]`:
          * For any i in `input_dimensions`, i < 0.
    C13c: Given `input_dimensions = [input_batch_dimension] +
          input_spatial_dimensions + [input_feature_dimension]`:
          * For any i in `input_dimensions`, i >= N.
    C14a: `dim(rhs, kernel_input_feature_dimension != dim(lhs, input_feature_dimension) / feature_group_count`.
    C15a: `dim(rhs, kernel_output_feature_dimension) % batch_group_count != 0`.
    C16a: `dim(rhs, kernel_output_feature_dimension) % feature_group_count != 0`.
    C17a: size(`kernel_spatial_dimensions`) != N - 2.
    C18a: Given `kernel_dimensions = kernel_spatial_dimensions +
          [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
          * Any dimensions in `kernel_dimensions` are not unique.
    C18b: Given `kernel_dimensions = kernel_spatial_dimensions +
          [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
          * For any i in$ `kernel_dimensions`, i < 0.
    C18c: Given `kernel_dimensions = kernel_spatial_dimensions +
          [kernel_input_feature_dimension] + [kernel_output_feature_dimension]`:
          * For any i in `kernel_dimensions`, i >= N.
    C19a: size(`output_spatial_dimensions`) != N - 2.
    C20a: Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * Any dimensions in `output_dimensions` are not unique.
         b) Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * For any i in `output_dimensions`, i < 0.
         c) Given `output_dimensions = [output_batch_dimension] +
         output_spatial_dimensions + [output_feature_dimension]`:
         * For any i in `output_dimensions`, i >= N.
    C21a: `feature_group_count <= 0`.
    C22a: `batch_group_count <= 0`.
    C23a: `feature_group_count` != 1 and `batch_group_count` != 1.
    C24a: size(`precision_config`) != 2.
    C25a: For result_dim in [0, N):
          `dim(result, result_dim)` != `dim(lhs, input_batch_dimension) / batch_group_count`, if `result_dim = output_batch_dimension`.
    C25b: For result_dim in [0, N):
          `dim(result, result_dim)` != `dim(rhs, kernel_output_feature_dimension)`, if `result_dim = output_feature_dimension`.
    C25c: For result_dim in [0, N):
          `dim(result, result_dim)` != `num_windows` otherwise, where:
            * `output_spatial_dimensions[spatial_dim] = result_dim`.
            * `lhs_dim = input_spatial_dimensions[spatial_dim]`.
            * `rhs_dim = kernel_spatial_dimensions[spatial_dim]`.
            * `dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) == 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1`.
            * `padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]`.
            * `dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) == 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1`.
            * `num_windows = (padded_input_shape[lhs_dim] == 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]) ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1`.
    C26a: rank(result) != N.
    C27a: element_type(`lhs`) != element_type(`rhs`).
    ```
    
    Notes:
    * (new C24) is left untouched as there are still pending action item
    regarding the number of precision config values allowed in #879.
    
    closes #2092

[33mcommit ace3f39ddf3abe755db44802c52ce58ada752be8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Apr 23 18:33:56 2024

    Stablehlo to TF saved-model (#2157)
    
    **Draft PR**
    
    Provides an API to convert stablehlo to tf saved model. Added a test to
    convert a simple linear model.
    
    Things to me done:
    
    - [x] Provide documentation of the API
    - [x] Test should used the API from the preinstalled stablehlo python
    package
    - [x] Make sure the API is provoded as part of stablehlo python wheel.

[33mcommit b13e8dafba0080da746b2540c1cfd75ab2ec12ed[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Apr 23 00:43:21 2024

    Integrate LLVM at llvm/llvm-project@2a47ee070145 (#2247)

[33mcommit fcd560c2893bed26c49a3fbb964c9bca1412787c[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Apr 22 18:46:06 2024

    Remove "eval" prefix from ops (#2246)
    
    This is part 2 of tackling #1049, making the function names shorter.

[33mcommit ac8ed4cf6f79b698f2cfc4c2e1d2ef6a0c78cff3[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Mon Apr 22 17:05:51 2024

    Rename variables to simplify refactoring (#2245)
    
    This is part of ongoing efforts to sync reference interpreter closer to
    the spec #1049

[33mcommit 1455e52d2d3014ef1fa79ab7dff7f1cd390e7a0c[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Mon Apr 22 14:59:57 2024

    Add rewrite pattern into AggresiveSimplification that turns compare+select into min/max (#2244)
    
    This patch rewrites a comparison and selection combination that utilizes
    the same operands into a min/max operation:
    
    ```
    func.func @test(%arg0: tensor<2xi32>, %arg1: tensor<2xi32>) -> (tensor<2xi32>, tensor<2xi32>) {
      %0 = stablehlo.compare GE, %arg0, %arg1, SIGNED : (tensor<2xi32>, tensor<2xi32>) -> tensor<2xi1>
      %1 = stablehlo.compare LE, %arg0, %arg1, SIGNED : (tensor<2xi32>, tensor<2xi32>) -> tensor<2xi1>
      %s0 = stablehlo.select %0, %arg0, %arg1 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>
      %s1 = stablehlo.select %1, %arg0, %arg1 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>
      return %s0, %s1 : tensor<2xi32>, tensor<2xi32>
    }
    ```
    transformed into:
    ```
    func.func @test(%arg0: tensor<2xi32>, %arg1: tensor<2xi32>) -> (tensor<2xi32>, tensor<2xi32>) {
      %0 = stablehlo.maximum %arg0, %arg1 : tensor<2xi32>
      %1 = stablehlo.minimum %arg0, %arg1 : tensor<2xi32>
      return %0, %1 : tensor<2xi32>, tensor<2xi32>
    }
    ```

[33mcommit f394f9be631b54dff0344298f19723ee2582fbc8[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Apr 19 22:03:35 2024

    Add compile_commands flag to README (#2240)
    
    We don't provide documentation on how to generate compile_commands.json
    and this could be a useful start. I left this flag out of CI since the
    CI doesn't need to generate this.

[33mcommit 7127fd337b9279979f24b6519ce66e0455dc8526[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 19 18:07:03 2024

    Implement ConditionallySpeculatable for SelectAndScatter (#2229)
    
    The spec says (C11): shape(operand) = shape(result).

[33mcommit d6ce331ffc29c529294990fd8a0a139eaf7b6fcb[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 19 17:59:27 2024

    Bump patch version after integrate 0.19.8 -> 0.19.9 (#2243)

[33mcommit 188da03bce57ff633dabc7084399362aa59fc31e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Apr 19 16:22:16 2024

    Title fix in PyTorch colab (#2241)

[33mcommit 473d77d79e3b71b66bf61c18b421b098fe4f7cc7[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Apr 18 21:56:21 2024

    Enable testdata for convolution ops (#2224)
    
    This is part 2 of #1964 to implement parts of #1314.
    
    Also fix a bug on generating invalid padding shape (used to generate
    `N-2` number of dim size 2 instead of shape `[N-2, 2]`).

[33mcommit c8d96c098cb929ebbb19eaf1bfe42c78d5956f19[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 18 21:18:38 2024

    Tutorial: PyTorch Export to StableHLO (#2237)
    
    View rich text GH rendering here:
    [pytorch-export.ipynb](https://github.com/openxla/stablehlo/blob/60adde95c9337ce031c91f3f3fb7e5b405f4cef0/docs/tutorials/pytorch-export.ipynb).
    
    Includes "Open in Colab" URL fix for JAX tutorial.

[33mcommit 5217297204acb9e5a21e40fa825aa0769fb3c33f[m[33m ([m[1;33mtag: [m[1;33mv0.19.8[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 18 20:44:45 2024

    Integrate LLVM at llvm/llvm-project@676d3bafc09d (#2236)

[33mcommit 9654ee61ec69ca37af8161ce80a387e94991f3ca[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 18 20:36:45 2024

    Tutorial: JAX Export to StableHLO (#2232)
    
    See tutorial rendering using "View File"
    ([example](https://github.com/openxla/stablehlo/blob/f85d9fffc3d4e0fe4f09bfadd4739d09baaf60c1/docs/tutorials/jax-export.ipynb)).
    
    Once merged the "Open in Colab" button should work. Also once integrated
    this should appear on the openxla.org documentation as well.

[33mcommit f212180cd5ccdf512467bf05e935ee8b2d10eaac[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 18 20:26:50 2024

    Implement ConditionallySpeculatable for SetDimensionSize (#2230)
    
    This op isn't specced yet but the speculation logic is relatively
    straightforward and follows from the tablegen/type inference/XLA
    documentation:
    https://openxla.org/xla/operation_semantics#setdimensionsize.

[33mcommit 376376361b25f9e67474d3e7b88cd3858ddcc863[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 18 20:14:27 2024

    Use arrays for DynamicConv example (#2235)
    
    We changed these attributes to use DenseArray instead of DenseElements
    some time ago.

[33mcommit b9118430c760e1283d2e48791e80065b7dcc0db5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 18 18:19:22 2024

    Implement ConditionallySpeculatable for DynamicReshape (#2231)

[33mcommit 1beb2c3ad9a0650477367dabd081d232b771352e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Apr 18 13:46:39 2024

    Relax constraint `num_bits(storage_type) < num_bits(expressed_type)` (#2211)
    
    fixes https://github.com/openxla/stablehlo/issues/2205
    
    Requesting the following for review:
    @loganchien
    @paulinesho
    @doyeonkim0

[33mcommit 541db997e449dcfee8536043dfdd49bb13f9ed1a[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Apr 18 04:06:12 2024

    Add a note: Type Inference for Quantization is not always feasible (#2207)

[33mcommit 19e3142bc0a3e95b9e8201756ab9ebd6466bd407[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 17 19:02:57 2024

    Implement ConditionallySpeculatable for ConvolutionOp (#2228)
    
    Relevant constraints from the spec:
    
    ```
    (C10) dim(lhs, input_batch_dimension) % batch_group_count = 0.
    (C11) dim(lhs, input_feature_dimension) % feature_group_count = 0.
    
    (C14) dim(rhs, kernel_input_feature_dimension) = dim(lhs, input_feature_dimension) / feature_group_count.
    (C15) dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0.
    (C16) dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0.
    
    (C25) dim(result, result_dim) is defined as:
    * dim(lhs, input_batch_dimension) / batch_group_count if result_dim = output_batch_dimension.
    * dim(rhs, kernel_output_feature_dimension) if result_dim = output_feature_dimension.
    * num_windows otherwise, where:
    * output_spatial_dimensions[spatial_dim] = result_dim.
    * lhs_dim = input_spatial_dimensions[spatial_dim].
    * rhs_dim = kernel_spatial_dimensions[spatial_dim].
    * dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1.
    * padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1].
    * dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1.
    * is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim].
    * num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1.
    ```
    
    Because of (C14), input_feature_dimension and
    kernel_input_feature_dimension must be static. input_batch_dimension
    must be static if batch_group_count > 1 (C10) or if
    output_batch_dimension is static (C25, first bullet).
    kernel_output_feature_dimension must be static if batch_group_count > 1
    (C15) or feature_group_count > 1 (C16) or if output_feature_dimension is
    static (C25, second bullet).
    
    Because of (C25), each spatial dimension in the output can depend on the
    spatial dimensions in the inputs (input + kernel), so if it is static in
    the output, it must be static in the inputs, otherwise mismatches could
    occur at runtime.

[33mcommit 5b75941d0558cb2355be43dbb3a184bb244c029b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 17 17:11:20 2024

    Implement ConditionallySpeculatable for DotGeneral (#2227)
    
    Relevant constraints from the spec:
    
    ```
    (C9) dim(lhs, lhs_batching_dimensions...) = dim(rhs, rhs_batching_dimensions...).
    (C10) dim(lhs, lhs_contracting_dimensions...) = dim(rhs, rhs_contracting_dimensions...).
    (C12) shape(result) = dim(lhs, lhs_batching_dimensions) + dim(lhs, lhs_result_dimensions) + dim(rhs, rhs_result_dimensions).
    ```

[33mcommit 9fb78c1850ed65fe8d13f0975565ad8c37c00f0d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 17 00:44:21 2024

    Implement ConditionallySpeculatable for Fft (#2225)
    
    The logic is the same as for unary ops, except that if the type of FFT
    is RFFT or IRFFT then the last `fft_length.size()` dimensions in the
    operand need to be static (for speculatability), because there are extra
    constraints on them that could turn out to be false at runtime if they
    are dynamic.
    
    Indeed, there is a general constraint that `shape(result) =
    shape(operand)`. For RFFT, the operand type element type is float, so
    the last `fft_length.size()` dims in the operand have to be static,
    because `fft_length` is static (it is an attribute). For IRFFT, the
    result element type is float, so the last `fft_length.size()` dims in
    the result are inferred from the `fft_length`, and they need to match
    the operand, so those dims have to be static in the operand. There are
    also constraints on the last dimension, but `size(fft_length) >= 1` so
    that is already covered by the previous check.
    
    The relevant constraints from the spec are:
    ```
    (C4) If among operand and result, there is a tensor real of a floating-point type, then shape(real)[-size(fft_length):] = fft_length.
    (C5) shape(result) = shape(operand) except for:
    * If fft_type = RFFT, dim(result, -1) = dim(operand, -1) = 0 ? 0 : dim(operand, -1) / 2 + 1.
    * If fft_type = IRFFT, dim(operand, -1) = dim(result, -1) = 0 ? 0 : dim(result, -1) / 2 + 1.
    ```

[33mcommit 651551371cefa8260c385137cb9c0bfa10fc9a78[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 23:17:35 2024

    Bump patch version after integrate 0.19.7 -> 0.19.8 (#2226)

[33mcommit 8adf5b8d150ea45a743279a6c8088766e96c93e1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 16 18:11:23 2024

    Migrate StableHLO to use properties (#2221)
    
    Migrate StableHLO to use properties with a few additional changes:
    
    - Constant op's custom fallback parsing needs to be updated to handle
    properties _or_ attributes.
    - There are PjRT who require handling version skew, and are yet to
    migrate to VHLO (in progress, est ~Jun next release), in the meantime we
    require inherent IR attribute downgrades (DenseArray->DenseElements in
    [openxla/xla/pjrt/mlir_to_hlo.cc](
    https://github.com/openxla/xla/blob/aefa3ab3a0613b538e14b449817dce986a765e84/xla/pjrt/mlir_to_hlo.cc#L180)).
    This isn't possible in a dialect that is using properties since
    properties require that property types are never invalid, so this PR
    introduces a DenseArray backed by generic Attribute storage. Cleanup
    tasks for this are tracked by #2216
    
    Closes #1584

[33mcommit 126eaf5a6d68e59ada35d9a7ee670c2a44cd371d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 18:02:59 2024

    Implement ConditionallySpeculatable for Pad and Slice (#2209)

[33mcommit fd0c20a10736b78b7f1911fe1c8512c89ef60d1b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 17:57:43 2024

    Use explicit template type to avoid Windows build errors (#2222)
    
    Co-authored-by: Kevin Gleason <gleasonk@google.com>

[33mcommit e87d2f4a3d79b0f47ef4cc7cacf071b72f69ce78[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 17:18:09 2024

    Remove extra quotes in getMaxVersion template (#2208)
    
    We haven't needed to use this yet, but without this change it would
    create invalid code like `return mlir::vhlo::Version("0, 10, 0");`.

[33mcommit e81411ef562e11337283ef24bb3c40b2f3a6ebfa[m[33m ([m[1;33mtag: [m[1;33mv0.19.7[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 16:54:34 2024

    Move constant parse/print so they can be shared with MHLO (#2220)
    
    Co-authored-by: Tom Natan <tomnatan@google.com>

[33mcommit dfc8fe547f54fa5f05076154847194c127b1e341[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 16:44:01 2024

    Replace Pure on Transpose with ConditionallySpeculatable and NoMemoryEffect (#2219)
    
    This was missed in https://github.com/openxla/stablehlo/pull/2199

[33mcommit c979359b73134849fcc97901b78117f4f6b2597a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 15:49:29 2024

    Integrate LLVM at llvm/llvm-project@20ed5b1f4587 (#2215)

[33mcommit 71ddfe8e0851dc282363a3454aae70ea02b2170a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 16 00:13:48 2024

    Simplify getResult().getType() to just getType() (#2218)

[33mcommit db730207efe47d832844e8382d666dee5ecf00cf[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 15 23:40:02 2024

    Remove unnecessary {RankedTensor,Shaped}Type casts (#2217)
    
    I revisited all the casts to ShapedType and RankedTensorType in light of
    the fact that most op input/output types are ranked and removed
    unnecessary casts.
    
    This PR is sort of an extension of
    https://github.com/openxla/stablehlo/pull/2183
    
    Also simplified a few `op.getResult().getType()` to `op.getType()`
    because I was modifying that code anyway.
    
    Fixes https://github.com/openxla/stablehlo/issues/2065

[33mcommit d68db56f7d2198fbc33baa7ac65b2912dc8e94f5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 15 22:17:11 2024

    Use cast instead of dyn_cast where possible (#2183)
    
    For type inference, if the infer/verify logic were a method on the op
    itself, we could remove the cast altogether, but with the current setup
    we can at least change dyn_cast into cast to remove any ambiguity and
    clean up redundant checks.
    
    In some cases it's not possible to remove the dyn_casts, for various
    reasons:
    - CHLO ops still allow RankedTensorType
    - Some dyn_casts are used on things that may not be tensors
    
    https://github.com/openxla/stablehlo/issues/2065

[33mcommit 82fff851ec410c2cf3383c773a3f5159af8ba332[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 15 21:42:16 2024

    Document that dimension sizes can be dynamic (#2213)
    
    This satisfies P4 of the Dynamism RFC:
    https://github.com/openxla/stablehlo/blob/main/rfcs/20230704-dynamism-101.md#p4

[33mcommit 16afab76189c98344ece60f4c14b49d09d879eee[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 15 21:38:38 2024

    Remove shape operand change from Dynamism RFC (#2116)
    
    On the whole, it seems like this change has low returns and high costs.
    
    See
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/HJRvFBum65k/m/Xzld0wJaAAAJ
    for discussion.

[33mcommit 411ffc0bf8f2b2e3df8df655010123f3b6cab121[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Apr 15 20:46:16 2024

    Bump patch version after integrate 0.19.6 -> 0.19.7 (#2214)

[33mcommit 1caca1dcdc7962f72b30a567dd0f725845bf86d4[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 15 17:37:30 2024

    Explain how constraints interact with dynamism (#2210)
    
    This is for (P3) of the dynamism RFC:
    https://github.com/openxla/stablehlo/blob/main/rfcs/20230704-dynamism-101.md#p3
    
    The RFC says to update the "Notation" section of the spec, but I think
    it makes more sense to have a separate section for dynamism, which I
    plan to expand in future PRs. For now, I am adding some explanations
    about constraint verification and undefined behavior.
    
    (P3) also requires auditing existing verifiers and shape functions to
    make sure that we allow dynamic dimension sizes everywhere. I am doing
    this as part of writing tests for (P2).

[33mcommit a120d3f05a578c3297a9b95ea5d8c49892eb4675[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Apr 15 16:52:40 2024

    Add prettier printing for constant operations (#2212)
    
    To make it easier to debug differences is non-constant operations, even
    if constants are different
    
    ---------
    
    Co-authored-by: Billy Moses <wmoses@google.com>

[33mcommit 1f249ac96859590f7b27bc2aacf935336a79d3ba[m
Author: Doyeon Kim <46413178+doyeonkim0@users.noreply.github.com>
Date:   Fri Apr 12 19:28:25 2024

    Update spec and verifier for convolution op according to the RFC for hybrid quantized op (#2171)
    
    This PR implements the specification and constraints for convolution as
    discussed in the [RFC](https://github.com/openxla/stablehlo/pull/1792).
    Please let me know what you think.
    Thanks!
    
    cc @sdasgup3

[33mcommit c304904fd892b4c8991cf7d6f85285c828620f47[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 17:13:41 2024

    Implement ConditionallySpeculatable for Scatter (#2195)
    
    I am mostly interested in shape mismatches so I left a TODO
    to check if the indices are unique or sorted. In the meantime,
    we will do the conservative thing, which is to assume that the
    op is not speculatable (the indices may not be unique/sorted).
    
    NOTE: This PR builds on top of
    https://github.com/openxla/stablehlo/pull/2193. See the 2nd commit for
    changes specific to this PR.

[33mcommit 837536cba5304d4185bea83f707ce4d1a3b5cc79[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:44:15 2024

    Implement ConditionallySpeculatable for Sort (#2200)
    
    NOTE: This PR builds on top of
    https://github.com/openxla/stablehlo/pull/2193. See the 2nd commit for
    changes specific to this PR.

[33mcommit 2fe11091dbd6bafd4bf756c8ccee31ee6810b0e1[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:43:47 2024

    Implement ConditionallySpeculatable for Gather (#2198)

[33mcommit 678bb66db435f2fb6a4ff3ee886d7d1f7948a5e2[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:22:11 2024

    Implement ConditionallySpeculatable for RngBitGenerator (#2201)

[33mcommit 5cc829295b8b34f31908a95e6f32093338d01493[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:21:50 2024

    Implement ConditionallySpeculatable for BitcastConvert (#2196)

[33mcommit e02eebf8862a9a2b1215d2ebdb68f6b5a7731bdf[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:21:11 2024

    Implement ConditionallySpeculatable for Map, Reduce and ReduceWindow (#2193)
    
    All of these ops have `same(shape(inputs...))` as a constraint,
    but they also have regions, so if all their inputs are static, the
    op is recursively speculatable.
    
    (Note that `scatter` also has this constraint, but it can have UB
    in other circumstances so it will be handled in a follow-up change.)

[33mcommit 2d7c94368cb3edb6a211392237c759b3c3ddb10d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:20:50 2024

    Implement ConditionallySpeculatable for Transpose (#2199)

[33mcommit e4d3c1e9f36f3aae704bbf25c206d62f4540ab82[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 12 16:20:07 2024

    Implement ConditionallySpeculatable for Concatenate (#2191)

[33mcommit cf5a9c88ccc4e5186be7b0f6c5bb04720a8846cf[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 11 22:33:39 2024

    Revert properties change temporarily (#2206)
    
    Hit some unexpected blockers when trying to export properties changes to
    openxla/xla. I'm going to temporarily revert this change (keeping all
    the properties related infra changes, just reverting this portion and
    CHECK tests).

[33mcommit c2c2428b3f610011121845d229b25b9f86e49eed[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Apr 11 22:06:20 2024

    ReverseOp : remove HLO_CompatibleOperandsAndResultType trait (#2204)
    
    for `reverse`, `HLO_CompatibleOperandsAndResultType` trait is no longer
    required. `SameOperandsAndResultElementType` is covering (C3)
    type(result) = type(operand) constraints. Previously
    `HLO_CompatibleOperandsAndResultType` trait was defining type and shape
    inference functions. This PR removes
    `HLO_CompatibleOperandsAndResultType` for the OP and copied-out required
    type inference function.
    
    `reverse` op is a `StableHLO_ShapedInterfaceOp` so shape inference
    already handled.
    
    Tested:
    existing shape and type inference tests passed
    
    fixes https://github.com/openxla/stablehlo/issues/2145

[33mcommit 341e063f0924fc1350538dc53a92c21ec5e022a3[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Apr 11 17:18:27 2024

    collective_broadcast, collective_permute : remove HLO_CompatibleOperandsAndResultType trait  (#2179)
    
    for `collective_broadcast` and `collective_permute` ops,
    `HLO_CompatibleOperandsAndResultType` trait is no longer required.
    `SameOperandsAndResultElementType` is covering `(C3) type(result) =
    type(operand)` constraints. Previously
    `HLO_CompatibleOperandsAndResultType` trait was defining type and shape
    inference functions. This PR removes
    `HLO_CompatibleOperandsAndResultType` for the OP and copied-out required
    inference functions.
    
    Tested:
    1. return type inference is ok. Added test to infer_stablehlo.mlir to
    validate.
    2. no builder errors
    
    ref: https://github.com/openxla/stablehlo/issues/2145

[33mcommit 714d9acacd96760f4d8a9fe3898bee4f204cd419[m[33m ([m[1;33mtag: [m[1;33mv0.19.6[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 10 19:15:29 2024

    Fix typo: missing backtick for inline code (#2194)

[33mcommit 3c3de28a51c0f33ed8c4bcfb592524701b678614[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 10 01:03:37 2024

    Implement speculatability for Clamp, Compare, Complex and Select (#2187)

[33mcommit 49dc86c0df9ac3a8a556208674204b0f68d8eb6d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 9 22:07:30 2024

    Integrate LLVM at llvm/llvm-project@1e6ce5e284f5 (#2189)

[33mcommit f0f377a2c65751bab839e67504dd1172eb88b2fa[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 9 22:06:43 2024

    Migrate CHLO to use properties (#2188)
    
    Migrate CHLO to use properties.
    
    1. Use adaptors when possible to avoid needing to think about attrs vs
    properties.
    2. Avoid using `get("string")` as in
    `attributes.get("broadcast_dimensions")` to get property values.
    
    Closes #1584

[33mcommit 65f4c3e7e25e71cf6531f2c520f02cd35d25d59f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 9 22:05:39 2024

    Fix deprecated uses of template dyn_cast/isa (#2184)
    
    This was missed in https://github.com/openxla/stablehlo/pull/2180

[33mcommit 1c5bad7e959a21466405861acfa0c369faf002d8[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Tue Apr 9 21:41:49 2024

    typo fix : Correct the float type in tanh test name (#2190)

[33mcommit ad2a3b354744b066a8635acf3fcb3ef1b976ae7e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 9 21:34:49 2024

    Implement ConditionallySpeculatable for collective ops (#2168)
    
    I'm actually not sure if speculation make sense for these ops. I don't
    see any indication in the spec that they could have UB except in the
    cases added here, so it's likely fine. Also, I'm not sure if these ops
    can have memory effects or not. They definitely involve interacting with
    some kind of global state, so they have side effects. The ops weren't
    marked "Pure" before this change, so there is no difference on that
    front being introduced with this change.
    
    Also I slightly updated the logic in TestUtils to delete the op when the
    speculation check succeeds. Indeed before we were relying on DCE, but
    these ops don't state that they don't have side effects so DCE won't
    remove them.

[33mcommit 26519076b79d1976b56d3018bc2f3bed4e0dd216[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 9 20:17:10 2024

    Migrate StableHLO to use properties (#2185)
    
    Part of #1584. Last step is to migrate CHLO.
    
    Regarding use of adapters in `reify` functions, see the following code
    comment which talks about why operands (and only operands) must be
    passed as arguments:
    
    ```
      ::mlir::LogicalResult inferReturnTypeComponents(::mlir::MLIRContext* context, ::std::optional<::mlir::Location> location, ::mlir::ValueShapeRange operands, ::mlir::DictionaryAttr attributes, ::mlir::RegionRange regions, ::llvm::SmallVectorImpl<::mlir::ShapedTypeComponents>& inferredReturnShapes);
      /// Reify the shape computation for the operation.
      ///
      ///       Insert operations using the given OpBuilder that computes the
      ///       result shape. This interface is supposed to be workable during dialect
      ///       conversion (e.g. convert from tensor world to buffer world),
      ///       where `getOperand` may be invalid. For example, some ops (e.g.
      ///       dynamic_reshape(input, target_shape)) may depend on their operands
      ///       to calculate the result shape. When the `matchAndRewrite ` method
      ///       of a conversion pattern is called, the operands of the op to convert
      ///       may have been converted into other types, which makes it invalid to
      ///       call the `getOperand` method of such op directly inside the
      ///       conversion pattern.  To solve this problem, this interface follows
      ///       the design of the conversion pattern, that is, accepting passed in
      ///       operands to avoid calling `getOperand` directly inside the interface
      ///       implementation.
      ::mlir::LogicalResult reifyReturnTypeShapes(::mlir::OpBuilder& builder, ::mlir::ValueRange operands, ::llvm::SmallVectorImpl<::mlir::Value> & reifiedReturnShapes);
    
    ```

[33mcommit 3050b4f11d09edf0c315e6e51d23c608aae6c675[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Tue Apr 9 19:09:58 2024

    Add UnusedResultReduceOpCanon pattern to AggressiveSimplification (#2165)
    
    This patch tries to simplify `stablehlo.reduce` operation with results
    that don't have any uses:
    
    ```
    func.func @test(%arg0: tensor<8xi64>, %arg1: tensor<8xi64>) -> tensor<i64> {
      %0 = stablehlo.constant dense<1> : tensor<i64>
      %1 = stablehlo.constant dense<2> : tensor<i64>
      %2:2 = stablehlo.reduce(%arg0 init: %0), (%arg1 init: %1) across dimensions = [0] :
      (tensor<8xi64>, tensor<8xi64>, tensor<i64>, tensor<i64>) ->
      (tensor<i64>, tensor<i64>)
       reducer(%arg2: tensor<i64>, %arg3: tensor<i64>)
              (%arg4: tensor<i64>, %arg5: tensor<i64>)
      {
        %3 = stablehlo.add %arg2, %arg3 : tensor<i64>
        %4 = stablehlo.subtract %arg4, %arg5 : tensor<i64>
        stablehlo.return %3, %4 : tensor<i64>, tensor<i64>
      }
      return %2#0 : tensor<i64>
    }
    ```
    is simplified into the following once unused result `%2#1` is removed:
    ```
    func.func @test(%arg0: tensor<8xi64>, %arg1: tensor<8xi64>) -> tensor<i64> {
      %0 = stablehlo.constant dense<1> : tensor<i64>
      %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [0] :
      (tensor<8xi64>, tensor<i64>) -> tensor<i64>
      return %1 : tensor<i64>
    }
    ```
    
    Current implementation is conservative i.e. it performs optimization
    **iff** number of live results of `stablehlo.reduce` matches number of
    operand pairs that are used to compute those results. This limitation
    ignores the following case:
    
    ```
      (a, a1) (b, b1) (c, c1)
        ‚ñ¥       ‚ñ¥       ‚ñ¥
        ‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
        ‚îÇ       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
        ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ       ‚îÇ       ‚îÇ
        r0      r1      r2
                U
    ```
    to compute
    - r0 we use (a, a1)
    - r1 we use (b, b1) and (c, c1)
    - r2 we use (a, a1), (b, b1) and (c, c1)
    
    number of live results |{ r1 }| = 1
    number of operand pairs to compute live results |{(b,b1), (c, c1)}| = 2

[33mcommit 474caa8370dccc753e4693a2ff4221864b227597[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 9 19:07:19 2024

    Use DenseElements to serialize/deserialize dense arrays (#2181)
    
    This avoids asan issues, makes the code a lot cleaner, and avoids
    depending on implementation details of how the data is laid out.

[33mcommit f1a852ed530ca4dc80b07472ea6c2e2fa70778f5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 9 18:29:29 2024

    Fix deprecated uses of cast/dyn_cast in docs (#2182)
    
    This was missed in https://github.com/openxla/stablehlo/pull/2180

[33mcommit 0836646e442d276abf605d1bec72c4ae0cda2d38[m
Author: penguin_wwy <940375606@qq.com>
Date:   Tue Apr 9 15:45:02 2024

    Avoid calling functions in asserts (#2152)
    
    Due to a problem with the compilation options, the
    `shapeSource.reifyReturnTypeShapes` was not called, fix it.

[33mcommit 3c6a65a25ebd9dff7ccf4c16caf53c226813fefa[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Apr 9 15:21:37 2024

    Fix deprecated uses of cast/dyn_cast/dyn_cast_or_null/isa (#2180)

[33mcommit eb9bc40f40dfb26eb71aedbc581076d70232a887[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 8 23:13:20 2024

    Add missing ReshapeOp common decl (#2169)
    
    In https://github.com/openxla/stablehlo/pull/2162 I added an
    extraDeclaration for this op but forgot to concatenate the common decl.

[33mcommit 5e035185e16bef9c579f1ccbc893eab8d4ed1beb[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Apr 8 18:26:20 2024

    Mark recursively speculatable ops as such (#2175)
    
    This includes the regioned ops case, if and while.

[33mcommit e2ad824c990961e61d7a941ba9af33f6f1394508[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Apr 8 17:07:31 2024

    Bump patch version after integrate 0.19.5 -> 0.19.6 (#2172)

[33mcommit 74ce90f7842563b8bb2607d80a76913709c4367c[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Apr 8 15:32:23 2024

    Enhanced zeta readability based on the article (#2167)
    
    changes are  authored-by: @pparuzel
    
    Porting changes to GH.
    
    Updated tests to address failures:
    1. allow flexible ordering using CHECK-DAG
    2. Avoid exact matching of the dense numbers. Different precision at GH
    vs G3
    
    G3 vs GH diff at 2nd commit of the PR :
    https://github.com/openxla/stablehlo/commit/227365ac25a894ff898c6b486ae1002bcf10aeb3
    
    ---------
    
    Co-authored-by: Pawel Paruzel <paruzelp@google.com>

[33mcommit 69ef2acd02875ceff6a322633a66b0558ec5e16a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 5 17:31:16 2024

    Implement ConditionallySpeculatable for TriangularSolve (#2160)
    
    If `a` is constant, we could check the diagonal to confirm that it
    is unit, but this may be costly and should go in the op's verifier.

[33mcommit 8f1f2e2ab65b80eb9383dafad79b31e81f651a68[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 5 16:54:13 2024

    Implement ConditionallySpeculatable for BatchNorm* ops (#2164)
    
    Also, rework "BinaryElementwiseSpeculatable" trait to be more
    descriptive and more generic: the relevant condition is "are all the
    inputs statically shaped".

[33mcommit c01ff49749f55e7584f55a8bcddd00851c874f9b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 5 16:15:44 2024

    Implement ConditionallySpeculatable for a few unary ops (#2166)
    
    Also, rework "UnaryElementwiseSpeculatable" to be more descriptive and
    more generic: the relevant condition is "every static dimension in the
    output is static in the input".

[33mcommit 127465d6ab9009ae9710401b01ec8003617e099f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 5 16:14:59 2024

    Implement ConditionallySpeculatable for Reshape (#2162)

[33mcommit 9b68ca8ba32bbc6a8f4f141bb6b47c7730bdccbb[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Apr 5 16:14:30 2024

    Remove Pure trait from Cholesky (#2159)
    
    The spec says:
    https://github.com/openxla/stablehlo/blob/a053baba039d6d9f9cf11abd8c9555cc055144de/docs/spec.md?plain=1#L1584-L1585.
    
    We could try to see if the inputs are constant and then try to determine
    if they are Hermitian positive-definite, and only then report that the
    op is speculatable, but that seems like a heavy check for a situation
    that isn't likely to occur frequently, so IMO it is more reasonable to
    say that this op is not speculatable.
    
    Pure is "NoMemoryEffect + AlwaysSpeculatable". If an op doesn't
    implement ConditionallySpeculatable, then it is never Speculatable, so
    we replace Pure with NoMemoryEffect, because this op doesn't have memory
    effects.

[33mcommit 92b22c26e0cc59c740b46a149973592890903215[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 4 20:31:14 2024

    Mention trace in spec (#2163)
    
    The op is slated for removal, but unlike other ops in a similar position
    it currently isn't mentioned in the spec.

[33mcommit 46b5186401c7b039bde7bd2b77aed1e4a3f8b48f[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Apr 4 19:50:25 2024

    Add interpreter for ConvolutionOp (#1964)
    
    Since the interpreter implementation is already a mouthful, I've split
    #1314 to separate the implementation from the remaining checklist items.
    Once this is merged, I'll work on the remaining to update the rest of
    the docs.
    
    closes #970

[33mcommit e4c8d65ae817e12e5d13bbf6ffc32aa25296031a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Apr 4 19:33:27 2024

    Use hasStaticShape instead of an explicit loop (#2161)

[33mcommit 1bdf7c2603b7e68d97c1b9be92a51826e06cb6ee[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Apr 4 18:12:06 2024

    Integrate LLVM at llvm/llvm-project@c511c90680ee (#2147)
    
    update - added missing explicit dependencies.

[33mcommit 80c7ec7360e39039233927815dcdb7acf43a3e38[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Apr 4 18:06:22 2024

    Quantization Verifiers : Add/update custom verifiers for quantized constraints (#2081)
    
    This PR add/update verifiers for ops to verify quantization specific
    constraints
    
    OPs: `bitcast_convert` `broadcast_in_dim` `transpose` `reshape` `sort`
    `select` `abs`
    
    ~`dot_general`~ dropping dot_general verifier changes due to expected
    internal test failures similar to
    https://github.com/openxla/stablehlo/pull/2122

[33mcommit a053baba039d6d9f9cf11abd8c9555cc055144de[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 3 18:32:00 2024

    Add missing dependency to chlo_ops target (#2156)
    
    I don't understand why this isn't failing in CI, but I can't build this
    target since 2a0595bd3, and a few others have reported the same for
    their local builds.
    
    In any case, the code `#include`s SideEffectInterfaces.h, so its target
    should depend on the corresponding BUILD target.
    
    Filed https://github.com/openxla/stablehlo/issues/2155 for follow-up.

[33mcommit f2a9b38056092302680108f31fa135e85e1374b4[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 3 18:12:44 2024

    Implement ConditionallySpeculatable for all Binary*Elementwise ops (#2151)
    
    Note that e.g. in C/C++ a few ops similar to our ops have undefined
    behavior
    under some circumstances:
    - Integer division/remainder by 0
    - Integer division of INT_MIN by -1
    - Signed integer overflow (as can occur through e.g. addition)
    
    Our spec does not specify any undefined behavior in such cases, so I did
    not take them
    into account while making this change. IMO, such issues may be worth
    some discussion, and they can be handled later, since the current change
    focuses on shape mismatches. Also, this change does not introduce any
    unsoundness that wasn't there before, since these ops were marked
    "Pure" until now.
    
    https://github.com/openxla/stablehlo/issues/1991

[33mcommit 633119601a3c438c2459a664d68286d877f83805[m
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Wed Apr 3 10:09:16 2024

    Add StableHLO dialect version for bytecode (#2040)
    
    This enables (at least) invalidating previously generated StableHLO
    bytecode serialization (not using VHLO) by bumping the version. The
    version is optionally set and if not set, not serialized - this would
    result in no changes to what is serialized by default today unless
    explicitly set.
    
    This would enable a better error message than the existing where a
    loaded bytecode would fail and knowledge required as to why:
    
    Before
    
    ```
    <unknown>:0: error: 'stablehlo.broadcast_in_dim' op attribute 'broadcast_dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.
    <unknown>:0: note: see current operation: %4 = "stablehlo.broadcast_in_dim"(%1) {broadcast_dimensions = array<i64>} : (tensor<f32>) -> tensor<500xf32>
    <unknown>:0: note: in bytecode version 6 produced by: MLIR19.0.0git
    ```
    
    Approximately after:
    
    ```
    reading newer dialect version than supported
    ```
    
    (Note this could also be refined, but at least point to version skew vs
    folks being concerned about generation error)
    
    This does not represent a policy change nor an automatic test added.

[33mcommit 8a553766f7d58365a25c224710d0118b5cffaec6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 3 04:54:58 2024

    Implement ConditionallySpeculatable for all UnaryElementwise ops (#2150)
    
    A somewhat unfortunate thing is that we allow programs like:
    
    ```
    func.func @foo(%arg: tensor<?xi64>) {
      %0 = stablehlo.abs %arg : (tensor<?xi64>) -> tensor<2xi64>
      return
    }
    ```
    
    Such a program is legal, but doesn't make much sense: in what scenario
    would we know the output shape, but not the input shape? If it weren't
    for such programs, the ops would all be `Pure`, but because at runtime
    dynamic dimensions in the input type could differ from static dimensions
    in the output type, these ops have to be `ConditionallySpeculatable`.
    
    Also note that the spec asserts that floating point exceptions are
    defined: https://openxla.org/stablehlo/spec#floating-point_exceptions
    
    https://github.com/openxla/stablehlo/issues/1991

[33mcommit 0b510c1444bb57094084bc13b824930f4f9b2dd9[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Apr 3 04:30:47 2024

    Implement ConditionallySpeculatable for DynamicIota (#2146)
    
    https://github.com/openxla/stablehlo/issues/1991

[33mcommit 88805239e59b1ae26e3f4303f3bf7f208234dc9d[m
Author: Yinying Li <yinyingli@google.com>
Date:   Tue Apr 2 22:01:47 2024

    Support sparsity through StableHLO => Linalg lowering (#2149)
    
    After StableHLO => Linalg lowering was implemented in the [linalg
    RFC](https://github.com/openxla/stablehlo/pull/1610), most of the ops
    stated by @aartbik in the [sparsity
    RFC](https://github.com/openxla/stablehlo/pull/1143) can generate
    correct linalg code with sparse encodings. And it can then be lowered
    through mlir-opt with --sparsifier pipeline and run with mlir-runner.
    
    The `sparse.mlir` file in this PR checks the generated linalg code with
    sparse encodings. These are the same tests for the dense ops but with
    sparse input/output.
    
    The only op that couldn't be lowered to correct linalg with sparsity is
    `stablehlo.concatenate`. Therefore, after discussing with @GleasonK, I
    added a flag `enable-sparse-ops` to the linalg pass and rewrite
    `stablehlo.concatenate` to `sparse_tensor.concatenate` when the flag is
    set to true and input/output have sparse encodings. If there're more
    linalg converters that need modification, a new pass for sparsity will
    be created to run before linalg pass.

[33mcommit 73eb47969cfb72eee2bc41256c1c70840a6d21dc[m
Author: Doyeon Kim <46413178+doyeonkim0@users.noreply.github.com>
Date:   Tue Apr 2 01:53:40 2024

    [RFC] Add hybrid quantized convolution and dot_general for weight-only quantization (#1792)
    
    This RFC proposes to add hybrid quantized convolution and dot_general
    for weight-only quantization.
    Please let me know your feedback on this.
    
    The RFC partially address the issue
    https://github.com/openxla/stablehlo/issues/1575 w.r.t supporting weight
    only quantization support in StableHLO. The remaining tasks for the
    above the are highlighted
    [here](https://github.com/openxla/stablehlo/issues/1575#issuecomment-1819957802).

[33mcommit 97d0de6343d5a32ddc630c492c10c10c1f30775c[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Mar 29 20:16:01 2024

    Add SameOperandsAndResultElementType Trait for type(operand) = type (result) ops  (#2112)
    
    ~~New trait for StableHLO ops. Trait tries to match `scale` and
    `zero_point` of per-tensor quantized `operands` and `result`.~~
    
    ~~Initially I added checks at custom verifiers for few ops but there are
    ops without verifier. Adding a Trait is less code/custom verifier
    changes.~~
    
    This PR adds SameOperandsAndResultElementType Trait for applicable ops
    with `type(operand...) = type(result..) ` constraint.
    Total Three OPs : `collective_broadcast` , `collective_permute`,
    `reverse`
    Testing: positive tests already exist for these ops. Negative tests:
    since this is a native mlir Trait and not custom verifier, skipped
    negative testing.
    
    There is a overlap of verifications done by
    SameOperandsAndResultElementType and HLO_comp.. For now we will keep
    both the Traits. I will create a follow up tracker to see if we can
    remove HLO_com.. . Tracker : #2145

[33mcommit 3b953c65f83eabdf81c45263f3e87dd002701a7f[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Mar 29 20:03:45 2024

    AddOp: Remove HLO_CompatibleOperandsAndResultType Trait, add custom verifier (#2127)
    
    AddOP allows mix of `per-tensor` and `per-axis` inputs. Existing
    StableHLO AddOp def uses `HLO_CompatibleOperandsAndResultType` Trait
    which does not allow mix of `per-tensor` and `per-axis` inputs.
    This PR:
    removed HLO_CompatibleOperandsAndResultType Trait for AddOP
    Added custom Verifier to implement OP constraints
    
    
    
    Thank you @sdasgup3 for the help with debugging builder issues after
    removal of `HLO_CompatibleOperandsAndResultType` suggestion for
    `inferReturnTypes` placement.

[33mcommit 416b87d33d11ac131035cc1447e61eddb9e7c166[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Mar 29 20:00:37 2024

    Add spec warning about shape mismatches (#2136)
    
    This implements part 1 of (P2) from the Dynamism 101 RFC:
    https://github.com/openxla/stablehlo/blob/main/rfcs/20230704-dynamism-101.md#-p2-ratify-the-existing-convention-for-shape-mismatches-constituting-undefined-behavior
    
    In a future change, I will implement `ConditionallySpeculatable` where
    relevant. Many ops have the `Pure` trait but they are not actually
    `Pure` in all cases because shape mismatches are undefined behavior.
    
    #1991

[33mcommit 6982985b2cd9da53757cb43fcbaddc52b3b45c19[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Mar 29 18:30:37 2024

    Remove redundant static result shape check on reshape (#2144)
    
    The check is already enforced by the [tablegen
    declaration](https://github.com/openxla/stablehlo/blob/f392f5fcae12ef445e18eef921720b3313336c38/stablehlo/dialect/StablehloOps.td#L2514)

[33mcommit f392f5fcae12ef445e18eef921720b3313336c38[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Mar 29 17:56:33 2024

    Use long form volume flag for docker run (#2143)
    
    I haven't used docker in a while, so I forgot what -v meant. In general,
    it is a good idea to use long form flag names in shell scripts for
    readability.

[33mcommit aa31b754c4df2c08e22c370a8af1c514ee1468e4[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Mar 29 17:40:03 2024

    Remove `RankedTensorType` verification check for `CholeskyOp` (#2141)
    
    closes #2138

[33mcommit 63b390736736bf4b6979eb01a5fe5ecaa0e11352[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Mar 29 17:39:47 2024

    Remove `RankedTensorType` verification check for `TriangularSolveOp` (#2140)
    
    closes #2139

[33mcommit 41728bacada291112880bd8bb806ea65d6497aa7[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Mar 29 16:50:30 2024

    reshape op spec example : fix a typo (#2142)

[33mcommit d7205db5f9b517ab5e715fef15837952786c433f[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Mar 29 00:59:44 2024

    Bump patch version after integrate 0.19.4 -> 0.19.5 (#2137)

[33mcommit 271e8634de184fbfafd677d3876170feb6d08c97[m[33m ([m[1;33mtag: [m[1;33mv0.19.4[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Mar 28 20:24:39 2024

    Integrate LLVM at llvm/llvm-project@3cf169ca160e (#2134)

[33mcommit d339f2b2add1a0f21af24efdcc986c81e1cd100a[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Mar 28 17:08:48 2024

    Delete rank_specialization_cluster ops (#2135)
    
    (P1) of the Dynamism RFC.
    
    These ops are no longer used by KernelGen and can safely be deleted.

[33mcommit d96ce9be0757239b287b67a0be54bd20ce3ef94a[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Mar 27 23:04:05 2024

    Bump patch version after integrate 0.19.3 -> 0.19.4 (#2133)

[33mcommit 757d53f3de4e5db68a61aab95bde22899d3d954a[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Mar 27 20:03:18 2024

    Update `BUILD.bazel` target name (#2131)
    
    All of our build target names are snake case. The only exceptions are
    `stablehlo-opt`, `stablehlo-lsp-server`, and `stablehlo-translate` which
    were named such to follow MLIR style for passes.

[33mcommit 00f2ab3e8901f6ed50441848bae359cb569e3ada[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 27 18:56:57 2024

    Check whitespace in all files except bytecode and image/vector files (#2130)
    
    The root cause of #2129 is that Python files (ending in `.py`) were
    excluded from the whitespace check. I originally proposed expanding the
    list of included files to include Python files, but @ghpvnist pointed
    out that there are more files that need this check than files that don't
    and suggested using an exclude list, so I did that instead.
    
    Also, use `-E` and rewrite the pattern to allow the exclude list to be
    expanded more easily in the future.
    
    Fixes #2129

[33mcommit f4459e76553770ecc94f23de29984c7859ad9f05[m[33m ([m[1;33mtag: [m[1;33mv0.19.3[m[33m)[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Wed Mar 27 17:46:37 2024

    Fix typo in error message (#2128)

[33mcommit e280569eb44a687a2ee1be7b728f8ed7fdb03771[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Mar 26 21:26:25 2024

    Execution semantics of Stablehlo quantized operations (#2050)
    
    The PR provides clarification on the execution semantics of the
    StableHLO quantized operations.
    
    @loganchien
    @paulinesho
    @doyeonkim0
    @dominicsymes
    @cbasile-g
    
    
    Looking forward to your feedback.

[33mcommit 94c30ae65e70b4f0ae2322e35ee930762f46526d[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Mar 26 16:01:23 2024

    Add terminating newline (#2126)

[33mcommit 2a0595bd31bbdf1a191fc46f249e1734900ab1da[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Tue Mar 26 00:55:59 2024

    Integrate LLVM at llvm/llvm-project@7ac7d418ac2b (#2124)

[33mcommit 13767db249be869fc732881c750f1cc89052aa93[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 25 22:26:10 2024

    Bump patch version after integrate 0.19.2 -> 0.19.3 (#2123)

[33mcommit d89e06f1ee77660aee8b65efbeb7b3d54f73738f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Mar 25 21:56:32 2024

    Clean up hasRank checks (#2119)
    
    These checks are no longer necessary in most cases (they are still
    necessary in some cases, e.g. when dealing with custom call args/results
    or CHLO op args/results).
    
    I plan to make further cleanups in future PRs, e.g. in
    ShapeRefinement.cpp (in this PR I did only the obvious ones). Also I
    plan to clean up casts, especially casting to `ShapedType` instead of
    `RankedTensorType`.
    
    #1991

[33mcommit c123a485f6c455ad95ba5edf9266b303b282b26e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 25 18:13:41 2024

    Augment example test with interpreter usage (#2098)
    
    Augmented the example test with interpreter usage demo.
    
    nit: Fixed the dependency of `MLIRParser` on `StablehloReferenceApi` as
    API.cc uses `parseSourceFile`.

[33mcommit 881154ec84f4d696c75b179998acaadad985caf0[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Mon Mar 25 16:52:37 2024

    Revert "Updates to ConvolutionOP verifier to support quantization constraints" (#2122)
    
    Reverts openxla/stablehlo#2079
    This change is as per the spec but caused internal test failures. The
    tests are using weight-only quantization. Decided to update the verifier
    after weight-only quantization is supported (ref: #1792). Otherwise PR
    (valid) changes will cause a disruption.

[33mcommit 7c9558a87a030ed29fa3991a5559cdd2ff15f385[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 25 16:27:01 2024

    Revert "ConvolutionOP verifier : allow rhs per_axis quantized and result per_tensor quantized" (#2115)
    
    Reverts openxla/stablehlo#2094. Adding verification for this change will
    be done as a part of hybrid quantization PRs.

[33mcommit 1860c4c4c899747d28fb3e6bf24fc135853a2cf7[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Fri Mar 22 22:08:01 2024

    Add specialization of ComputeReshapeShapeOp (#2102)
    
    This enables specialization of stablehlo.compute_reshape_shape operator
    that operates on constant data:
    
    ```
    func.func @test() -> (tensor<4xi32>, tensor<1xi32>) {
      %0 = arith.constant dense<[2, -1, 2, 64]> : tensor<4xi32>
      %1 = arith.constant dense<[-1]> : tensor<1xi32>
      %2 = arith.constant 32768 : index
      %3 = stablehlo.compute_reshape_shape %2, %0 : (index, tensor<4xi32>) -> tensor<4xi32>
      %4 = stablehlo.compute_reshape_shape %2, %1 : (index, tensor<1xi32>) -> tensor<1xi32>
      func.return %3, %4 : tensor<4xi32>, tensor<1xi32>
    }
    ```
    is transformed into:
    
    ```
    func.func @test() -> (tensor<4xi32>, tensor<1xi32>) {
      %0 = stablehlo.constant dense<[2, 128, 2, 64]> : tensor<4xi32>
      %1 = stablehlo.constant dense<32768> : tensor<1xi32>
      return %0, %1 : tensor<4xi32>, tensor<1xi32>
    }
    ```

[33mcommit 7c8815a247cba7a69af472fa5fef83d046a82a20[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Mar 22 21:26:47 2024

    Disallow unranked complex tensors (#2120)
    
    This was missed in #2045.

[33mcommit 2c82fb4633e5f0c7ce21763a38ebf31989b134f2[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Fri Mar 22 17:53:18 2024

    Add support for Python example (CMake) (#2113)
    
    This is the canonical Python example similar to what was recently
    introduced for C++ in #2083
    
    * Add a stablehlo_add.py file which demonstrates nice usage of the
    Python bindings API
    * Integrate it with the CMake build system
    * Introduce a CMakePresets for buildings and Python bindings
    
    We have never had support for building Python bindings with Bazel so
    that support is missing as of this PR as well.
    
    ```console
    ‚ùØ cmake --build --preset debug-python --target stablehlo-python-add
    [89/89] cd /usr/local/google/home/fmzakari/code/github.com/open...ages/stablehlo /tmp/stablehlo-venv/bin/python3 stablehlo_add.py
    module {
      func.func @main() -> tensor<3x4xi64> {
        %0 = stablehlo.constant dense<0> : tensor<3x4xi64>
        %1 = stablehlo.constant dense<0> : tensor<3x4xi64>
        %2 = stablehlo.add %0, %1 : tensor<3x4xi64>
        return %2 : tensor<3x4xi64>
      }
    }
    ```

[33mcommit 4f180d3c2236a15f82f29aad1b47f6ea2c14fc52[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Mar 21 22:13:43 2024

    Fix formatting on spec examples (#2117)
    
    As pointed out in
    https://github.com/openxla/stablehlo/pull/2097#discussion_r1534620186,
    MLIR doesn't insert spaces between the brackets and the content. I've
    also made minor formatting improvements.

[33mcommit b27ef13c377983d04c233adb8e1de093ec7f350a[m[33m ([m[1;33mtag: [m[1;33mv0.19.2[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Mar 20 22:32:16 2024

    Fix indentation (#2110)

[33mcommit e7117ace402a2042643eefce91b70900e2258552[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 20 20:18:52 2024

    Exit early from suffix loop when skipped suffix is found (#2114)

[33mcommit c3f456500f0f2e96fdb4a98fde4fbe48b4d624b8[m
Author: Pawel Paruzel <paruzelp@google.com>
Date:   Wed Mar 20 16:48:31 2024

    Fix bug in Horner's rule (#2111)
    
    This is related to openxla/xla#5838 (fixed by cl/615457374). The bug has
    been producing inaccurate results for Hurwitz Zeta.
    
    Co-authored-by: Adam-Banas <adambanas@google.com>

[33mcommit 969b9ad925113d740195d72bb01cc41fe11fcfdd[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Tue Mar 19 21:03:38 2024

    Add support of tensor types with unit dimension prefix in EvalSlice (#2090)
    
    Edit: Sure, sorry for missing description.
    
    Currently our small research team experimenting with BERT model
    represented in number of *-hlo dialects and wants to simplify it in
    terms of variety of operators. This PR fixes an issue we stumbled upon:
    
    ```
    %42 = stablehlo.constant dense<"..."> : tensor<1x512xi64>
    ...
    %66 = stablehlo.slice %42 [0:1, 0:128] : (tensor<1x512xi64>) -> tensor<1x128xi64>
    ```
    Previous implementation of EvalSlice can't handle such case -- tensor
    type prefixed with unit dimension(s) i.e. 1x128.
    
    This PR adds support of the above case and can slice from any position,
    e.g.
    
    ```
      %0 = stablehlo.constant dense<[[[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]]> : tensor<1x3x2x2xi64>
      %1 = "stablehlo.slice"(%0) {
        start_indices = array<i64: 0, 1, 1, 0>,
        limit_indices = array<i64: 1, 2, 2, 2>,
        strides = array<i64: 1, 1, 1, 1>
      } : (tensor<1x3x2x2xi64>) -> tensor<1x1x1x2xi64>
    ```
    is folded to
    ```
    %1 = stablehlo.constant dense<[[[[7, 8]]]]> : tensor<1x1x1x2xi64>
    ```

[33mcommit 3442dbe0cd65ced69f0d1d4a6c274cb154873764[m
Author: srcarroll <50210727+srcarroll@users.noreply.github.com>
Date:   Tue Mar 19 20:52:17 2024

    Fix failed reify for ReverseOp (#2109)
    
    The `ReverseOp` did not implement a `reifyReturnTypeShapes`. This
    resulted in a failed assertion in the `getEmptyTensorFor` helper
    function that gets used during the `stablehlo-legalize-to-linalg` pass
    when the op has dynamic input/output. This patch implements this
    function as well as adds a test for the newly supported case (input can
    have dynamic dims as long as they don't correspond to the `dims`
    attribute of the op)
    
    Various other quality of life changes, such as having match failure
    notifications have been added.

[33mcommit ec4ec7847908b06ed6170c4aba96c164a22f7396[m
Author: Michael <pantlimonmv@gmail.com>
Date:   Mon Mar 18 19:52:28 2024

    Skip conversion of shape.shapeof with 0-ranked tensor operand (#2107)
    
    Currently `--shape-legalize-to-stablehlo` fails on the following code:
    
    ```
    func.func @test1() -> tensor<0xindex> {
      %0 = arith.constant dense<0> : tensor<i32>
      %1 = shape.shape_of %0 : tensor<i32> -> tensor<0xindex>
      func.return %1 : tensor<0xindex>
    }
    ```
    at `stablehlo/dialect/TypeInference.cpp:1700`:
    ```
    // concatenate_c5
    auto elementType = inputTypes[0].cast<ShapedType>().getElementType();
    ```
    as `inputTypes.size()` is zero.
    
    I have checked how it works on non-0 ranked tensor type:
    ```
    func.func @test2() -> tensor<2xindex> {
      %1 = arith.constant dense<0> : tensor<2x128xi32>
      %3 = shape.shape_of %1 : tensor<2x128xi32> -> tensor<2xindex>
      func.return %3 : tensor<2xindex>
    }
    ```
    produces:
    ```
    func.func @test2() -> tensor<2xindex> {
      %cst = arith.constant dense<0> : tensor<2x128xi32>
      %0 = stablehlo.get_dimension_size %cst, dim = 0 : (tensor<2x128xi32>) -> tensor<i32>
      %1 = stablehlo.reshape %0 : (tensor<i32>) -> tensor<1xi32>
      %2 = stablehlo.get_dimension_size %cst, dim = 1 : (tensor<2x128xi32>) -> tensor<i32>
      %3 = stablehlo.reshape %2 : (tensor<i32>) -> tensor<1xi32>
      %4 = stablehlo.concatenate %1, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
      %5 = builtin.unrealized_conversion_cast %4 : tensor<2xi32> to tensor<2xindex>
        return %5 : tensor<2xindex>
    }
    ```
    
    I suggest considering an alternative approach instead of simply bailing
    out; one option could be generating a constant tensor with zero
    dimension:
    ```
    func.func @test1() -> tensor<0xindex> {
      %cst = arith.constant dense<0> : tensor<i32>
      %0 = stablehlo.constant dense<> : tensor<0xi32>
      %1 = builtin.unrealized_conversion_cast %0 : tensor<0xi32> to tensor<0xindex>
      return %1 : tensor<0xindex>
    }
    ```
    but i am not entirely certain in semantic equivalence.

[33mcommit 9c1bccf5f6d86dd5c09ac345d6dc3aa1ab592e1d[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Mon Mar 18 18:46:16 2024

    fix typo of py args (#2104)
    
    because that the lambda's signature is `[](MlirContext context,
    std::string artifact)`.

[33mcommit b8a10b861fdbcb3d52fda438b3ac9f853eef2604[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Mar 18 18:31:07 2024

    Integrate LLVM at llvm/llvm-project@a4ca07f13b56 (#2105)

[33mcommit 09bec0d669c9ce45fa7c469bce7202985190a783[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Mar 18 18:25:47 2024

    Bump patch version after integrate 0.19.1 -> 0.19.2 (#2095)

[33mcommit 538b1e11ca83fd5f14390e73a54c41af8f7ee87d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Mar 18 17:56:21 2024

    Fix asan build issue (#2108)
    
    Seems the image itself has a bug:
    https://github.com/actions/runner-images/issues/9491.
    
    Apply this patch from
    https://github.com/actions/runner-images/issues/9491\#issuecomment-1989718917
    to fix the issue while we wait for a new version of the image to roll
    out.
    
    Fixes https://github.com/openxla/stablehlo/issues/2106

[33mcommit dc4bc72de7e483adf7c3d5fac9b14bec526b10ef[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Thu Mar 14 23:32:48 2024

    ConvolutionOP verifier : allow rhs per_axis quantized and result per_tensor quantized (#2094)
    
    Previous check was restrictive and causing test failures during
    integration.

[33mcommit 888e4ae509f013e8f8eb2d73f79e80150d8918be[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Mar 14 21:06:38 2024

    Add IDE instructions for Vim (#2091)
    
    These are the main components that I use.

[33mcommit db1a8b1739ec8b37692c3d3304fd8ff10b21d161[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 14 18:55:50 2024

    Add a python API for reference interpreter (#2062)
    
    ```py
    ASM = """
    func.func @test(%arg0: tensor<2x2xf16>) -> tensor<2x2xf16> {
      %0 = stablehlo.add %arg0, %arg0 : (tensor<2x2xf16>, tensor<2x2xf16>) -> tensor<2x2xf16>
      func.return %0 : tensor<2x2xf16>
    }
    """
    
    m = ir.Module.parse(ASM)
    arg = np.asarray([1, 2, 3, 4], np.float16).reshape(2,2)
    args = [ir.DenseIntElementsAttr.get(arg)]
    res = stablehlo.eval_module(m, args)
    ```
    
    - Added python API for reference interpreter, and test cases for
    int/float datatypes
    - Added reference API which takes DenseElementsAttrs to be used by
    python API
    - Added method to convert between `stablehlo::Tensor` and
    `mlir::DenseElementsAttr` more easily (only supports int/float types)
    - Added more error checking at interpreter entry point to improve UX
    - Changed error reporting at API boundary from `llvm::Error` to
    `mlir::LogicalResult` to be more consistent for external users, also
    improves python error experience

[33mcommit d4c8682499de7b05154c8a41e290b29f38835663[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 14 18:19:42 2024

    Extensibility RFC (#1883)
    
    This PR does not contain an actual RFC - it's merely tracking a
    discussion on openxla-discuss:
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/Ao5K8fvXoEk.
    Once this RFC reaches an outcome, we'll update the status field in this
    markdown file and merge this PR to document that.

[33mcommit edc27cc4ca7a2fd1d165c3c117951e22447143b6[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Thu Mar 14 17:52:50 2024

    Fix typo in `ide.md` (#2089)

[33mcommit 1677f7f15b90b8236214df2c7ad3854836fce919[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Mar 14 17:32:54 2024

    Ensure emitting bytecode does not yield warnings (#2080)
    
    This mirrors the checks we have for VHLO in
    `stablehlo_legalize_to_vhlo.mlir`.
    
    Emitting StableHLO bytecode should not emit such warnings either.

[33mcommit d8e78cc2c5209bad29985d89381e1e8aa32a85cc[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Thu Mar 14 02:50:44 2024

    Add sensible setup for vscode (#2085)
    
    * Add CmakePresets.json file with the default Debug build. This actually
    works for Cmake itself without vscode.
    
      `cmake --preset debug`
    
    * add `ide.md` to describe how to setup vscode to get intellisense and
    to use the CMakePresets.json file
    
    End result of this should be that CMake works nicely in vscode **with**
    clangd intellsense information
    
    
    ![image](https://github.com/openxla/stablehlo/assets/605070/1d0b5123-21f0-4e39-adef-9ae194d4ded2)

[33mcommit aa69baea1409d7c341705e0e9342ed62802d8a4d[m[33m ([m[1;33mtag: [m[1;33mv0.19.1[m[33m)[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Wed Mar 13 22:40:35 2024

    Add examples directory (#2083)
    
    Add the beginnings of an `examples` directory structure that will help
    demonstrate how to use the StableHLO library.
    
    At the moment I've only added a c++ example, and a simple one at that
    but the directory structure should indicate that we can support Python
    in the future.
    
    * Added a meta library 'stablehlo_lib' that you can link against which
    should eventually include everything
    * Added the add stablehlo example
    * Added a simple README
    
    ```console
    ‚ùØ ./build/examples/c++/stablehlo-add
    module @test_module {
      func.func @main(%arg0: tensor<3x4xf32>, %arg1: tensor<3x4xf32>) -> tensor<3x4xf32> {
        %0 = stablehlo.add %arg0, %arg1 : tensor<3x4xf32>
        %1 = stablehlo.add %arg0, %arg1 : tensor<3x4xf32>
        return %1 : tensor<3x4xf32>
      }
    }
    ```

[33mcommit 1d61de4e446e912042c9e5692002919ed65692f7[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 13 21:10:25 2024

    Integrate LLVM at llvm/llvm-project@e371ada409b2 (#2087)

[33mcommit b0ef72f8df144dd6f12605dbf25ba7cd4453a4cb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 13 19:18:26 2024

    Bump patch version after integrate 0.19.0 -> 0.19.1 (#2086)

[33mcommit 04365f85cfbffe3d95ba2fb79ff34cd929d4a9a6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Mar 11 21:29:39 2024

    Fix broken links (#2082)
    
    Most of these links are only broken when rendered out of tree, e.g. on
    the OpenXLA website: https://openxla.org/stablehlo. For example, the
    "More Examples" links lead to e.g.
    https://openxla.org/stablehlo/tests/interpret/add.mlir, which is 404.
    
    For links under `docs/` it is possible to use relative paths.
    
    Also, add labels to images used in spec. This will materialize as the
    "alt" text, which is used by screen readers.

[33mcommit a8f44d0becc773e0ec01703235559827bd16f68f[m
Author: Abhinav Gunjal <agunjal@google.com>
Date:   Fri Mar 8 19:13:54 2024

    Updates to ConvolutionOP verifier to support quantization constraints (#2079)

[33mcommit d978408a5fb1155dd89ad0ed6ac4d43ef2493c9f[m[33m ([m[1;33mtag: [m[1;33mv0.19.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 7 01:31:48 2024

    Integrate LLVM at llvm/llvm-project@be15a6b3b68b (#2078)

[33mcommit 6909631c594a0ac4f26a4e9bebbaa0f30740290c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Mar 7 00:09:18 2024

    Fix stablehlo_legalize_to_vhlo.0_19_0.mlir bytecode (#2077)
    
    Also, use same header in all tests, and avoid command substitutions
    since they don't work on Windows:
    https://github.com/openxla/stablehlo/pull/1764
    
    ### Why does `0_19_0` have a different header?
    
    I followed the instructions in
    https://github.com/openxla/stablehlo/blob/main/docs/vhlo.md, except for
    the part that says `Replace RUN commands`. That is, I copied
    `stablehlo_legalize_to_vhlo.mlir`, but I forgot to update the RUN
    commands to match the other versioned tests. IMO, we should avoid
    duplicating the RUN commands across tests and documentation, better to
    just instruct developers to copy the latest versioned test and add their
    tests in there.
    
    ### Why was the bytecode incorrect?
    
    I serialized the bytecode a while ago. During review, I ended up
    changing the order of some attributes, which caused the bytecode to be
    invalid (e.g. the `name` attribute showed up where
    `composite_attributes` should be).

[33mcommit d85ebed1d83ee12afbcfb23fdffc128c4ce9c93f[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Wed Mar 6 22:00:13 2024

    DynamicBroadcastInDimOp make arguments per-axis quantized (#2076)
    
    ref: https://github.com/openxla/stablehlo/pull/2059 for background.
    
    For now we are aligning with BroadcastInDimOp spec. It will be further
    reviewed during speccing the DynamicBroadcastInDimOp.

[33mcommit 7c4ccacffca0c55a43a9b1982086419a8831c5d3[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 6 21:29:40 2024

    Add patterns in ShapeLegalizeToStablehlo for tensor::ExtractOp (#2075)
    
    Add patterns in ShapeLegalizeToStablehlo for tensor::ExtractOp
    
    1. Add support for tensor.Extract, which is lowered to mhlo.slice
    2. Support a special case from TF graph:
       tensor.extract tensor<?xi32> -> i32
       arith.index_cast i32 -> index
       This is lower to:
       mhlo.slice tensor<?xi32> -> tensor<1xi32>
       mhlo.reshape tensor<ixi32> -> tensor<i32>
       unrealized_conversion_cast tensor<i32> -> i32
       unrealized_conversion_cast i32 -> tensor<i32>
       unrealized_conversion_cast tensor<i32> -> index
    
    Co-authored-by: Andy Wan <quanwanandy@google.com>

[33mcommit 2cdab422a749a270e2138ec8fd3e00caa289fccf[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 6 21:04:43 2024

    Implement composite inlining pass (#2073)
    
    Co-authored-by: Farid Zakaria <fmzakari@google.com>

[33mcommit c6279a60f772b989cd123b804e4b98b8f4ebb427[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 6 20:57:40 2024

    Clean up aggressive simplification and add tests (#2071)
    
    I wanted to read this pass to familiarize myself with the code. In
    passing, I made a few readability improvements here and there and while
    making these I realized that we did not import the tests for this code.
    Import the tests so we can have some confidence when making changes.
    
    The tests are from
    https://github.com/openxla/iree/blob/7782a414ea473c59f6d7a882cb510690ed666c79/compiler/plugins/input/StableHLO/stablehlo-iree/Conversion/Preprocessing/test/canonicalization.mlir,
    with minor tweaks (formatting, reordering tests to match the
    implementation, and adding missing test separators).

[33mcommit 5385f1671a4ae8f1268fd20bfb637b9568fdbbfc[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Mar 6 17:27:41 2024

    Extensibility: Implement StableHLO CompositeOp (#2024)
    
    This introduces the op definition, spec, verifier, compatibility, and
    interpreter support.
    
    The spec has few constraints. Most are covered by ODS. Others are
    covered by the verifier, and tests were added for each.

[33mcommit d214e2e2bd568a80a9f25515907b64dd7b98c538[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Mar 5 23:21:10 2024

    Add support for Bazel 6 & 7 (#2009)
    
    Previous PR #2006 added support for Bazel 7 but some of the changes in
    Bazel 7 are not backwards compatible with 6.
    
    This is a problem for some of our consumers that embed our project in
    other Bazel workspaces.
    
    Specifically, the name of the runfiles directory was changed from the
    workspace name to be '_main'.
    
    The solution adopted here is to use @python_rules and programmatically
    determine the runfiles directory relative to the lit.cfg.py file.

[33mcommit da04b39d1721f893559a70a96323a0a38ba46341[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Tue Mar 5 23:19:05 2024

    Quantization Verifiers based on T2x set of Traits (#2041)
    
    * made `isCompatibleElementTypeForHloTypeInference` stricter to return
    error for {not Quantize, Quantize}, {per-axis Quantized, per-tensor
    Quantized} cases
    * `AddOp` VHLO Test failures : addressed test failures because {not
    Quantize, Quantize} is not allowed
    * CorrectedTraits for `CholeskyOp` and `ClampOp` to match it with the
    spec
    
    ~~Note: This PR is based on in review PR
    https://github.com/openxla/stablehlo/pull/2007~~
    
    Follow up PR will add/update OP verifiers for OPs which need special
    handling

[33mcommit c5292f64eb61726e37d34fff22b316ed5bd224a2[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Mar 5 20:00:23 2024

    Update code of conduct to reflect new project stewards (#2068)

[33mcommit 3269dccdf7b3fba96172e28d0a64a9dca8197122[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Mar 5 19:50:59 2024

    Add shape refinement pass for DotOp (#2064)
    
    `DotOp` currently doesn't have shape refinement, so if downstream
    exports stablehlo with dot, and it contains unbounded dynamic shapes
    with `stablehlo.dot`, it fails to refine shape.
    
    This change enables refinement, and prevents users from having to
    transform `dot` back to `dot_general` whenever canonicalization pass
    (for `dot_general` to `dot`) is run implicitly.

[33mcommit ab9ea3ee8b587d8cb2e1ffc7a8daac130bbf2337[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Tue Mar 5 17:42:38 2024

    GetDimensionSizeOp : make arguments quantized type (#2059)
    
    ~~DynamicBroadcastInDimOp: is yet to be a part of spec.~~ (see comments)
    GetDimensionSizeOp: will be changed to quantized types in spec.
    Back porting from the integration as file check tests were failing for
    both.

[33mcommit b5a199f2b1d5cc3563d830b494f110c526677226[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 5 17:27:01 2024

    Add Elementwise trait for ReducePrecisionOp (#2063)
    
    Co-authored-by: Tom Natan <tomnatan@google.com>

[33mcommit 5e2484345b32bca04b8d3a8ed3462015029178ac[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Mar 5 17:22:55 2024

    Remove duplicated CHLO tests (#2067)
    
    The test files have exactly the same contents, except that one of them
    (kept in this change) has 3 more tests at the end.
    
    Also, add missing test separators.

[33mcommit c2fccec9f02daa2d709001ed5f37924d2526676f[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Thu Feb 29 18:23:07 2024

    Bump patch version after integrate 0.18.4 -> 0.18.5 (#2061)

[33mcommit 57036b66db301a0f2bcd62b1a36f868e516741dd[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Feb 29 18:04:19 2024

    Update bazel lockfile and check lockfile in CI (#2060)
    
    I ran `bazel build --lockfile_mode=error` and it told me to run `bazel
    mod deps --lockfile_mode=update`, which led to a pretty big diff in the
    lockfile. I also tried just doing `bazel build` and that leads to a
    smaller diff (just the changes at the top of the file), but it seems
    more prudent to do what the tool says.

[33mcommit 62bf2421d2f2483bac26394f272c882474d94436[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 29 14:49:59 2024

    Add pass to refine dynamic function parameters into static arguments (#2056)
    
    We have `StablehloRefineShapes` but using it is currently not easy.
    Function arguments must be replaced with static argument sizes, and a
    buffer must be put in place to stop the new types from creating invalid
    IR, then refine shapes can be used.
    
    This CL simplifies this entire process by adding a pass which takes care
    of the argument refinement, preparing the program for shape refinement.
    
    For example, consider the following program which broadcasts and
    computes: `add(tensor<?x10xf32>, tensor<1xf32>)`
    
    ```sh
    $ cat /tmp/t.mlir
    module @IrToHlo.21 {
      func.func @main(%arg0: tensor<1xf32>, %arg1: tensor<?x10xf32>) -> tensor<?x10xf32> {
        %0 = stablehlo.constant dense<1> : tensor<2xi32>
        %1 = stablehlo.constant dense<10> : tensor<1xi32>
        %2 = stablehlo.get_dimension_size %arg1, dim = 0 : (tensor<?x10xf32>) -> tensor<i32>
        %3 = stablehlo.reshape %2 : (tensor<i32>) -> tensor<1xi32>
        %4 = stablehlo.concatenate %3, %1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
        %5 = stablehlo.maximum %4, %0 : tensor<2xi32>
        %6 = stablehlo.dynamic_broadcast_in_dim %arg1, %5, dims = [0, 1] : (tensor<?x10xf32>, tensor<2xi32>) -> tensor<?x10xf32>
        %7 = stablehlo.dynamic_broadcast_in_dim %arg0, %5, dims = [1] : (tensor<1xf32>, tensor<2xi32>) -> tensor<?x10xf32>
        %8 = stablehlo.add %6, %7 : tensor<?x10xf32>
        return %8 : tensor<?x10xf32>
      }
    }
    ```
    
    The parameter refinement pass validates refinements, replaces the
    FuncOp's arguments with static operands, and wraps the operands in a
    custom_call `shape_refinement_operand_wrapper`, which allows the rest of
    the program to remain dynamic until shape refinement is called, which
    then removes these custom_calls:
    
    ```
    $ stablehlo-opt /tmp/t.mlir --stablehlo-refine-parameters=shapes="tensor<1xf32>,tensor<5x10xf32>"
    module @IrToHlo.21 {
      func.func @main(%arg0: tensor<1xf32>, %arg1: tensor<5x10xf32>) -> tensor<?x10xf32> {
        %0 = stablehlo.constant dense<[5, 10]> : tensor<2xi64>
        %1 = stablehlo.custom_call @stablehlo.shape_refinement_operand_wrapper(%arg1, %0) {indices_of_shape_operands = dense<1> : tensor<1xi64>} : (tensor<5x10xf32>, tensor<2xi64>) -> tensor<?x10xf32>
        [...]
        %8 = stablehlo.dynamic_broadcast_in_dim %1, %7, dims = [0, 1] : (tensor<?x10xf32>, tensor<2xi32>) -> tensor<?x10xf32>
        %9 = stablehlo.dynamic_broadcast_in_dim %arg0, %7, dims = [1] : (tensor<1xf32>, tensor<2xi32>) -> tensor<?x10xf32>
        %10 = stablehlo.add %8, %9 : tensor<?x10xf32>
        return %10 : tensor<?x10xf32>
      }
    }
    ```
    
    With this pass, and leveraging existing shape refinement / dynamism
    canonicalization / aggressive simplification, we can refine unbounded
    dynamic program into a static program which computes
    `add(tensor<5x10xf32>, tensor<1xf32>)`
    
    ```sh
    $ stablehlo-opt /tmp/t.mlir --stablehlo-refine-parameters=shapes="tensor<1xf32>,tensor<5x10xf32>" --stablehlo-refine-shapes --stablehlo-canonicalize-dynamism --stablehlo-aggressive-simplification
    module @IrToHlo.21 {
      func.func @main(%arg0: tensor<1xf32>, %arg1: tensor<5x10xf32>) -> tensor<5x10xf32> {
        %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<1xf32>) -> tensor<5x10xf32>
        %1 = stablehlo.add %arg1, %0 : tensor<5x10xf32>
        return %1 : tensor<5x10xf32>
      }
    }
    ```
    
    Much of this work is a refactoring / MLIR-ification of work
    @mlevesquedion completed, i.e. the wrapping of operand in custom_call.
    
    Co-authored-by: Michael Levesque-Dion <mlevesquedion@google.com>
    
    ---------
    
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit c37035f023ec4beb016783059749177bfb977b33[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Wed Feb 28 23:02:08 2024

    Backport minor fix for "unused variable" from the integration (#2058)

[33mcommit 1d304fc91069b13d8abcb218c952e6cbc8ca6eb2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Feb 28 15:27:38 2024

    Check quantized-storage-type to be a signless integer (#2054)
    
    Current StablehLO quantized signed int
    [cs](https://github.com/openxla/stablehlo/blob/4a26ddee5fdbe178a84f219513bfc48f565919b7/stablehlo/dialect/Base.td#L60)
    uses `mlir::quant::QuantizedType::isSIgned` to decide on the signed-ness
    of the storage type.
    
    StableHLO, as bootstrapped from MHLO, inherits `signless` integers
    (added in MHLO for some legacy reasons) to be treated as signed integer.
    This is not captured by the check `mlir::quant::QuantizedType::isSigned`
    because of the following reason.
    
    
    Based on
    [this](https://github.com/llvm/llvm-project/blob/16e74fd48988ac95551d0f64e1b36f78a82a89a2/mlir/include/mlir/Dialect/Quant/QuantTypes.h#L102),
    `mlir::quant::QuantizedType::isSIgned()` is true if the associated bit
    in
    [flag](https://github.com/llvm/llvm-project/blob/16e74fd48988ac95551d0f64e1b36f78a82a89a2/mlir/lib/Dialect/Quant/IR/TypeDetail.h#L30)
    is 1. Here are a few ways to set that flag as true.
    
    ```
     auto signed_flag_bit = storage_type.cast<mlir::IntegerType>().isSignless();
    ... mlir::quant::UniformQuantizedPerAxisType::get(
         is_signed, storage_type, expressed_type, scales, zero_points,
         quantization_dimension, storage_min, storage_max);
    
    or
    
     auto signed_flag_bit = storage_type.cast<mlir::IntegerType>().isSigned();
    ... mlir::quant::UniformQuantizedPerAxisType::get(
         is_signed, storage_type, expressed_type, scales, zero_points,
         quantization_dimension, storage_min, storage_max);
    ```
    
    In other words, It is on the producers of `mlir::quant::QuantizedType`
    to set that bit based on **their interpretation of signedness**, which
    in case of StableHLO is signedless. That means the a `true` value of
    `mlir::quant::QuantizedType::isSIgned()` is not enough to ensure that
    the desired signed-ness of storage type.
    
    ## Suggested change
    
    Replace
    
    ```
    CPred<"$_self.cast<mlir::quant::UniformQuantizedType>()" #
                     ".isSigned()">]>,
    ```
    
    with
    
    ```
               CPred<"$_self.cast<mlir::quant::UniformQuantizedType>()" #
                     ".getStorageType().cast<mlir::IntegerType>().isSignless()">]>,
    
    ```
    
    IMO, we can skip the check
    `mlir::quant::UniformQuantizedType::isSigned()` altogether as is it not
    sufficient nor necessary.
    
    Context:
    https://github.com/openxla/stablehlo/issues/1812#issuecomment-1963280512

[33mcommit 9bbf6d80fce758b49e0b81b0c6df4ebb5b8db9c0[m[33m ([m[1;33mtag: [m[1;33mv0.18.4[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Feb 28 01:36:07 2024

    Move reduce and while printing to AssemblyFormat to be shared with MHLO (#2027)
    
    [NFC] Moved code from StablehloOps.cpp to AssemblyFormat.cpp to be
    shared.
    
    Implemented in openxla/xla first to ensure that MHLO was able to share
    this code: https://github.com/openxla/xla/pull/9687

[33mcommit 879f1802a37251c96db890a8e371909e50ba95b0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 28 00:39:18 2024

    Remove suffixes for files that already have a license (#2033)

[33mcommit 6539b373b8a03696e6480b315ebf3ca9549b44c0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 28 00:38:17 2024

    Bump patch version after integrate 0.18.3 -> 0.18.4 (#2057)

[33mcommit 77a4b4cbe58563cb47341522a511c87d1376445a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 27 23:52:00 2024

    Allow CustomCallOp to have unranked tensor operands and results (#2055)
    
    Custom call op permits just about everything: Tuple, Tensor, Token,
    Unranked, all quantization, etc. This change restores its ability to
    operate on unranked tensors.
    
    This was squashed in a merge conflict between #2045 and #2007. Adding a
    testpoint to avoid this issue going forward.

[33mcommit 4a26ddee5fdbe178a84f219513bfc48f565919b7[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Tue Feb 27 18:33:40 2024

    Make HLO_FpOrQuantizedIntTensor def RankedTensorOf (#2052)
    
    and make Quantized test compatible. This was missed during resolving
    merged conflict
     for https://github.com/openxla/stablehlo/pull/2007

[33mcommit 19fd17d9e3098364689e576c5c9fcc531d6bc6ff[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Tue Feb 27 02:17:53 2024

    Introduce PerAxis Quantized constraint for StableHLO Quantized OPs (#2007)
    
    StableHLO OPs supporting Quantization can be categorized into following
    three types
    a. Only PerTensor Quantized Tensors
    b. Only PerAxis Quantized Tensors (no OP in this category for now)
    c. Both PerTensor and PerAxis Quantized Tensors
    PerAxis constraint from the PR allow only PerTensorQuantized Tensor
    inputs to type (a) OPs and don't allow PerAxis Quantized Tensors.
    Also,
    Added negative test cases to validate this behavior
    Added positive test cases for PerTensor , PerAxis Quantized Tensor
    support
    
    
    Excluded OPs
    * Deprecated :`BroadCast` and `Call`
    * Not Specced :
    ¬†`dynamic_iota`, `create_token`, `dynamic_broadcast_in_dim`,
    `cross-replica-sum`, `einsum`, `unary_einsum`, `dynamic_reshape`,
    `set_dimension_size`, `trace`, `return`, `torch_index_select`,
    `real_dynamic_slice`, `dynamic_pad`, `dynamic_gather`, `dynamic_conv`,
    `dynamic_reshape_shape`, `cstr_reshapable`

[33mcommit 902f6756da19c852349e454d6ebf87ed5b5889e1[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Feb 27 00:46:35 2024

    Fix incorrect restore-key in cache action (#2051)
    
    Fix incorrect prefix in the restore-keys for the GitHub action for bazel
    builds.

[33mcommit 70db15a3433318aa922c7a7d7075cfc4af17c3df[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Feb 27 00:20:33 2024

    Disable runfile tree creation (#2046)
    
    Investigating why Bazel builds, specifically hydrating from Cache have
    gotten so bad seems to be on account of rules_python addition.
    
    I debugged the cache step and found it caching duplication of
    python_rules: https://pastebin.com/ibgVB8Yk in the build tree.
    
    These are in fact sym-links however:
    ```
    ‚ùØ ls -l bazel-out/k8-fastbuild/bin/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir.test.runfiles/rules_python\~0.30.0\~python\~python_3_10_x86_64-unknown-linux-gnu/bin/
    lrwxrwxrwx 175 780412 26 Feb 19:58 2to3 -> /usr/local/google/home/fmzakari/.cache/bazel/_bazel_fmzakari/17bce12c4b47a4a2fc75249afee05177/external/rules_python~0.30.0~python~python_3_10_x86_64-unknown-linux-gnu/bin/2to3
    lrwxrwxrwx 180 780412 26 Feb 19:58 2to3-3.10 -> /usr/local/google/home/fmzakari/.cache/bazel/_bazel_fmzakari/17bce12c4b47a4a2fc75249afee05177/external/rules_python~0.30.0~python~python_3_10_x86_64-unknown-linux-gnu/bin/2to3-3.10
    lrwxrwxrwx 176 780412 26 Feb 19:58 idle3 -> /usr/local/google/home/fmzakari/.cache/bazel/_bazel_fmzakari/17bce12c4b47a4a2fc75249afee05177/external/rules_python~0.30.0~python~python_3_10_x86_64-unknown-linux-gnu/bin/idle3
    ```
    
    However these are a ton additional inodes that `tar` has to expand and
    store in the GitHub action.
    
    People on the Bazel slack pointed me to "-nobuild_runfile_links" which
    disables the runfile **at build creation**.
    Our tests however still run because `bazel test` has support for
    on-demand runfile generation.
    
    This should make the caching much faster.
    
    ---------
    
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit 30dac42b2ad1e2447805c628ca05f711be28aef3[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Feb 27 00:19:52 2024

    Improve caching keys for Bazel (#2047)
    
    Improve caching key based on
    [documentation](https://github.com/actions/cache/blob/a2ed59d39b352305bdd2f628719a53b2cc4f9613/examples.md?plain=1#L673)
    
    * key hashes the relevant files that can cause variance
    * removed LLVM version in the key since it's already in the WORKSPACE
    file
    * added `restore-keys` which hydrates another cache for potential re-use

[33mcommit 8f78f6a1aef87142de3d614c0a4596a5c8acac6c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Feb 26 22:38:18 2024

    Check copyright year and license type in license check (#2043)
    
    Addressing prior review feedback:
    https://github.com/openxla/stablehlo/pull/2018/files/001999860b0f236614b0e64493a447c9b0c85490#r1495173847

[33mcommit 882e162aac8805436eeb114caa00daf072b08c6f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Feb 26 22:19:40 2024

    Disallow unranked tensors (#2045)
    
    #1991

[33mcommit fc9c602ae6cacd05215061fa325f041ae95e660f[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Mon Feb 26 21:47:28 2024

    Integrate LLVM at llvm/llvm-project@e630a451b457 (#2048)

[33mcommit 32c69d06a4ca3c22c3de544f38d676769b24fe13[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 26 18:10:31 2024

    Migrate Shape legalizations to StableHLO (#2039)
    
    As described in
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/httnobiya7U
    
    Migrate Shape->MHLO to Shape->StableHLO. Shape computations are produced
    in CHLO decompositions so it makes sense to keep these together. This
    will allow for full CHLO->StableHLO legalization.

[33mcommit 01d32e888bfb6d31188c6c4e7a2485047952f04b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Feb 26 17:58:09 2024

    Use initialize and generate getDependentDialects for passes from IREE (#2030)
    
    Extending work done in #1919 and #1966.

[33mcommit a96c816c8bf5fc9aecfa88323aa1e73560e48dd4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 23 22:07:12 2024

    Specification of quantized collective_broadcast op (#2037)
    
    Please find the specification of quantized `collective_broadcast` op.
    The specification is proposed to be similar to other distribution ops
    like `collective_permute`.
    
    Let me know your feedback.
    
    @loganchien
    @paulinesho
    @doyeonkim0

[33mcommit e6b96b2733c9229800f7a468f307ee9611460361[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Feb 23 20:27:39 2024

    Fix type error in example in Dynamism RFC (#2042)

[33mcommit d40f3516698fc539545305ebd7b4a5462b2e10de[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 23 19:59:50 2024

    Simplify creation of `TensorType`  variants using `TensorType::clone` (#2036)
    
    Let's use `TensorType::clone` which is customized to create TensorType
    variants (`RankedTensorType` and `UnRankedTensorType`).

[33mcommit 4e463ed48b1564d513478342acea0e9f7b0b8349[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Feb 23 16:29:27 2024

    Use bash strict mode in license check script (#2038)
    
    When I added the script in
    https://github.com/openxla/stablehlo/pull/2018, I bootstrapped it from a
    script that didn't use strict mode. Around the same time, I sent
    https://github.com/openxla/stablehlo/pull/2012 for review, but forgot to
    apply the change to this new script as well.
    
    Also, change the condition on whether UNLICENSED_FILES contains any
    elements to avoid triggering nounset.

[33mcommit 115941b8b0fc3cd289f741d6734a0040361ae725[m[33m ([m[1;33mtag: [m[1;33mv0.18.3[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Feb 23 00:38:57 2024

    Integrate LLVM at llvm/llvm-project@e899641df239 (#2035)

[33mcommit f02a2c64a905570288d5d371e64791f080b8560b[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Feb 22 23:30:45 2024

    Print clang-format version in CI (#2032)
    
    The version of clang-format available in CI seems to be different from
    the one on my machine, but I can't easily see what the version of
    clang-format running in CI is, so I can't really debug. Print the
    version so that moving forward if someone runs afoul of the check they
    can repro and fix.

[33mcommit 2f49d24ad882a3d919b18f29618ae24af53f57c0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 22 23:10:24 2024

    Bump patch version after integrate 0.18.2 -> 0.18.3 (#2034)

[33mcommit e708c82502982697540886738a307f72f9e9a7ff[m[33m ([m[1;33mtag: [m[1;33mv0.18.2[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 22 00:43:02 2024

    Integrate LLVM at llvm/llvm-project@4c6043de0b83 (#2029)

[33mcommit 267602feabc777bf59b7e88c625c0cee662e7267[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 22 00:27:35 2024

    Align CHLO->StableHLO with existing CHLO->MHLO logic for migration (#2028)
    
    Will begin migrating CHLO->MHLO uses to CHLO->StableHLO->MHLO so that
    existing MHLO pass can be removed.
    
    This PR does the following:
    - Decouple canonicalizers from CHLO->StableHLO
    - Aligns test cases between StableHLO and MHLO to ensure smooth
    migration
    - Change to ModuleOp pass to match existing infra / other StableHLO
    infra

[33mcommit 0ef900047ed15140b1452d77367550b17a559e79[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 21 23:22:18 2024

    Ignore all files that shouldn't have licenses (#2031)
    
    Unblocks #2029
    
    I ran the license checker with `find` as input instead of `git diff` to
    find all the relevant files and exempted the files that don't already
    have licenses.

[33mcommit 0264c4d64c82ae74a54b85d274eec5084c2c0abf[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 21 17:19:55 2024

    Exclude MLIR bytecode files from license check (#2025)
    
    Also, don't add an extra line between each skip, and use the same
    message for each type of skipped file (easier to read).
    
    I tested the script locally by creating files with various suffixes and
    confirmed that the skipped suffixes are indeed skipped (and files with a
    non-skipped suffix are not skipped).

[33mcommit f52f0035a66a383dfdddb972002ecf1b5aeb0929[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Feb 21 16:57:29 2024

    Bump patch version after integrate 0.18.1 -> 0.18.2 (#2023)

[33mcommit 422a01560278e74cd81f56ef90e9574830654e41[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 21 02:22:39 2024

    Update recommended path for examples (#2021)
    
    This was missed in #1913.

[33mcommit 44840aa52f9654aae57a3ef847047e26b5b5a03d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 21 02:22:12 2024

    Default stablehlop op traits to empty list (#2022)
    
    This only concerns a few ops, but it avoids having to specify an empty
    list when an op has no traits.

[33mcommit 408aac0b60e7b79fef679bcafab55f5d20d126a6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 21 00:00:34 2024

    Remove extra % prefix for function inputs (#2020)
    
    The way the spec is currently written, a function input would be written
    e.g. `%%arg0` instead of `%arg0`.

[33mcommit 2907e119be5d5700b3a15e8551396897db30f84d[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Feb 20 18:14:49 2024

    Replace addlicense with find and grep (#2018)
    
    addlicense is not maintained and is not extensible. find and grep are
    sufficient for our use case. Note that what the new script is doing is
    basically equivalent to addlicense:
    - Find files that may need licenses:
    https://github.com/google/addlicense/blob/4caba19b7ed7818bb86bc4cd20411a246aa4a524/main.go#L197
    - Look for a license:
    https://github.com/google/addlicense/blob/4caba19b7ed7818bb86bc4cd20411a246aa4a524/main.go#L372
    
    Unfortunately, addlicense only handles files with extensions it knows
    about, and there is no way to specify additional extensions:
    https://github.com/google/addlicense/blob/4caba19b7ed7818bb86bc4cd20411a246aa4a524/main.go#L163
    
    I believe the reason for this is that addlicense can also add licenses
    to files automatically, which requires knowing e.g. what is the comment
    syntax for each given file. However, in our case we add new files rarely
    enough that doing so manually should not be too painful. If it does turn
    out to be painful, I can update this script to provide an autofix mode.
    
    See this thread for additional context:
    https://github.com/openxla/stablehlo/pull/2016#discussion_r1493040821

[33mcommit 5943c6b390adfeb1fe395159f46f16666d93573e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Sat Feb 17 00:47:26 2024

    Use bash strict mode in all CI scripts (#2012)
    
    "Bash strict mode" is a colloquial expression for a set of settings that
    makes Bash stricter (safer). See e.g.
    https://gist.github.com/robin-a-meade/58d60124b88b60816e8349d1e3938615
    
    TLDR: to avoid errors from being silently ignored, we should set errexit
    (exit script on error), nounset (exit script when an undefined variable
    is encountered) and pipefail (propagate error codes in pipelines) in all
    our scripts.
    
    Unfortunately, `errexit` is in fact not enough to exit when any error
    occurs, some errors are swallowed:
    https://dev.to/banks/stop-ignoring-errors-in-bash-3co5
    
    Because of this, I changed the whitespace script to gather the files in
    a different way (I copied the approach from the clang-format script),
    such
    that the script will exit when an error occurs, instead of continuing to
    run.
    
    The whitespace script was missing a fetch which was causing `git
    merge-base` to fail. Maybe our other scripts should also be updated to
    only run on changed files.
    
    Interestingly, GitHub already sets `-e` (`errexit`) when running
    scripts, but it's best to enforce it in the script itself:
    
    ```
    Run ./build_tools/github_actions/lint_whitespace_checks.sh
      ./build_tools/github_actions/lint_whitespace_checks.sh
      shell: /usr/bin/bash -e {0}
    ```
    
    Fixes #2010

[33mcommit feb49e3930135ad8350c46b884f8a0cd5ec3452a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 16 23:34:55 2024

    fix copyright message for a few files (#2016)
    
    Also includes framing the version string in ways expected by some
    internal integration tooling.

[33mcommit f3b20b3a0558187ee27a9428bc1d3c2a3ba459cf[m[33m ([m[1;33mtag: [m[1;33mv0.18.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 16 17:03:36 2024

    Align the definition of `checkIfOperandAndResultElementTypesMatch` with its name (#2015)
    
    As pointed by @abhigunj , having this mismatch is confusing.

[33mcommit 5d1a9c892500c2e9fecbfedfa66ffe84ff1caf7b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 16 00:46:24 2024

    Remove redundant downgrade test (#2005)
    
    This is covered by the invalid 0.17.0 test, don't need it in both. We
    tend to only have the illegal downgrade test at the boundary where it
    would occur

[33mcommit ab83efb7fb7e94b2d79d0626e4349b2bd5325166[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Feb 15 22:36:32 2024

    Integrate LLVM at llvm/llvm-project@d592c8ec8f71 (#2013)

[33mcommit 69264010bf51e4d3175ac8492764560b33407568[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Feb 15 02:01:15 2024

    Use operand type as default value for result type in CHLO unary ops (#2011)

[33mcommit 0b2acc582b8e0d5844c60bfe6abd02df19180504[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Feb 14 20:14:48 2024

    Bump patch version after integrate 0.18.0 -> 0.18.1 (#2008)

[33mcommit 4ac26f8786d491c5d8376e6e563d1b72af09de75[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Feb 13 23:10:55 2024

    Bump Bazel to 7.0 (#2006)
    
    Major change: Bump Bazel to the new LTS at 7.X
    
    * Included a default Python toolchain 3.10
    
      There is a problem at the moment running lit using Python311 where it
      cannot find the module.
    
      Our CI system already enforces Python310 to get around it but we should
      set the toolchain in Bazel itself to avoid this problem.
      I had already filed https://github.com/llvm/llvm-project/issues/75963
    
    * I moved skylib to MODULE.bazel to test the MODULE portion of Bazel is
    working. A remaining action item is to move LLVM itself to MODULE.bazel
    which would preclude needing to define skylib since its a dependency of
    LLVM.
    
    * Workspace names are no longer named in Bazel 7+
    
      I had to change stablehlo to _main to fix the runfiles directory.
      Please see https://github.com/bazelbuild/bazel/issues/18128
    
    We should consider using
    https://github.com/bazelbuild/rules_python/blob/main/python/runfiles/runfiles.py#L262
    instead to get the runfiles directory for future proofing.
    
    Fixes #1878
    
    ---------
    
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit de5a7ab2e6d2fecf7f757a5dffb4de0ee542aaa5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 13 22:08:59 2024

    Per-axis specification for `dot-general` and `add` (#1961)
    
    closes https://github.com/openxla/stablehlo/issues/1574
    The PR implement the spec changes to support per-axis quantization
    scheme for `add` and `dot_general` ops. Here is the rationale for the
    support:
    
    - For `add` op: Used for bias addition following convolution. Bias and
    [convolution
    op](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#semantics-25)
    outputs can both be per-axis quantized and have the same shape, and same
    quantization parameters, but current `stablehlo.add` only support
    per-tensor quantized scheme. [Relevant
    dicussion](https://github.com/openxla/stablehlo/issues/1574#issuecomment-1659466321)
    - For `dot_general`: Some uses cases of quantized models requires
    support for a mixed quantization scheme where it accepts per-tensor
    quantized `lhs` and per-axis quantized `rhs`. The quantization schemes
    (per-axis/per-tensor) proposed for `dot_general` op is similar to the
    convolution. [Relevant
    discussion](https://github.com/openxla/stablehlo/issues/1574#issuecomment-1676914854)
    - We do not expect the the contracting dimension
    `rhs_contracting_dimensions` to include the `quantization
    dimension(rhs)`. The multiply-accumulation step for dot_general iterate
    over the elements along the contracting dimensions. Having a uniform
    quantization parameters (scales and zero-points) for those elements
    allows full-integer computations (refer to section 2.2. of
    https://arxiv.org/pdf/1712.05877.pdf). However, if the quantization
    parameters are different within the contracting dimensions, which is
    what will happen if we allow `quantization_dimension(rhs)` to be in
    `rhs_contracting_dimensions`, then the computation must rescale integers
    (or dequantize) while doing multiply-accumulation which is not
    desirable.
    
    
    
    Looking forward to feedback!
    cc @dansuh17, @loganchien, @rewu93, @dominicsymes

[33mcommit 7509a264b4552ecaf89713ba67edbb59422fb859[m[33m ([m[1;33mtag: [m[1;33mv0.18.0[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 13 00:20:00 2024

    Fix verification of DynamicBroadcastInDimOp to handle unranked operand (#2004)
    
    Verification of `dynamic_broadcast_in_dim` does not address the event
    when the operand is of unranked type and crashes instead on nullptr
    dereference.
    
    The PR fixes the issue and adds a relevant test to highlight the
    problem.

[33mcommit cb09aa10b765d8b616da66d69252a0a9fe1c93ee[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Feb 12 18:42:33 2024

    Integrate LLVM at llvm/llvm-project@2cbe5a33a5fd (#2003)

[33mcommit 5405149c33d8eb1f0ec5dc639c98d2659ac92fa4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Feb 10 02:46:32 2024

    VHLO legalization of StableHLO UniformQuantizedPerAxisType (#1986)
    
    The PR enables
    1. VHLO legalization of StableHLO UniformQuantizedPerAxisType.
    2. Writing/reading from bytecode format.
    
    The mlir::quant dialect has two different Quantized types,
    [one](https://github.com/llvm/llvm-project/blob/e2bb91b25c8740625fecd127c1d908a2fabd0102/mlir/include/mlir/Dialect/Quant/QuantTypes.h#L255)
    for per-tensor and the
    [other](https://github.com/llvm/llvm-project/blob/e2bb91b25c8740625fecd127c1d908a2fabd0102/mlir/include/mlir/Dialect/Quant/QuantTypes.h#L315)
    for per-axis. Following the same analogy, we are now having two types in
    VHLO `UniformQuantizedV1Type` and `UniformQuantizedPerAxisV1Type` resp.
    Also added the legalization of StableHLO -> VHLO and VHLO -> StableHLO
    for the corresponding types.
    
    Regarding testing the compatibility, we only added this feature (of new
    per-axis type) to
    [stablehlo_legalize_to_vhlo.0_17_0.mlir](https://github.com/openxla/stablehlo/compare/main...sdasgup3:serialize-per-axis-quantization-type?expand=1#diff-d78cbc82314e64545ee7b9f6a66a0b910f4a8ac2e7f27e2ad55c4b33d2ea409c.
    @GleasonK Please let me know if the feature needs to be added to earlier
    version as well.

[33mcommit e191eb4c3c3f3144503a8a117d760de5ddcc7e89[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Feb 9 01:18:17 2024

    Set Python bindings to OFF by default (#2002)
    
    Users building the project by following the README may not need the
    Python bindings, so we shouldn't recommend building them by default.
    
    #2001

[33mcommit 2ed1466a6b137aad2300f5bad1287fefecb8b498[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 8 23:48:31 2024

    Rename function to match spec (#2000)
    
    With the recent change to rename a function to match spec in #1987, I
    noticed that several ops also rely on this meta function that can also
    benefit from name change.

[33mcommit a3b36a3bb4af3967432c3c6808e287ae5dd51e4c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 8 22:55:36 2024

    Convert lambda call to a function call (#1999)
    
    The `hasDuplicates` function was originally factored out because it is
    used frequently. However it was still defined as lambda, so I've updated
    it to be a function.
    
    There was also an unnecessary conversion to a vector (the underlying
    data does not change), so I've removed the conversion too.

[33mcommit c4c0465d71f20717e522a7011a6ca55ef654e9ec[m
Author: Alexander Pivovarov <apivovarov@gmail.com>
Date:   Wed Feb 7 18:10:12 2024

    Add cmake --build to Build instructions (#1998)
    
    to build the project we need to run `cmake --build .` after `cmake ..`

[33mcommit c85cefd8e917f1529b166f5d096c75678d70ead8[m
Author: lonely eagle <2020382038@qq.com>
Date:   Wed Feb 7 03:52:17 2024

    Chlo decompose patterns (#1984)
    
    Adding CHLO->StableHLO decompositions and simplification patterns, per discussion in:
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/httnobiya7U

[33mcommit 2f9ddfa4c5777c3a8f2325b53d3261ba29c6b6fd[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 7 01:43:17 2024

    Disallow unranked dynamism in rng_op (#1995)
    
    I missed this in https://github.com/openxla/stablehlo/pull/1970.
    
    https://github.com/openxla/stablehlo/issues/1991

[33mcommit 5f9f07bf221096af300b342a17d987685c5c35cf[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Feb 7 01:32:35 2024

    Use new constant function instead of deleted makeScalar function (#1996)
    
    This seems to be from https://github.com/openxla/stablehlo/pull/1987
    (see
    https://github.com/openxla/stablehlo/actions/runs/7808382192/job/21298492956).
    
    Not sure how presubmit failed to catch this. I think what happened is
    the following:
    
    1. https://github.com/openxla/stablehlo/pull/1987 was uploaded and
    passed presubmit.
    2. https://github.com/openxla/stablehlo/pull/1983 was submitted, adding
    a new use of `makeScalar`.
    3. https://github.com/openxla/stablehlo/pull/1987 was submitted,
    deleting `makeScalar` and causing tests to fail at HEAD.
    
    What I don't understand is why this sequence of operations is possible
    in the first place. I feel like when
    https://github.com/openxla/stablehlo/pull/1983 was submitted, the
    earlier passing presubmit on
    https://github.com/openxla/stablehlo/pull/1987 should have been
    invalidated, blocking submit. I guess presubmits only re-run if you add
    a commit? So in this case since there was no new commit and no conflict,
    no new commit needed to be pushed and the presubmits were not run again.
    
    In the grand scheme of things, I think this is fairly minor, but we may
    want to investigate whether we can prevent this somehow by tweaking some
    configuration settings or something.

[33mcommit 6520b58922a0eda7f590a76c60dace25279c4896[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 7 01:03:19 2024

    Rename function to closely match spec (#1987)
    
    As an example, looking at the spec of `collective_broadcast`, we come
    across the statement:
    
    `broadcast_in_dim(constant(0, element_type(result)), [], type(result))`
    otherwise.
    
    This PR renames `makeScalar()` to `constant()` with arguments matching
    the spec above for parity.

[33mcommit 034ef4b3b0b343041800ff74f80a229406bbe090[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Feb 6 22:48:17 2024

    Bump patch version after integrate 0.17.7 -> 0.17.8 (#1994)

[33mcommit 7d7977a5d8eca42194803d6f2de39987f21c641a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 6 21:18:24 2024

    Use default visibility for symbols (#1993)
    
    As pointed out by @mlevesquedion in
    https://github.com/openxla/stablehlo/pull/1983#discussion_r1480049496,
    the symbol visibility is public by default, and making it implicit
    follows the precedent of other tests in this directory.

[33mcommit 36189e2ec9837098c77ab35ba1174b485878a05f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 6 19:39:42 2024

    Add interpreter for CollectiveBroadcastOp (#1983)
    
    closes #1982

[33mcommit aa9a196a967e008bee16c5c56e3e89fa90deaa2c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Feb 6 18:00:46 2024

    Avoid duplicating extra class declarations (#1990)
    
    I noticed some duplication and wondered if it was possible to remove it.
    Then I stumbled upon this thread which provided a solution:
    https://discourse.llvm.org/t/how-to-add-new-content-to-extraclassdeclaration-in-subclasses/71537
    
    The idea is to declare the common class declaration in the base class
    and concatenate it with the child-specific declarations.
    
    In some cases, the field was being overriden only to be set to the same
    value as the parent. In such cases, I simply removed the override.

[33mcommit 8fbf9915b4cb4c92ed7b4c4cda60da573d6e6430[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Feb 6 17:38:47 2024

    Add a simple example for execution order (#1992)
    
    The text refers to "the example program above", but said example is
    actually near the top of the file (thousands of lines above). The
    example is also longer than necessary for the purposes of illustrating
    execution order.

[33mcommit be5ef8f2d0adf449a5953dca19e7145c30bc48fa[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Feb 6 17:33:29 2024

    Disallow unranked dynamism in ops that take output shape as an operand (#1970)
    
    This includes:
      - dynamic_broadcast_in_dim
      - dynamic_iota
      - dynamic_reshape
    
    #1991

[33mcommit dae2985ea54e7101ef4be781af24bda5c77f8a23[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 6 02:04:42 2024

    Update status of approved and merged RFCs (#1989)
    
    Usually we try to modify the "Status" line to show approved right before
    merging. Looks like we forgot to do that for a few RFCs. Updating
    accordingly.

[33mcommit b0edcdd6810551bf6aaec7d5aa00451cfe207582[m[33m ([m[1;33mtag: [m[1;33mv0.17.7[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Feb 5 21:41:26 2024

    Integrate LLVM at llvm/llvm-project@cca9f9b78fc6 (#1985)

[33mcommit cb1952a1fe0ef0a3d84113241c5ab4cafdde2fc9[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 5 21:41:10 2024

    Bump patch version after integrate 0.17.6 -> 0.17.7 (#1988)

[33mcommit 229b9a3f3340f0d02a36fa5900fc9849d57c4252[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 5 21:05:49 2024

    Dynamism RFC (#1881)
    
    The current design of dynamism in MHLO and StableHLO has been
    practically useful. There are success stories of it connecting JAX,
    PyTorch and TensorFlow to a number of compilers, in a mix of research
    and production environments. This RFC aims to leverage existing support
    for dynamism in the StableHLO dialect, discuss improvements to the
    existing design and then formalize the improved design in the StableHLO
    opset.
    
    The main challenge with writing this RFC was that it affects the entire
    opset. The current design involves a lot of corner cases, so it took
    about a year of practical evaluation by the author - primarily within
    JAX native serialization, but also correlating with other ongoing and
    prior projects - to distill the design into just a few general design
    principles. I think I'm happy with the outcome, but please take a look
    at the "Summary" section for what that entails.
    
    This RFC addresses a considerable chunk of community feedback on the
    existing design, but some feedback is deliberately left out of scope for
    this RFC to enable incremental progress while areas which require
    additional alignment are developed in parallel. See sections "Community
    feedback" and "Out of scope" for details.
    
    The only open question at the moment is interoperability with HLO with
    respect to bounded dynamism. The proposed representation for bounded
    dynamic types in StableHLO is in parity with the representation for
    bounded dynamic types in HLO. However, I'm not sure whether this parity
    covers 100% of bounded dynamism functionality. For example: 1) there
    appears to be a mismatch in how broadcasts are represented, 2) there is
    misalignment in representations of dynamic windows (HLO has a high-level
    representation: VALID and SAME, whereas StableHLO expects the producers
    to dynamically compute window sizes). Nonetheless, I think that this
    shouldn't block the initial review of the RFC, since there's a lot of
    stuff to discuss - and in the meanwhile, I'll be working on confirming
    interoperability with HLO.
    
    Finally, I'd like to acknowledge Smit Hinsu's work on the [Bounded
    Dynamism RFC](https://github.com/openxla/stablehlo/pull/194) from Q4
    2022, which was superseded by this work. The representation for bounded
    dynamic types in the StableHLO dialect was designed and implemented by
    Smit, and Smit's proposal to allow bounded dynamic types everywhere is
    compatible with the more general proposal from this RFC to enable
    dynamism for all size-related program elements. Furthermore, Smit
    contributed the formal spec for get_dimension_size as well as the
    informal spec for set_dimension_size.

[33mcommit d50b6fc2d9ab6fc97be8548beceba2db2e6b15d9[m[33m ([m[1;33mtag: [m[1;33mv0.17.6[m[33m, [m[1;31morigin/tensor_types[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 1 23:59:54 2024

    Refactor portable api to use main registration method (#1981)
    
    Less moving pieces. Will also get the QuantizationDialect registration
    that was added earlier.
    
    We end up registering _some_ dialects that aren't needed in
    serialization, but the cost is likely not worth the duplication.

[33mcommit 550d946af59c1af094648bc22bf5b6005e8e9c52[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Feb 1 23:40:32 2024

    Perform initialization in pass initialize method (#1966)
    
    This is unlikely to give us performance gains since most of our passes
    run on modules anyway (so the initialization probably already occurs
    only once), but it is cleaner to separate the initialization of a pass
    from the actual running of the pass.
    
    The code in `initialize` will run when the pass runs regardless of
    whether there is at least one instance of the target operation. However,
    modules are pretty much always present so this is unlikely to change
    anything.

[33mcommit 68da793945526c2b83913599ed6ac133f35fa7b5[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Feb 1 23:40:15 2024

    Fix shellcheck issues (#1980)
    
    https://github.com/openxla/stablehlo/pull/1971 introduced a `shellcheck`
    lint. However, it also introduced some bugs due to adding quotes in a
    few places where word splitting was used.
    
    Thanks to gleasonk@ for spotting this:
    https://github.com/openxla/stablehlo/pull/1976
    
    As described in https://www.shellcheck.net/wiki/SC2086, quoting is
    generally preferable when expanding a variable. However, when building a
    command line it is better to create an array and then expand the array.
    When using a regular variable, quoting can actually lead to incorrect
    behavior: for example, given `x="file1 file2"`, `clang-format $x` will
    correctly call `clang-format` on `file1` and `file2`, however
    `clang-format "$x"` will call `clang-format` on a non-existent file
    called `file1 file2`.
    
    The lint itself was actually broken because of using `-printf '%P\0'`
    with `find`. This produces null-delimited output, which `mapfile` does
    not expect and does not parse correctly (only the first file name will
    be extracted). (While implementing the check I thought I needed to use
    `printf` to remove the the `./` prefix introduced by `find`, but turns
    out that's not the case).
    
    While investigating this change I discovered that the whitespace lint
    check is also subtly broken. Indeed in `get_source_files` `xargs` is
    used at the end of the pipeline, which puts everything on one line,
    which causes subsequent uses of `xargs -L1` to behave incorrectly (the
    underlying command is executed only once with all files as arguments).

[33mcommit 10dc10d3347f6cae4d7287696c82e9170c9654a5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 1 23:25:08 2024

    Enhance rendezvous docs and fix race condition (#1956)
    
    The PR removes the use of `shared_ptr`s as it complicates the logic of
    `rendezvous`. Instead of relying on the use count of `shared_ptr`, I've
    added a variable to track the state separately instead. Now, there is no
    need to worry about the scope of the `shared_ptr`s and cleaning up
    resource before exiting.
    
    Thanks @mlevesquedion for sharing code snippets offline to help simplify
    this!

[33mcommit aeeca3c4f827131d6e2fd53c732618e54d515308[m
Author: cryptodeal <39316528+cryptodeal@users.noreply.github.com>
Date:   Thu Feb 1 22:32:34 2024

    Register `QuantizationDialect` (#1978)
    
    Bug fix to register `mlir::quant::QuantizationDialect`.

[33mcommit 39c4a00dcc694d2497ffc8fc8e90426aac2ee191[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 1 21:39:08 2024

    Add explicit error reporting to transform passes (#1976)
    
    Currently pass failures are silent (only written to `llvm::dbgs`) in
    these passes Adding explicit error reporting will at least make it clear
    what pass has failed.
    
    Revert change to clang format script since it breaks when there are
    multiple files, see:
    https://github.com/openxla/stablehlo/actions/runs/7743835139
    
    ---------
    
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit f5eadaa5995b708458d5a8510cdfc125975fc15c[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Thu Feb 1 19:40:34 2024

    Remove generator expression (#1979)
    
    Based on feedback from
    https://gitlab.kitware.com/cmake/cmake/-/issues/25649 removed the
    generator expression in favor of if statement.
    
    Example output:
    ```
    -- Building StableHLO with an installed MLIR
    -- Using MLIRConfig.cmake in: /usr/local/google/home/fmzakari/code/github.com/openxla/stablehlo/build/../llvm-build/lib/cmake/mlir
    -- Using LLVMConfig.cmake in: /usr/local/google/home/fmzakari/code/github.com/openxla/stablehlo/llvm-build/lib/cmake/llvm
    -- Enabling LLD as the linker
    -- Enabling split-dwarf build
    -- Enabling GDB index in binary
    -- Building with -fPIC
    -- Configuring done (0.2s)
    -- Generating done (0.1s)
    -- Build files have been written to: /usr/local/google/home/fmzakari/code/github.com/openxla/stablehlo/build
    ```

[33mcommit 040ca282c1a5fd732eeb1bf3ea82955cc651e3d3[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 1 16:58:54 2024

    Integrate LLVM at llvm/llvm-project@ca7fd2549238 (#1977)

[33mcommit 013ef950ac6f862e57e3cddb46feb1429b43e608[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Thu Feb 1 00:09:02 2024

    Add ccache by default when building MLIR (#1974)
    
    * Add ccache use for build_mlir.sh script
    * Remove LLVM_CCACHE_BUILD which is not used and [deprecated
    ](https://discourse.llvm.org/t/llvm-ccache-build-is-deprecated/68431 )
    (Although our use of it is not necessarily the LLVM's version)
    
    Note: this change was pulled out of
    https://github.com/openxla/stablehlo/pull/1973

[33mcommit 03c09d9262e53e2359b1f74e42cab20bc93b47a8[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Thu Feb 1 00:07:43 2024

    Add option to separate debug information (DWARF) from executable (#1973)
    
    When building Debug or RelWithDebInfo, the size of the binaries (i.e.
    `stablehlo-opt`) can be quite beefy.
    I noticed it can be up to **2GiB** in size and attaching GDB is quite
    slow -- in fact loading the process to run is slow.
    _This is much larger and noticeable than the Bazel build._
    
    This change introduces a common idiom to speed up large binaries by
    separating the debug information (DWARF) into separate files (`*.dwo).
    In order to speedup GDB attachment, a gdb-index is created and stored in
    the file itself.
    
    One could achieve the same effect with `.gdbinit` settings however the
    management of the index files are never pruned and it's a bit cumbersome
    to rely on developers to set it up.
    
    For reference here is a possible `.gdbinit`
    ```
    # History.
    set history filename ~/.gdb_history
    set history save on
    set history size 100000
    
    # Makes multiple invocations much faster
    set index-cache on
    
    # Allow per-project gdbinit files
    set auto-load local-gdbinit on
    add-auto-load-safe-path /
    ```
    
    **Note**: Looks like you also need to build MLIR with separate dwarfs as
    well. To be honest, the interplay between settings on LLVM and then on
    StableHLO are not very clear at times. Some settings seem to propagate
    based on how the previous object code was created and others did not.
    
    Specifically for GDB launch, you can see a very big time saving in the
    below benchmark. **76s vs 1.3s**
    
    ## Inline debug info
    **Size**: 3.2GiB
    ```shell
    ‚ùØ ll -h bin/stablehlo-opt
    Permissions Size User   Date Modified Name
    .rwxr-xr-x  3.2G 780412 31 Jan 18:14  bin/stablehlo-opt
    ```
    
    Most of the space is the debug information.
    ```shell
    ‚ùØ /google/bin/releases/protobuf-team/bloaty/bloaty bin/stablehlo-opt --allow_unsafe_non_google3_input
        FILE SIZE        VM SIZE
     --------------  --------------
      60.2%  1.80Gi   0.0%       0    .debug_info
      12.4%   378Mi   0.0%       0    .debug_str
      10.3%   314Mi   0.0%       0    .debug_loclists
       5.9%   180Mi   0.0%       0    .debug_line
    ```
    
    **Benchmark**
    ```
    # benchmark just running the build
    ‚ùØ hyperfine './build/bin/stablehlo-opt --version'
    Benchmark 1: ./build/bin/stablehlo-opt --version
      Time (mean ¬± œÉ):     181.6 ms ¬±   3.8 ms    [User: 73.4 ms, System: 108.2 ms]
      Range (min ‚Ä¶ max):   173.8 ms ‚Ä¶ 188.3 ms    16 runs
    
    
    # benchark with GDB
    ‚ùØ hyperfine 'gdb -ex run --args ./build/bin/stablehlo-opt --version' --warmup 1 --runs 3
    Benchmark 1: gdb -ex run --args ./build/bin/stablehlo-opt --version
      Time (mean ¬± œÉ):     74.063 s ¬±  2.381 s    [User: 71.044 s, System: 3.958 s]
      Range (min ‚Ä¶ max):   72.361 s ‚Ä¶ 76.784 s    3 runs
    ```
    
    ## With separate debug info
    
    **Size**: 847M
    ```shell
    ‚ùØ ll -h build/bin/stablehlo-opt
    Permissions Size User   Date Modified Name
    .rwxr-xr-x  847M 780412 31 Jan 17:29  build/bin/stablehlo-opt
    ````
    
    Much of the space is now a `gdb-index` to make attaching to the debugger
    much faster
    ```shell
    ‚ùØ /google/bin/releases/protobuf-team/bloaty/bloaty build/bin/stablehlo-opt --allow_unsafe_non_google3_input
        FILE SIZE        VM SIZE
     --------------  --------------
      38.0%   306Mi   0.0%       0    .gdb_index
      24.2%   195Mi   0.0%       0    .debug_addr
      20.6%   166Mi   0.0%       0    .debug_line
       6.5%  52.5Mi  55.5%  52.5Mi    .text
    ```
    **Benchark**:
    ```
    # benchmark just running the build
    ‚ùØ hyperfine './build/bin/stablehlo-opt --version'
    Benchmark 1: ./build/bin/stablehlo-opt --version
      Time (mean ¬± œÉ):      38.6 ms ¬±   1.6 ms    [User: 15.7 ms, System: 23.1 ms]
      Range (min ‚Ä¶ max):    36.5 ms ‚Ä¶  44.6 ms    68 runs
    
    # benchark with GDB
    ‚ùØ hyperfine 'gdb -ex run --args ./build/bin/stablehlo-opt --version'
    Benchmark 1: gdb -ex run --args ./build/bin/stablehlo-opt --version
      Time (mean ¬± œÉ):      1.401 s ¬±  0.037 s    [User: 1.532 s, System: 0.883 s]
      Range (min ‚Ä¶ max):    1.345 s ‚Ä¶  1.445 s    10 runs
    ```

[33mcommit 968b59aadde0aa2b51e3059d392a4dfef4917b06[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 31 23:09:23 2024

    Fix SC2155 issues (#1975)
    
    I forgot to include this in
    https://github.com/openxla/stablehlo/pull/1971 (made the change locally
    and forgot to push before merging).
    
    I originally suppressed these lints because I misunderstood how `set -o
    errexit` interacts with `readonly`. I thought a failure would still
    bring down the whole script, but that's not the case. So instead of
    suppressing the lint, I fixed the issue.

[33mcommit 9a52ae04b09a9017b2c4be7f3ff305ba900b936e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 31 22:30:10 2024

    Add shellcheck lint to CI (#1971)
    
    shellcheck is a shell script analysis tool that can find bugs and subtle
    issues in shell scripts: https://www.shellcheck.net/
    
    As part of adding the check, this commit also fixes any issues that are
    currently surfaced by shellcheck. These fell into the following
    categories:
    - Adding missing shebangs to scripts because shellcheck recommendations
    depend on the shell being used.
    - [SC2086](https://www.shellcheck.net/wiki/SC2086) (info): Double quote
    to prevent globbing and word splitting.
    - [SC2034](https://www.shellcheck.net/wiki/SC2034) (warning): <variable>
    appears unused. Verify use (or export if used externally).
    - [SC2207](https://www.shellcheck.net/wiki/SC2207) (warning): Prefer
    mapfile or read -a to split command output (or quote to avoid
    splitting).
    - [SC2185](https://www.shellcheck.net/wiki/SC2185) (info): Some finds
    don't have a default path. Specify '.' explicitly.
    - [SC2068](https://www.shellcheck.net/wiki/SC2068) (error): Double quote
    array expansions to avoid re-splitting elements.
    - [SC2145](https://www.shellcheck.net/wiki/SC2145) (error): Argument
    mixes string and array. Use * or separate argument.
    - [SC2236](https://www.shellcheck.net/wiki/SC2236) (style): Use -n
    instead of ! -z.
    - [SC2155](https://www.shellcheck.net/wiki/SC2155) (warning): Declare
    and assign separately to avoid masking return values. (We have a few
    cases of this where we define `readonly` variables using a command that
    could fail.)
    
    I am proposing that we use a lint level of `style`, which is the minimum
    level provided by shellcheck (so we will be running all lints). Most of
    the lints reported by shellcheck were of the `info` variety, due to
    variables being expanded outside of quotes (e.g. `echo $x` instead of
    `echo "$x"`). However there were a few warnings and errors as well, and
    one "style" lint which I believe is a no-brainer (use `-n` instead of `!
    -z`).
    
    I am also adding suppression comments for the following:
    - [SC2016](https://www.shellcheck.net/wiki/SC2016) (info): Expressions
    don't expand in single quotes, use double quotes for that. (There are a
    few places where we actually do not want `$...` to be expanded.)
    - [SC1003](https://www.shellcheck.net/wiki/SC1003) (info): Want to
    escape a single quote? echo 'This is how it'\''s done'. (This occurs in
    a specific use of `sed` to add a newline to files: `sed -i -e '$a\'` --
    apparently shellcheck doesn't know about this idiom.)

[33mcommit fd52182f76cadb82f2064fe5fc49a4fb4347a826[m
Author: Sambhav Jain <sambhav.jain@getcruise.com>
Date:   Wed Jan 31 20:25:46 2024

    Make setup_sanitizer include optional in CMakeLists.txt (#1972)
    
    Fixes
    https://github.com/openxla/stablehlo/issues/1962#issuecomment-1919576685
    
    Context:
    This breaks torch-mlir's CMake build when bumping stablehlo.
    
    The CMake build
    [fails](https://github.com/llvm/torch-mlir/actions/runs/7729517677/job/21072878412?pr=2821)
    with setup_sanitizers. Probably related to
    https://github.com/openxla/stablehlo/pull/1959 (cc: @fzakaria ,
    @mlevesquedion ). I see the default for `STABLEHLO_ENABLE_SANITIZER`
    being `OFF`, but this code probably needs to be enclosed in an
    `if-endif`:
    ```
    include(SetupSanitizers)
    setup_sanitizers()
    ```
    Error:
    ```
      -- Building StableHLO as an external LLVM project
      -- Building with -fPIC
      CMake Error at /_work/torch-mlir/torch-mlir/externals/stablehlo/CMakeLists.txt:147 (include):
        include could not find requested file:
    
          SetupSanitizers
    
    
      CMake Error at /_work/torch-mlir/torch-mlir/externals/stablehlo/CMakeLists.txt:148 (setup_sanitizers):
        Unknown CMake command "setup_sanitizers".
    
    
      -- Configuring incomplete, errors occurred!
    ```

[33mcommit 728a7b143924156de0c945f3f0af1673574fce39[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 30 21:52:34 2024

    Add custom builder to reduce op allowing type inference (#1965)
    
    The PR implements the a custom `reduce` op builder similar to what we
    have for mhlo
    [code](https://github.com/openxla/xla/blob/50aec2b3b54ce7a861f45bc3b0ae9b2cc2ee2a28/xla/mlir_hlo/mhlo/IR/hlo_ops.cc#L3917).
    
    ## Background
    https://github.com/openxla/stablehlo/pull/1869 allows the block
    arguments of reduce op to have different element types than that of the
    input arguments of reduce op and the output element type of the reduce
    op has to equal to those block arguments. As a consequence the output
    type of reduce op can no longer be inferred from the operand types. The
    auto-generated builders creates a reduce op with empty block and, as a
    result, does not allow inferring the type.
    
    The proposed solution is to create a custom builder which takes the
    element-type of the block arguments as arguments allowing result type
    inference.

[33mcommit 5cad2345a9bf8006854c9b2f2e4028547446f461[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 30 21:20:44 2024

    Remove tests for deleted attrs (#1969)
    
    The relevant attrs were removed in
    https://github.com/openxla/stablehlo/pull/1952. The tests kept passing
    because the tests don't target the attrs directly, rather they target
    ops that used to use the attrs. The tests were modified in
    https://github.com/openxla/stablehlo/pull/1930/ and I forgot to delete
    them.

[33mcommit 5dfbaca5158df1cfb33fedef78f2aebb1edb8934[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 30 21:20:25 2024

    Port dense literals to array literals in spec (#1963)
    
    This was accidentally omitted in previous changes.
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit 8f50622849c1321424b14f8149f532e54dc466d5[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Tue Jan 30 18:50:31 2024

    Add instrumentation for Google Sanitizers (#1959)
    
    Google Sanitiziers (https://github.com/google/sanitizers) have been
    integrated into Clang & GNU.
    
    Add support for ASan + UBSan if a CMake option is set. This should help
    catch some of the same errors that can be provided typicaly within
    internal Google development.
    
    TODO: Add support for TSan which needs to run individually since it
    cannot be instrumented alongside ASan.
    
    ---------
    
    Co-authored-by: mlevesquedion <mlevesquedion@google.com>

[33mcommit a808888867766dd4354d0841b87d1c84a54912ab[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 30 18:35:36 2024

    Bump patch version after integrate 0.17.5 -> 0.17.6 (#1968)

[33mcommit 6661fb318a166c61ddde7867af473989bf227dc8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Jan 29 20:12:50 2024

    Add CollectiveBroadcastOp to the status tracker (#1957)
    
    * Spec: yes, merged through RFC
    * Verification: revisit, currently all of the collective ops are marked
    revisited. Will need to look into the verifications of this category
    further.
    * Type Inference: yes, inferred trivially
    * Pretty Printing: no
    * Interpreter: no

[33mcommit bfea42289b25e9a9a3cd34a0276dabf66be3212a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jan 29 16:18:17 2024

    Add instructions for opset change contributions to CONTRIBUTING.md (#1921)
    
    Although the RFC process under OpenXLA is not fully standardized yet,
    there have been a few changes to StableHLO lately which had success
    using a similar process. This PR adds some documentation on the steps
    they followed to `contributing.md`.

[33mcommit 00ae7d439dc1bf7a758ac0fa4b420559fab183cb[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 26 21:00:44 2024

    Avoid explicitly constructing compatible attributes in a few places (#1958)
    
    Attrs will be implicitly copied. Avoid explicitly constructing the attrs
    as this introduces a dependency on the type of the attrs, meaning the
    code will need to be changed if the attrs' type changes. (Indeed I had
    to change this code recently when we changed the attrs from
    DenseIntElementsAttr to DenseI64ArrayAttr).

[33mcommit d4cb569b5d093d0013e670b5891abb2118409275[m[33m ([m[1;33mtag: [m[1;33mdev-wheels[m[33m)[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Fri Jan 26 18:49:28 2024

    Change secret for wheel publishing (#1955)
    
    The workflow GitHub App default credential in OpenXLA cannot create
    GitHub releases.
    
    Follow the convention identified by IREE in OpenXLA which uses a
    [repository
    secret](https://github.com/openxla/iree/blob/79b6129e2333ae26e7e13b68c27566102dcece6e/.github/workflows/validate_and_publish_release.yml#L112C37-L112C55)
    linked to a developers credential with repo permissions.
    
    The secret created here was created by gleasonk@ and the secret at the
    moment of this commit belongs to fzakaria@.
    It is valid for 1-year.

[33mcommit a404438cffcde8a61f698561139e00c7dd2de507[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 26 17:50:55 2024

    Only build Python bindings when requested (#1951)
    
    The `cmake` command has the `DMLIR_ENABLE_BINDINGS_PYTHON` key twice,
    once defined from the `MLIR_ENABLE_BINDINGS_PYTHON` shell variable, and
    once defined to `ON`. We should always use the shell variable.

[33mcommit 7eb9902e1ef75bc395f030f937a0ef3a9be4ad65[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 26 00:43:27 2024

    Port *ArrayOr* -> Dense*ArrayAttr in CHLO and SHLO (#1952)
    
    This completes the migration to `DenseI64ArrayAttr` started in
    https://github.com/openxla/stablehlo/pull/1658 (superseded by
    https://github.com/openxla/stablehlo/pull/1872) and announced in
    https://groups.google.com/a/openxla.org/g/openxla-discuss/c/hEoA4V5DZF0
    
    Fixes https://github.com/openxla/stablehlo/issues/1578

[33mcommit e37b4b0f4b97410dbbb7652f7ca862fafdcf703e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 26 00:16:50 2024

    Replace non-breaking spaces with regular spaces (#1953)
    
    Even though there is (for the most part) no difference in how these
    characters are rendered, in some cases this can cause subtle issues that
    are hard to debug.
    
    I used `ripgrep` to find the non-breaking spaces: `rg '\xA0'`.

[33mcommit cd0ca6917cb19d79bb16ce6142aca1bfcf4706a4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 25 23:36:12 2024

    Notify before returning to avoid race condition (#1947)
    
    There's a data race when the last process returns via `std::move`. If
    the other processes are notified before the move happens, they will hang
    since `state.result` is still not null. To prevent this, we move it to
    another variable first so that `state.result` is empty before calling
    notify to other waiting processes.
    
    Similarly, once the last process that contributed exits and notifies
    another process, it should notify other waiting processes before
    exiting. Otherwise, they will hang until the timeout, check the
    condition is true, and then proceed forward, which is not ideal.
    
    Also includes minor formatting & typo fixes.
    
    
    Here's an example output from debugging:
    ```
    (0,0): (A) values.size() = 1
    (0,1): (A) values.size() = 2
    (1,0): (A) values.size() = 3
    (1,1): (B) values.size() = 4
    
    (1,1): use_count: 1
    
    (0,0): finished waiting for all process to contribute
    (0,0): state.result is null? 0
    (0,0): use_count: 2
    (0,1): finished waiting for all process to contribute
    (0,1): state.result is null? 0
    (0,1): use_count: 3
    (1,0): finished waiting for all process to contribute
    (1,0): state.result is null? 0
    (1,0): use_count: 4
    
    (1,1): finished reading
    
    (1,1): use_count: 0
    (1,1): im exiting
    
    (0,0): use_count: 0
    (0,0): last process exited, im exiting
    (0,1): use_count: 0
    (0,1): last process exited, im exiting
    (1,0): use_count: 0
    (1,0): last process exited, im exiting
    ```

[33mcommit 1b939551042c51339cef10c9daa14ce43a31cbd1[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Jan 25 21:10:35 2024

    Bump API version (#1950)

[33mcommit 07a518baeeff11418e22d4520618641b9c629673[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 25 20:45:54 2024

    Fix clangtidy issue found in downstream integrate (#1949)

[33mcommit c30f551469ca37a1f2a8c8ac42ef1b989573dce6[m[33m ([m[1;33mtag: [m[1;33mv0.17.5[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 25 19:27:25 2024

    Integrate LLVM at llvm/llvm-project@98509c7f9792 (#1948)

[33mcommit cd58390da733d79144b905a60dee720d929ad338[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Wed Jan 24 23:00:35 2024

    Introduce capability to build and archive commit level Python wheels (#1924)
    
    This PR introduces a new GitHub action which triggers on commits to main
    (or manual execution) that build Pythonn (3.10 & 3.11) wheels
    (distribution format) and stores the wheels under a GitHub releases.
    
    ```shell'
    pip install stablehlo -f https://github.com/openxla/stablehlo/releases/expanded_assets/dev-wheels
    ```
    
    The 'trick' of using GitHub releases as a PyPI storage mechanism is
    outlined in [this blog
    post](https://fzakaria.com/2024/01/15/abusing-github-as-a-pypi-server.html).
    
    At a high level the changes are:
    * Introduce a `setup.py` that covers the necessary files to include in
    the wheel file. The setup.py **presumes** that a StableHLO was built
    already **and** with Python bindings
    * A new GitHub action is triggered on commits to main.
    * Made minor changes to all the scripts to make a few CMake variables
    configurable such as CMAKE_BUILD_TYPE and whether to build the Python
    bindings
    
    An example of this working can be seen on my fork
    https://github.com/fzakaria/stablehlo release page
    https://github.com/fzakaria/stablehlo/releases/tag/dev-wheels
    
    Many thanks to @makslevental who helped me a lot on the Discord channel.
    He pointed me to a few other solutions in MLIR that do similar approach
    which was the inspiration such as:
    - https://github.com/makslevental/pristine-llvm-release
    - https://github.com/Xilinx/mlir-aie
    - https://github.com/llvm/torch-mlir

[33mcommit 513d56c2686956b04a355695bd6cb10bdc1754ae[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jan 24 19:30:41 2024

    Update TOC to have more natural groupings (#1946)
    
    Updates the openxla.org navbar to have groupings:
    
    <img
    src="https://github.com/openxla/stablehlo/assets/3036970/dd326976-8ace-499a-8403-0446e51bfedd"
    height=400>
    
    This is a first step toward incremental documentation improvements, TOC
    and doc page content should be revisited.

[33mcommit 5ccd103041eec65d0954fd140c7ab7006f9ecb9e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jan 24 18:16:10 2024

    Update link and filename to match op mnemonic (#1945)

[33mcommit 94e40fb3f43e5793b8eda6f29da419bf55a5287a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jan 24 17:57:52 2024

    Fix broken spec example links (#1944)

[33mcommit aaa3544aeb7c1c1922c58450b93ce0ddce685159[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jan 24 17:57:41 2024

    Add missing spec example links (#1943)

[33mcommit 35220d18dae3772c202ef8ba1e08cea54be793a0[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Wed Jan 24 00:32:25 2024

    minor - correct the link to a file vhlo_to_version_downgrade_invalid.0_9_0.mlir (#1942)

[33mcommit 645d89dbfd6017b4d405dc7d282c0dd392fd1959[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 24 00:12:18 2024

    Emit dense arrays and use dense arrays in tests (#1940)
    
    The purpose of this change is to ease the transition from
    I64DenseArrayOrElements1DAttr to DenseI64ArrayAttr. The attrs themselves
    will be changed in a follow-up.
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit b417e92fac8a4d0f302b36960b5821378c92987d[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Wed Jan 24 00:02:24 2024

    Minor update to compatibility doc wrt serialization (#1941)
    
    ref: https://github.com/openxla/stablehlo/issues/1653

[33mcommit acbbb83fe9ff1cf17e4cb9d9b914f05ac928c1f1[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 23 22:24:23 2024

    Fix `all_gather` to concat in order of process group (#1934)
    
    Currently, the implementation concatenates rendezvous tensors which are
    sorted by process ids. However, the spec implies that the order of
    concatenation happens in the same order as the generated `processGroup`,
    which is not necessarily sorted. This change reflects what is implied in
    the spec.
    
    Thanks @linuxlonelyeagle for filing the issue!
    
    closes #1933

[33mcommit 0bfc22b950125460869f61e47e6261a6b5671a34[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 23 22:23:45 2024

    Bump patch version after integrate 0.17.4 -> 0.17.5 (#1939)

[33mcommit 85d9ad563f0d1672c8532d7abc886778223bdf45[m[33m ([m[1;33mtag: [m[1;33mv0.17.4[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Jan 22 22:08:52 2024

    Integrate LLVM at llvm/llvm-project@21830c913505 (#1938)

[33mcommit 85e839ee3e8ff4f7c1c07fed6963260351cd40f0[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jan 22 16:18:24 2024

    Use array literals where possible in MLIR source (#1930)
    
    This change flips all the literals that can be written using `array`
    instead of `dense` without requiring C++ changes. The purpose of this
    change is to reduce the diff in a future change, where I will actually
    change the op fields to use `DenseI64ArrayAttr` instead of
    `I64DenseArrayOrElements1DAttr`. That will require making changes to
    parsing and serialization, among others.

[33mcommit f1a390e768cbc61548fc0fbe564c603f816d77bf[m
Author: Abhinav Gunjal <154359538+abhigunj@users.noreply.github.com>
Date:   Sat Jan 20 01:41:57 2024

    Add CHECK to validate resulting VHLO include reduce_v1 (#1936)
    
    ref: https://github.com/openxla/stablehlo/issues/1843
    Tested: local test run, presubmit run

[33mcommit cf8607dd52659381a6df1e76d6730b65386af3ef[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jan 19 22:50:35 2024

    Bump patch version after integrate 0.17.3 -> 0.17.4 (#1937)

[33mcommit e4c563f43f7480716411d7e2f740b1314ec3eb1d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jan 19 19:07:12 2024

    Fix typo in the spec example for all_reduce (#1935)
    
    The spec example for `all_reduce` a typo.
    [ref](https://github.com/openxla/stablehlo/blob/20255865ba299ed67bdf6267478c8477aef7a60d/stablehlo/dialect/StablehloOps.td#L1350)

[33mcommit 20255865ba299ed67bdf6267478c8477aef7a60d[m[33m ([m[1;33mtag: [m[1;33mv0.17.3[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jan 19 01:39:30 2024

    Integrate LLVM at llvm/llvm-project@3a82a1c3f6bd (#1932)
    
    Also accommodating changes related to
    
    -
    https://github.com/llvm/llvm-project/commit/5fcf907b34355980f77d7665a175b05fea7a6b7b
    -
    https://github.com/llvm/llvm-project/pull/78376/files#diff-04e8290220df887ebaa6c4bf8ba9d121af3df7536732e447029b6f342b5aaa62

[33mcommit 7669d3adf6ca9b99c68746d18ca97b8a060a3531[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 19 00:34:58 2024

    Fix memory alignment bug in dense array conversion (#1925)
    
    I'm not sure specifically why this is happening, but creating a
    SmallVector from an ArrayRef can cause an illegal instruction because
    the memory can be misaligned. I will investigate further offline, but
    for now let's fix the bug.

[33mcommit 3bd2fadafabe5cc3c0c737fbbce94fc7c0d16f30[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jan 19 00:06:24 2024

    Updates to shape functions enabling reuse from MHLO (#1918)
    
    The upstream change https://github.com/openxla/stablehlo/pull/1869 in
    StableHLO updates various API related to shape inference. MHLO shape
    inference functions in
    [hlo_ops.cc](https://github.com/openxla/xla/blob/main/xla/mlir_hlo/mhlo/IR/hlo_ops.cc)
    uses those APIs. The PR updates the visibility and signature of those
    API for a clearer integration.
    
    Specifically, the PR does the followings:
    1. **updates `getAccumulatorTypes` to return a error status when the
    input regions is empty**: This function is used in type inference of
    various reduction based operations
    ([eg](https://github.com/openxla/stablehlo/blob/d5b464925371092095ac934b46ba93ebd4284223/stablehlo/dialect/TypeInference.cpp#L2589)).
    This functions enables infering type based on the reduction block of the
    operation, which is something proposed in
    [RFC](https://github.com/openxla/stablehlo/pull/1664). However, there
    could be
    [instances](https://github.com/openxla/xla/blob/a91877b9c9aa1edf307c5927782111b1a81cd81d/xla/mlir_hlo/mhlo/transforms/group_reduction_dimensions/group_reduction_dimensions.cc#L228)
    when type inference can be called with empty region in which case we
    would like to report a meaningful diagnostic message.
    
    2. **Allow `hlo::inferAllReduceOp` to accept multiple operands
    information**: In stableHLO, `all_reduce` op have a single operand
    ([e.g.](https://github.com/openxla/stablehlo/blob/d5b464925371092095ac934b46ba93ebd4284223/stablehlo/dialect/StablehloOps.td#L1355)),
    whereas in MHLO the op can take multiple operand
    ([e.g.](https://github.com/openxla/xla/blob/79aba0801ef75c1c2dffbb4ecc506a0d8144c9ac/xla/mlir_hlo/mhlo/IR/hlo_ops.td#L1528).
    The `hlo::inferAllReduceOp` signature is updated to accommodate both
    cases.
    
    3. Remove unused arguments to functions
    `verifyReduceOpInputsAndInferShape` and `inferReduceOp`.

[33mcommit f4e084c2648f7b61bad5c18c829f041536c725b8[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Jan 18 21:23:14 2024

    Add licensecheck lint to CI and add missing licenses (#1929)
    
    For the most part, `addlicense` handles the file types that we care
    about. It also doesn't handle some files that we (AFAICT) don't want
    license headers in: `.md` and `.mlir` files, e.g. However, it doesn't
    handle some files for which we may want license headers: `.bazel`, some
    `.in` files, `.pdll`, `.td`, `.bazelignore`, `.bazelrc`, `.bazelversion`.
    I think it's debatable whether we want license headers in `.gitignore`
    and `.clang-format` files, and these are also not handled by `addlicense`.
    
    For `.bazel`, `.pdll` and `.td`, I will see if I can contribute to
    `addlicense` to handle those. For `.in` files (e.g.
    `lit.site.cfg.py.in`), I think whether or not we want a license actually
    depends on what the "true" type of the file is (e.g. `.py`). Supporting
    that would likely require additional changes to `addlicense`. Actually,
    I think it would be great if `addlicense` had some kind of extensibility
    mechanism (https://github.com/google/addlicense/issues/50).
    
    Anyway, although imperfect I think on the whole this should help us
    catch missing licenses in a lot of cases we care about.
    
    Inspired by https://github.com/openxla/stablehlo/pull/1928

[33mcommit 9aa9a26a3c233d440335a0c3153a541c2b7c921a[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Thu Jan 18 21:05:38 2024

    Make building Python bindings optional (#1926)
    
    Many developers are using the ci_build_* scripts to build LLVM and
    StableHLO.
    
    A previous PR #1923 made the CI system by default building python
    bindings however there is an undocumented step of installing Python
    dependencies such as pybind.
    
    To simplify local development, make the option configurable with the
    default to OFF so that the ci_build_* actions work without any
    additional setup.
    
    The CI system itself (GitHub actions) set the appropriate variables to
    build the Python bindings.

[33mcommit 3be61bdfa46d46314a288c7d1d4b5b583367d96d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jan 18 18:50:52 2024

    Add missing copyright message (#1928)
    
    The copyright message was missing `docs/_toc.yaml`.

[33mcommit fe5a549a522a2c7b506d2419ef1536cb0700208f[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jan 18 18:48:59 2024

    Bump patch version after integrate 0.17.2 -> 0.17.3 (#1927)

[33mcommit cbdf9315f02a4b43aa1295db6cb504a705441321[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 17 21:43:36 2024

    Leverage tablegen more for StablehloLegalizeToLinalg and other passes (#1919)
    
    For the `StablehloLegalizeToLinalg` pass:
    - Define the `dependentDialects` field instead of explicitly defining
    `getDependentDialects`.
    - Use the generated pass declaration and registration (`GEN_PASS_DECL`,
    `GEN_PASS_REGISTRATION`).
    - Use `GEN_PASS_DEF_*` instead of `GEN_CLASSES` (deprecated).
    - A few tweaks to the way the pass struct is written.
    
    In addition to that:
    - Use `GEN_PASS_DECL` to include generated pass declarations, rather
    than explicitly listing the passes.
    - Use `register*Passes` instead of explicitly listing the passes being
    registered.
    
    In total these changes reduce duplication of functionality that can be
    provided by Tablegen and make our use of Tablegen more consistent across
    different parts of the codebase.

[33mcommit fd3df66858d4e06cbf427d3829ca68be18435062[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Wed Jan 17 05:29:43 2024

    Remove separate cmake_python_api github action (#1923)
    
    Remove separate cmake_python_api github action and have ci_build_cmake
    and ci_build_cmake_llvm always create the python bindings.
    
    This should cut down on rebuilding LLVM itself which takes ~5 minutes
    with ccache and much longer on new builds.

[33mcommit 9c8a1b7ddd5e4ef5abbdc7ef33b041a56343ae2f[m[33m ([m[1;33mtag: [m[1;33mv0.17.2[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 16 23:44:48 2024

    Move definition of vhloOperands right before it is used (#1920)
    
    This makes the code a bit more readable. On my first read I was puzzled
    as to why this getter was being called.

[33mcommit 2a79b7c0a03d45009fd50f49af7829bd7c376a23[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 16 22:11:53 2024

    Move large categories of tests to their own folders (#1913)
    
    The `tests` folder contains a lot of tests for various things. In
    particular, there are a lot of tests for the interpreter and a lot of
    tests related to VHLO, so IMO it makes sense to give these their own
    folders. I find that this helps navigate the tests.
    
    I considered creating folders for CHLO, printing related tests, and
    verification related tests, but IMO there aren't enough of those to
    really warrant it.

[33mcommit a918afcea484528e05fca4ff8d030be1368ec2f0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 16 19:16:07 2024

    Integrate LLVM at llvm/llvm-project@baba0a4cb431 (#1922)

[33mcommit d5b464925371092095ac934b46ba93ebd4284223[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Jan 11 20:16:29 2024

    Bump patch version after integrate 0.17.1 --> 0.17.2 (#1917)

[33mcommit 16a5169870c1d9d5f450ff16df420d38ce6d280c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 10 22:17:43 2024

    Lint bazel files automatically in pull requests (#1915)
    
    I adapted the job definition from
    https://github.com/openxla/xla/blob/main/.github/workflows/buildifier.yml.
    
    Also removing a repeated comment in `lint.yml` that seems to be
    inaccurate in most places.

[33mcommit dced11ed01c6fdc559f1ed3ab01d10961441d609[m
Author: Farid Zakaria <fmzakari@google.com>
Date:   Wed Jan 10 21:05:10 2024

    Layering Add support for layering_check and change default toolchain (#1916)
    
    Bazel has support for layering_check identification which helps validate
    that sources are only pulling in header files from their direct
    dependencies.
    
    This is considered best practice in Bazel, since if you depend on
    indirect dependencies it's easy to break upstream package consumers
    unknowingly. At the moment layering_check is only supported by Clang.
    The change also changes the default CC variable to be Clang as a result.
    (ref: https://bazel.build/reference/be/c-cpp)
    
    * Changed BUILD files accordingly to support layering_check. The build
    fails otherwise without the changes once layering_check is enabled.
    * Additionally ran Buildifier on all build files to make them consistent
    as a separate commit.
    
    Additional Notes:
    - We should consider adding a toolchain directly in Bazel to make it
    hermetic or consider using Nix to bring in workspace tooling.
    -  We should add add buildifier as a github_action lint check (#1915)

[33mcommit 926b4e597080134da835ec8b82b3714d6a8794d0[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 10 19:04:57 2024

    Use distinct, more specific name for testdata suite (#1914)
    
    The name seems to have been copied from the tests suite. Sharing the
    name makes it a bit confusing which is which when listing tests, since
    tests under tests/ and testdata/ will be prefixed with the same name,
    e.g.
    
    ```
    STABLEHLO_TESTS_SUITE :: infer_chlo.mlir
    ...
    STABLEHLO_TESTS_SUITE :: zeros_like_shape_uint8_3_4_5.mlir
    ```

[33mcommit 9edace67ae94a4c5c1c1c802f00e2d36c947a0a4[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 10 19:03:56 2024

    Fully qualify storage types of ArrayOrElements attrs (#1912)
    
    There is no guarantee that users of the generated .inc files are working
    in an `mlir` namespace, so we need to include the explicit `mlir::`
    prefix.
    
    I discovered this because in some (out of repo) generated code an
    unqualified `Attribute` caused compilation failures.

[33mcommit 67b748f07f945f2ad0330e3e2ed75359a5da5b6c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 9 22:46:42 2024

    Disallow reusing variables across CHECK-LABEL directives (#1911)
    
    Ran into this while integrating: internal configuration sets
    `--enable-var-scope`, but we don't, so some tests failed internally but
    not here. I think in general this is a good setting to use: it's easier
    to understand a test if the only variables in scope are the variables
    defined by that test.

[33mcommit e2be1211367ac4e3c38b03679dec84ceb08b2282[m
Author: David Huntsperger <5672572+pcoet@users.noreply.github.com>
Date:   Tue Jan 9 19:00:46 2024

    Add a _toc.yaml file to specify doc nav (#1910)
    
    This is needed for integration with internal publishing tools.

[33mcommit 54fce144323c720bd40df098115f723f8103283f[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Jan 9 17:51:12 2024

    Use ArrayRef where SmallVector is not needed (#1909)
    
    For some reason while porting SelectAndScatter I used SmallVector for
    its verify function, and I used ArrayRef everywhere else. ArrayRef is a
    slightly better fit as it is more efficient and meant for reading only.
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit 11804dd311bcb5630074cd48a67caad823385dd3[m[33m ([m[1;33mtag: [m[1;33mv0.17.1[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jan 8 23:53:28 2024

    Integrate LLVM at llvm/llvm-project@ba4cf31facda (#1908)
    
    Bumping LLVM caused some failures because of `arith.constant` ops in the
    `infer_stablehlo.mlir` test being generated in a different (albeit still
    valid) order. I changed the checks for those ops to `CHECK-DAG` instead
    of `CHECK` since we do not care about the order.

[33mcommit c472822db47ac73545a0b2057e50e34d9f1a4f15[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jan 8 22:21:36 2024

    Port {Convolution,DynamicConv,Gather}Op to dense elements or array attrs (#1905)
    
    In the same vein as https://github.com/openxla/stablehlo/pull/1893.
    
    Also introduce an attr that can be either dense elements of bool or dense array of bool.
    
    Not porting the MLIR source files yet.
    
    #1578

[33mcommit a2d26249a669582a3815a43901fb61c6d155a58c[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Jan 8 19:38:59 2024

    Delete BroadcastDimAttr (no longer used) (#1907)
    
    Uses were removed in the following PRs:
    
    https://github.com/openxla/stablehlo/pull/1887/files#diff-c20bffbe3bc2037741cb7cf73b71ebf259fc419c15c2a6d3c39d6952ae59f89c
    https://github.com/openxla/stablehlo/pull/1893/files#diff-c20bffbe3bc2037741cb7cf73b71ebf259fc419c15c2a6d3c39d6952ae59f89c
    https://github.com/openxla/stablehlo/pull/1891/files#diff-8d62a195e9ed8776265bc22a7b8300534d4661e1c6bd659a21f27143311745dc

[33mcommit bc5713846727d01eacae92b6b5e808418919bd9e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Sat Jan 6 01:48:11 2024

    Port ReduceWindowOp to I64DenseArrayOrElements1DAttr (#1903)
    
    In the same vein as https://github.com/openxla/stablehlo/pull/1893.
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit 98eeea5db66eeef4bcb026eb8e120204fb7f0444[m
Author: Farid Zakaria <farid.m.zakaria@gmail.com>
Date:   Sat Jan 6 00:57:18 2024

    Remove location constraint on compact printing of ReduceOpt (#1906)
    
    During discussion of #1523 we came to the conclusion that it's
    impossible to continously guarantee that location information is kept
    forwrad and backwards through transformations.
    
    There is already precendent in other transformations that location
    information may be ellided.
    
    Remove the constraint for the compact pretty printing of ReduceOpt that
    requires the locations be the same. Update the tests as necessary.

[33mcommit e776cc9fdfc118d5f5d21b75a4c240ea817a8d96[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Sat Jan 6 00:02:22 2024

    Port SelectAndScatterOp to I64DenseArrayOrElements1DAttr (#1900)
    
    In the same vein as #1893.
    
    Not migrating the MLIR source yet.
    
    #1578

[33mcommit ff9fd7b8566d39f03d8abaf06925165c3918a5ae[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 5 23:33:29 2024

    Port {Map,Reduce}Op  to I64DenseArrayOrElements1DAttr (#1902)
    
    In the same vein as https://github.com/openxla/stablehlo/pull/1893.
    
    Not migrating the MLIR source yet since:
    - For ops that don't have custom printing, round tripping through VHLO doesn't work: if you use an array literal as input, you will get a dense literal as a result when converting from VHLO, unless you do a special conversion; but if you do the special conversion and you have a dense as input, you will get an array as a result.
    - Smaller changes are easier to integrate
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit 3e9303fd78acc04b28a619865ca8436428315f58[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 5 23:26:54 2024

    Port CHLO ops to I64DenseArrayOrElements1DAttr (#1891)
    
    This is an intermediate step in the migration to `DenseI64ArrayAttr`.
    
    I had to move the definition for the attr to `Base.td` because it's now
    used in `ChloOps.td` as well as `StablehloOps.td`.
    
    I also added a helper function to get a vector of int64_t from an Attribute,
    e.g. when getting `broadcast_dimensions`  through generic means:
    `op->getAttr("broadcast_dimensions")`.
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit a9be66eca1aa9854e2656b244350331d454b0e74[m
Author: luis <lpenagos@google.com>
Date:   Fri Jan 5 18:42:40 2024

    Introduce StableHLO interpreter module parsing API (#1885)
    
    This PR builds off of the recent interpreter reference API PR (#1797),
    by exposing an additional method `parseModule` which takes a serialized
    StableHLO MLIR program, and an accompanying `MLIRContext` input:
    
    ```
    llvm::ErrorOr<OwningOpRef<ModuleOp>> parseModule(const std::string &mlir, MLIRContext& context);
    ```
    
    This serves to augment the existing `evalModule` interface in
    `stablehlo/reference/Api.h` by allowing users to easily execute a
    serialized StableHLO program, without needing to handle all proper
    dialect registrations. Prior to this PR, users are responsible for
    providing an already parsed StableHLO `ModuleOp`, which requires they
    register all required MLIR/StableHLO dialects a priori (less user
    friendly interface).

[33mcommit 70bbd0a3444ece7cd2a2103c75307880119b2155[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Jan 5 18:12:06 2024

    Remove a few dense elements attrs that were missed in #1872 (#1888)
    
    Some of these were in the description for `slice` and some others were
    in tests where an earlier error caused the dense elements literals
    (which would have caused errors) to not be reached.
    
    I used the following command to look for missed dense literals:
    
    ```
    rg --multiline --multiline-dotall 'stablehlo\.(slice|dynamic_slice|fft|pad|reverse|rng|transpose)[^\n{]*?\{[^}]*?dense[^}]*?\}'
    ```
    
    (where `rg` is `ripgrep`)
    
    Thanks to @GleasonK  for catching this in the first place!

[33mcommit 130b55db62150009e571c3ff20074a5fe0b1492a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jan 5 16:12:49 2024

    Bump patch version after integrate 0.17.0 -> 0.17.1 (#1904)

[33mcommit e26de40109f6c117317b5f2adf90fae6c2e80de3[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Jan 3 17:41:32 2024

    Bump action versions to silence node12 warning (#1901)
    
    I noticed that some of our actions produce a warning when they run,
    e.g.:
    
    ```
    The following actions uses node12 which is deprecated and will be forced to run on node16: actions/checkout@v2, llvm/actions/install-ninja@55d844821959226fab4911f96f37071c1d4c3268. For more info: https://github.blog/changelog/2023-06-13-github-actions-all-actions-will-run-on-node16-instead-of-node12-by-default/
    ```
    
    This isn't a huge deal but it's unnecessary noise and it's probably best
    to use more recent versions of the actions we use anyway.
    
    So I'm bumping the `checkout` action (which is developed by GitHub) to
    the latest version (v4), and I'm bumping the LLVM `install-ninja` action
    to [the
    commit](https://github.com/llvm/actions/commit/6a57890d0e3f9f35dfc72e7e48bc5e1e527cdd6c)
    where the `node` version was bumped to 16.

[33mcommit f5785b09b831345782ae80900204c630634c59fb[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 2 22:07:29 2024

    Clarify semantics of `ReduceWindowOp` and `SelectAndScatterOp` (#1897)
    
    closes #1818

[33mcommit f9150988ff6ab9c93de23ee4310c00e134e154e8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 2 19:20:57 2024

    Fix table formatting in spec (#1899)

[33mcommit f8dcebfa1ec166806974f6ae0dfb902d36b47238[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 20 23:02:06 2023

    Verifier and Type inference changes for reduction operations (#1869)
    
    Implements the specification changes at
    https://github.com/openxla/stablehlo/pull/1796.
    
    The PR adds/updates ¬†the verifier and type inference routines for the
    following ops: `reduce, reduce_window, select_and_scatter, all_reduce,
    reduce_scatter, scatter`. Please refer to #1796 for the updated
    constraints which the PR implements. Note the #1796 is going to be
    merged soon.
    
    Here are the changes for each operation:
    
     - reduce¬†
         - #1796 added¬†a new constraint C8¬†
         - Updated labels
    - Add positive tests and negative tests verifying reduce_c6¬† at
    verify_reduce.mlir¬† and type inference tests¬†at infrer_stablehlo.mlir,
    for reduce_c8.
    
      - reduce_window:¬†
         - #1796 updated the C16
    - Add positive tests ; negative tests verifying reduce_window_c13¬† at
    verify_reduce_window.mlir¬† and type inference tests¬†at
    infrer_stablehlo.mlir, for reduce_window_c16.
    
      - select_and_scatter
         - #1796 added a new constraint C12
         - Updated labels
    - Add positive tests ; negative tests verifying selelct_and_scatter_c10¬†
    at verify_select_and_scatter.mlir¬† and type inference tests¬†at
    infrer_stablehlo.mlir, for select_and_scatter_c12.
    
      - scatter
    ¬† ¬†- #1796 added a new constraint C17
    ¬† ¬†-¬† ¬†Updated labels
    ¬† ¬†- Add positive tests ; negative tests verifying scatter_c15¬† at
    verify_scatter.mlir¬† and type inference tests¬†at infrer_stablehlo.mlir,
    for scatter_C17.
    
      - reduce_scatter
         - #1796 added a new constraint C9¬†
    - This op does not have the type inference supported. We had a trait
    `SameOperandsAndResultElementType` implementing the outdated constraint.
    For the new constraint C9, we added a check at `verifyReduceScatterOp`.¬†
    - Add positive tests; negative tests verifying reduce_scatter_C7¬† at
    ops_stablehlo.mlir¬† and type inference tests¬†at ops_stablehlo.mlir,
    for¬†reduce_scatter_C9.
    
      - all_reduce
         - #1796 added a new constraint C7
    - This op implemented an outdated constraint related to type inference
    using `inferReturnTypeComponentsFromOperands`. For the new constraint
    C9, we added a trait¬†`InferTensorType` in the tablegen definition¬†of the
    op.¬†
         - Updated labels¬†
    - Add positive tests; negative tests verifying all_reduce_C5¬† at
    ops_stablehlo.mlir¬† and type inference tests¬†at ops_stablehlo.mlir,
    for¬†all_reduce_C7.

[33mcommit cb42882825af722cc108e0051f760ec670321ce7[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Wed Dec 20 17:33:38 2023

    build: add missing dependency on InterpreterOps (#1894)
    
    Commit afd8f5bd6 introduced a file (StablehloInstrumentWithProbe.cpp)
    that references `interpreter::ProbeOp` (defined in the InterpreterOps
    library target), but this dependency was not added to the CMake build
    rules for building the StablehloPasses target, which includes code from
    StablehloInstrumentWithProbe.cpp.  As a result, builds (sometime) fail
    with the following error:
    
    ```
    Linking CXX shared library lib/libStablehloPasses.so
    CMakeFiles/obj.StablehloPasses.dir/StablehloInstrumentWithProbe.cpp.o:
    ...
    undefined reference to `mlir::stablehlo::interpreter::ProbeOp::build
    ...
    ```
    
    This patch fixes the CMake build error by adding InterpreterOps as a
    library dependency of the StablehloPasses target.
    
    Credit to @hamptonm1 for discovering the problem.

[33mcommit 3bdd8593a46ababb362928187d58ccc418e15b99[m[33m ([m[1;33mtag: [m[1;33mv0.17.0[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Dec 18 08:05:11 2023

    Updating API version (#1892)
    
    Updated API version + Added copyright declaration in the newly
    introduced `.bazelversion` file.

[33mcommit 76901e3cc780958d663858366923aa0584cb88b4[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Sat Dec 16 03:45:52 2023

    Port DynamicBroadcastInDim to I64DenseArrayOrElements1DAttr (#1893)
    
    This is an intermediate step in the migration to `DenseI64ArrayAttr`.
    
    This also includes changing `dense<...>` to `array<...>` in the tests
    for `BroadcastInDim` and `DynamicBroadcastInDim` (both are supported,
    but we want to move towards `array<...>`).
    
    https://github.com/openxla/stablehlo/issues/1578

[33mcommit 69eeee71540dc4e3470ddfc8afbb3ee5d8594ac5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Dec 16 00:32:18 2023

    Integrate LLVM at llvm/llvm-project@b2cdf3cc4c08 (#1890)

[33mcommit b8ccf29f784510ee2ef9d78d7fd453d8dd9150e9[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 15 22:21:44 2023

    Bump patch version after integrate 0.16.2 -> 0.16.3 (#1889)

[33mcommit 00a30800a72b6e24590ba276c35b1baac902dc76[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Fri Dec 15 18:05:00 2023

    Introduce attr that can be backed by either DenseI64ArrayAttr or I64ElementsAttr (#1887)
    
    Port `BroadcastInDim`'s `broadcast_dimensions` as an example.
    
    This will allow us (and downstream consumers) to port tests bit by bit.

[33mcommit e2e1deefdabff6397421dd8b02eff8836adba6c6[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Thu Dec 14 16:47:54 2023

    Remove redundant cppNamespace fields (#1886)
    
    The `cppNamespace` field for `AttrDef`s is
    [inherited](https://github.com/llvm/llvm-project/blob/2e45326b088b3b2f5c8327f6d5e61bdd2845bbbe/mlir/include/mlir/IR/AttrTypeBase.td#L255C7-L255C18)
    from the `Dialect`'s `cppNamespace` [via
    DialectAttr](https://github.com/llvm/llvm-project/blob/2e45326b088b3b2f5c8327f6d5e61bdd2845bbbe/mlir/include/mlir/IR/CommonAttrConstraints.td#L86).

[33mcommit 3b59670b339e5712fb0898860e11e9784cb407dd[m[33m ([m[1;33mtag: [m[1;33mv0.16.2[m[33m)[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Tue Dec 12 20:38:48 2023

    Bump patch version after integrate 0.16.1 -> 0.16.2 (#1884)

[33mcommit b02f545b54927595d95b4f38f149f44608c09c05[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Dec 11 23:49:07 2023

    Use ODS for StableHLO types (#1877)
    
    Closes #1876.

[33mcommit 3260a31f09744419377dae409043f12bb7418c38[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Dec 11 23:13:29 2023

    Integrate LLVM at llvm/llvm-project@2a988a38a0c9 (#1880)
    
    This includes a fix to the `infer_chlo.mlir` test, as this test is
    affected by folding behavior of the `shape_of` op, and this behavior was
    recently changed (see https://github.com/openxla/stablehlo/pull/1874 for
    details).

[33mcommit a63af12fa6de3b4354f93bac33c3fa0efbcdfb2e[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Mon Dec 11 21:27:33 2023

    Pin bazel version to 6.1.0 (#1879)
    
    This should be an effective stopgap for migrating to Bazel 7, which
    currently breaks our bazel build. This version of bazel should be
    supported until EOY 2025: https://bazel.build/release\#support-matrix
    
    This specific version was chosen to align with XLA:
    https://github.com/openxla/xla/blob/83bcebe6dd1081ab501d8d6689153c6bc3ba0344/.bazelversion#L1
    
    https://github.com/openxla/stablehlo/issues/1878

[33mcommit 059319f87bd019f24c6bf4be59beb7871060b382[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sat Dec 9 04:29:30 2023

    Fix VHLO DenseI64Array bug and refactor shape refinement pass (#1875)
    
    VHLO bug: `DenseI64ArrayAttr` doesn't have a notion of splats like
    `DenseIntElementsAttr` does. The following array when serialized as as
    the VHLO wrapper around `DenseIntElementsAttr` could not be converted to
    `DenseI64ArrayAttr` properly:
    
    ```
    dense<0> : tensor<2xi64>
    vs
    array<i64: 0, 0>
    ```
    
    There the value that gets serialized is just `0`. When converting to
    `DenseI64ArrayAttr` must look at the type and see if the splat should be
    expanded. This isn't a huge memory issue since these attrs are only used
    in cases where the number of values in the array is small.
    
    Shape refinement: Refactor into a header file with some patterns that
    can be populated, instead of a single API entrypoint.

[33mcommit ab709fe48de88c67717abfbd7ef17425eb95ddaf[m[33m ([m[1;33mtag: [m[1;33mv0.16.1[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 7 03:29:41 2023

    Integrate LLVM at llvm/llvm-project@c4cebe5b4489 (#1871)

[33mcommit b046f114ae5f6abfe29b7af11a11985b96023a80[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Dec 6 23:01:53 2023

    Use DenseI64ArrayAttr for ops which already specialized the printing/parsing as DenseI64ArrayAttr (copy and update of #1658) (#1872)
    
    This PR is based on #1658. I rebased the commits onto main, resolved
    conflicts, and made necessary changes to code that had been added since
    the last rebase.
    
    Since I was getting a _lot_ of compilation failures after rebasing and
    I'm new to the project/MLIR, I decided to port each op one at a time and
    I gave each op its own commit.
    
    Porting slice op required a lot of changes to `testdata` files. I wrote
    a Python script that used a regex to make the replacements:
    
    ```
    r'(.*?)"stablehlo.slice"(.*?){limit_indices = dense<([^>]+?)> : tensor<(\d)xi64>, start_indices = dense<([^>]+?)> : tensor<(\d)xi64>, strides = dense<([^>]+?)> : tensor<(\d)xi64>}( : .*)'
    ```
    
    This change partially implements #1578.
    
    **Special note for reviewers:** because I split the original commit into
    multiple commits, I don't know if authorship will be appropriately
    reflected (the original change was made by @joker-eph). Please advise.

[33mcommit 42387b0fecd4a2782b0b12f806dfc617e05c33f7[m
Author: mlevesquedion <mlevesquedion@google.com>
Date:   Wed Dec 6 02:45:58 2023

    Fix broken links (#1873)
    
    The roadmap is in its own document (it no longer has a header in the
    README) and some XLA stuff was moved out of tensorflow.
    
    The XLA MLIR interpreter was deleted in
    https://github.com/tensorflow/tensorflow/commit/079ab56a5b0c5aefcdffa4f20f79c7bb137c6bd0.

[33mcommit c015a21a7b494dc140a061ef0c1ade3953e04711[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 6 00:53:56 2023

    Specification of quantized `reduce` and other related ops (#1796)
    
    The PR implements the approved [RFC for reduce
    op](https://github.com/openxla/stablehlo/pull/1664) by proposing the
    specification related changes for `reduce`, `reduce_window` and
    `select_and_scatter` ops.
    
    In https://github.com/openxla/stablehlo/pull/1647, we talked about some
    of the other ops which will depend on the quantized specification of
    reduce op. Initially, I thought about creating separate PRs for them,
    but for the interest of time and the fact that their handling is going
    to be very similar to how `reduce` op is handled, I propose to include
    their PR in the current PR.
    
    Here are the additional ops (other than `reduce`, `reduce_window`,
    `select_and_scatter`) whose specification is added
    - Ops with explicit computation regions: `all_reduce`, `reduce_scatter`,
    `scatter`: They are handled similar to how `reduce` op is handled.
    
    - Ops w/o explicit computation region: `batch_norm_grad`,
    `batch_norm_training`: For these ops, the semantics of the operation
    implicitly does reduction with a custom computation function. As there
    is no explicit computation function in the IR, the proposal in the RFC
    cannot be applied. I propose (A) to handle these ops similar to how
    `batch_norm_inference` is handled with `dequant-op-quant` strategy, (B)
    we can revisit this op later if there is use case to do that implicit
    reduction using higher accumulation type.
    
    One implementation detail: The fact that `batch_norm_grad`,
    `batch_norm_training` ops returns multiple outputs and current
    `dequantize_op_quantize` returns a single output, is handled using a
    dedicated meta function for these two ops.
    
    
    
    Next steps: Once the PR is approved the plan is to propose the
    corresponding changes in the verifier/shape functions for these ops.
    
    **Only support promotion of reduction operands to reduction-block
    arguments**
    The specific question bought up in the discussion is: should we promote
    demotion as well? It seems that there aren't any use-cases for that. For
    example, consider the following cases of implicit conversion for
    reduction operations. The examples are shown using integer type but can
    be generalized to other types as well.
    
    | input element type | accumulator element type | result element type |
    use-cases |
    |:--:|:--:|:--:|:--|
    | i8| i8 |i8| Reduction using min/max|
    | i8| i32 |i32| Reduction using average/sum|
    | i8| i32| i8 | Demotion from accumulation type to result type. (No
    use-case)|
    | i8| i32| i64 | Extra promotion from accumulation type to result type.
    (No use-case) |
    | i32 | i8 | i8 | Demotion from input value type to accumulation type.
    (No use case) |
    
    With that in mind, we propose to develop the specification based on the
    scenario we have the use-case for (just for promotion: _none of the
    cases of demotion or extra promotion will be supported in the
    specification_.

[33mcommit b101dde7070e79506383599fa3d29596db825518[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Dec 5 02:23:53 2023

    Bump patch version after integrate 0.16.0 -> 0.16.1 (#1870)

[33mcommit afd8f5bd60e2ae50b446a127f223a8604d479f33[m
Author: luis <lpenagos@google.com>
Date:   Fri Dec 1 16:54:40 2023

    Introduce a compiler pass to instrument modules (#1841)
    
    This PR adds a compilation pass to insert the recently introduced
    `interpreter.probe` instrumentation op (#1784) into StableHLO programs.
    The pass is registered as `stablehlo-instrument` in the `stablehlo-opt`
    target (open to other naming suggestions).
    
    The pass will instrument all SSA values which are not the result of a
    `stablehlo.constant` operation (as there is no benefit in instrumenting
    unchanging values). The pass will attempt to use MLIR named location
    data to derive a suitable `probe_id`, and fallback to an increasing
    unsigned integer otherwise. For example, given the following simple
    function:
    
    ```
    func.func @main(%arg0: tensor<1x2xi32>, %arg1: tensor<1x2xi32>) -> tensor<1x2xi32> {
      %0 = stablehlo.constant dense<[0, 0]> : tensor<1x2xi32>
      %1 = stablehlo.add %0, %arg0 : tensor<1x2xi32> loc("add1")
      %2 = stablehlo.add %1, %arg1 : tensor<1x2xi32>
      func.return %2 : tensor<1x2xi32>
    }
    ```
    
    This pass would instrument it as follows:
    
    ```
    func.func @main(%arg0: tensor<1x2xi32>, %arg1: tensor<1x2xi32>) -> tensor<1x2xi32> {
      %0 = stablehlo.constant dense<[0, 0]> : tensor<1x2xi32>
      %1 = stablehlo.add %0, %arg0 : tensor<1x2xi32> loc("add1")
      %2 = interpreter.probe %1, probe_id = "add1" : tensor<1x2xi32>
      %3 = stablehlo.add %2, %arg1 : tensor<1x2xi32>
      %4 = interpreter.probe %3, probe_id = "1" : tensor<1x2xi32>
      func.return %4 : tensor<1x2xi32>
    }
    ```
    
    Where any operation without an explicit MLIR named location defaults to
    a unique, increasing `probe_id` integer value (i.e. in the example, the
    second add op). Note that as described above, constant operations are
    not instrumented. The pass walks all operations in the `mlir::ModuleOp`,
    so it will instrument values inside control flow structures as well
    (while, if, etc).

[33mcommit ad6a9000cc97dd018bffe961e8483410e77011e4[m
Author: luis <lpenagos@google.com>
Date:   Fri Dec 1 01:17:32 2023

    Do not throw error when executing empty StableHLO modules (#1864)
    
    This PR addresses a bug introduced by PR #1797 where as a result of the
    new interpreter API, running empty StableHLO modules would result in an
    error `Requested main function not found.` whereas previously, the
    `stablehlo-translate` utility would not throw any error.
    
    (Found by @ghpvnist in #1841).

[33mcommit 57e5a4a528a7e999f53c3719c1e68587efb9f0e6[m[33m ([m[1;33mtag: [m[1;33mv0.16.0[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Dec 1 00:56:11 2023

    Add EvalOrPattern to StablehloRefineShapes pass (#1867)
    
    Port of
    https://github.com/tensorflow/mlir-hlo/commit/c1322650917cd25f835db8a4cec49644011b1b73#diff-c4f3cba740037085e1b02f7f7de83522697ba55cf0577ca9d3e63a7a0f71e1ecR321

[33mcommit b3d018fb10dfaa36c91753f7475bffbb221c7a5d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Nov 30 23:45:52 2023

    Integrate LLVM at llvm/llvm-project@511ba45a47d6 (#1866)

[33mcommit 97ad2b12e18c6cd6869ab9508227a9c08e6f26eb[m
Author: luis <lpenagos@google.com>
Date:   Thu Nov 30 23:18:04 2023

    Serialize MLIR type when instrumenting reference interpreter values (#1828)
    
    Recently, we introduced a way to extract intermediate tensor state from
    the StableHLO reference interpreter (#1784) for
    instrumentation/debugging purposes. As part of this instrumentation
    process, the interpreter will create an `index.csv` metadata file which
    contains all serialized tensor paths and uniquely identifying `probe_id`
    values in the form of:
    
    ```
    probe_id,/some/absolute/path/to/numpy_0.npy
    ...
    ```
    
    In the event that an `interpreter.probe` instruction is executed more
    than once (i.e. due to being within a loop), there could be `N` entries
    with the same `probe_id` value.
    
    This current schema however does not serialize the `mlir::TensorType` of
    the data. The inclusion of this type information can make it easier for
    post processing tools/scripts to better interpret the metadata file,
    without needing to load the accompanying `npy` file into memory to
    extract size/type information. With these changes, the serialized
    metadata file format now becomes:
    
    ```
    probe_id,tensor<T>,/some/absolute/path/to/numpy_0.npy
    ...
    ```
    
    Where `tensor<T>` is the type string produced by `mlir::debugString`
    (i.e. `tensor<1x2xf32>`, etc). Additionally, the `expect_serialized_eq`
    check dialect operation can now perform a stricter check, by also
    locking down the serialized type vs the expected type.

[33mcommit 021e197590248825a8c11f320bb9b48336834608[m
Author: luis <lpenagos@google.com>
Date:   Thu Nov 30 23:11:43 2023

    Allow unsafe probe_id values to be serialized properly to disk (#1840)
    
    This PR builds off of the recently submitted changes in #1784. Prior to
    this change, the value of `probe_id` and an internal execution counter
    is used to derive the filename for serializing probed tensor values
    (i.e. `<probe_output_dir>/<probe_id>_<execution_count>.npy`). This
    current design has the following disadvantage:
    
    1. This can be unsafe as no filename sanitation is performed, relying on
    the compiler (or other tooling) to produce friendly `probe_id` values.
    Instead of having this implicit assumption on the instrumented StableHLO
    program, this change uses internally generated serialization filenames
    by using a strictly increasing counter.
    
    This change proposes that the serialization filename now be derived as
    follows:
    ```
    <probe_output_dir>/probe<i>.npy
    ```
    
    Where `i` is a uniquely increasing positive `int64_t`.

[33mcommit f2257d22bd8c7af4139cd0fe93593340cd6b5be5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Nov 30 23:10:45 2023

    Clarifying the semantics of  `baseline_type` meta function (#1865)
    
    The PR intends to add more clarification to the existing definition of
    `baseline_type` meta function.

[33mcommit dea89ec13b518da568154c4cdb0cf79fc1803217[m
Author: Chase Riley Roberts <chaser@nvidia.com>
Date:   Thu Nov 30 02:55:38 2023

    Integration of collective_broadcast into spec (#1856)
    
    This is the first PR for RFC #1809. I did not add an interpreter
    implementation as @GleasonK specifically asked me to leave that for new
    staff joining his team.

[33mcommit df1262c6196053033a50ef593a39b22d5112b18d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 29 20:08:05 2023

    Fix 0.15.0 compatibility test to use proper run commands (#1863)
    
    Discovered in
    https://github.com/openxla/stablehlo/pull/1856#discussion_r1406841979

[33mcommit 83f095e7217c897f1eccac5652600ceb944cb0e0[m[33m ([m[1;33mtag: [m[1;33mv0.15.5[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 28 22:37:05 2023

    Integrate LLVM at llvm/llvm-project@5e5a22caf88a (#1861)

[33mcommit 23116b3c6aa0754e12c6f155f012dba2d1384f70[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 28 22:36:50 2023

    Fix typo in all_reduce constraint comment (#1860)

[33mcommit bfc711d86f653afb5ade6bf8855e1f6ca81b7e1a[m
Author: Andrew Cain <andrew@cain.id.au>
Date:   Wed Nov 22 23:46:37 2023

    TableGen Headers Included Before They Are Generated (#1849)
    
    ## TableGen Headers Included Before They Are Generated
    
    **Issue:**
    
    The issue is due to **`StablehloLegalizeToTosa.cpp`** attempting to
    include a TableGen'd file (**`StablehloEnums.h.inc`**) that hasn't been
    generated yet, resulting in the following build failure:
    
    ```bash
    In file included from /stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp:25:
    /stablehlo/stablehlo/dialect/StablehloOps.h:45:10: fatal error: 'stablehlo/dialect/StablehloEnums.h.inc' file not found
    #include "stablehlo/dialect/StablehloEnums.h.inc"
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    1 error generated.
    ```
    
    **Reproduce:**
    
    To reproduce this issue on Linux or Mac, follow these steps:
    
    ```bash
    export LLVM_PROJECT_BUILD="/path/to/llvm-project/build"
    git clone https://github.com/openxla/stablehlo.git
    python3 -m venv stablehlo_venv
    source stablehlo_venv/bin/activate
    pip install ninja cmake
    cd stablehlo
    cmake -B build -G "Ninja" \
            -DCMAKE_C_COMPILER=/usr/bin/clang \
            -DCMAKE_CXX_COMPILER=/usr/bin/clang++ \
            -DMLIR_DIR=${LLVM_PROJECT_BUILD}/lib/cmake/mlir \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLVM_ENABLE_ASSERTIONS=ON
    cmake --build build -j2
    ```
    
    **Resolution**:
    
    To resolve this issue, explicitly define the dependency between
    `StablehloLegalizeToTosa.cpp` and `StablehloEnums.h.inc`. Otherwise it
    is the luck of the draw which one will compile first.
    
    This fix defines the StableHLO TableGen'd headers as a dependency to for
    the for the MLIR library that includes `StablehloLegalizeToTosa.cpp` .
    This ensures that the stablehlo header files are always generated before
    the library is compiled.

[33mcommit 4df877d3bbc274793d8ed7994a938962fd441447[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 22 23:13:12 2023

    Bump patch version after integrate 0.15.4 -> 0.15.5 (#1858)

[33mcommit e3276cd896751bfbebd7b18112f81547fbc2bc9c[m[33m ([m[1;33mtag: [m[1;33mv0.15.4[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 22 17:55:42 2023

    Integrate LLVM at llvm/llvm-project@2b0b0ad7e760 (#1857)

[33mcommit 234b79341a7d0b6d2fdbaeb1d0118a221f99bc48[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Nov 21 20:10:47 2023

    Fix clang-tidy error (#1855)
    
    The following error is surfaced during the StableHLO integration.
    
    ```
    function template specialization 'mlir::stablehlo::impl::mapStablehloOpToStdScalarOp<mlir::stablehlo::ClampOp>' has a primary template declaration with different parameter names
    ```

[33mcommit 8a7bea7e5b589502ad419d77c9f5e824532f7d98[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Nov 21 19:57:27 2023

    Bump patch version after integrate 0.15.3 -> 0.15.4 (#1854)

[33mcommit 2e78612f34a7e43cb94169e446204913bf353457[m[33m ([m[1;33mtag: [m[1;33mv0.15.3[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Nov 20 23:21:08 2023

    Bump patch version after integrate 0.15.2 -> 0.15.3 (#1853)

[33mcommit 95fb1d4131314202cce36386d092c409663ce214[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 20 21:11:00 2023

    Fix markdown lint issues (#1852)

[33mcommit 76e25a5f5f07c15982f45df82e4c9b32fa2b9d85[m
Author: Chase Riley Roberts <chaserileyroberts@gmail.com>
Date:   Mon Nov 20 19:18:34 2023

    [RFC] Add collective_broadcast to the StableHLO specification (#1809)
    
    This RFC proposes adding `collective_broadcast` as one of the collective
    communication primitives.
    Please provide any feedback you feel is valuable.

[33mcommit 458b691ea89a862a9d72529583fc922a8f39439b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 20 19:14:50 2023

    Prevent OOB indexing in StableHLO/MHLO ops. (#1846)
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/ff93081bde146359869caff42bf6346383f9eb8b

[33mcommit 202cb2b1ad92a897610056945400d1757c30f1f0[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 20 17:52:24 2023

    Integrate LLVM at llvm/llvm-project@506c47df00bb (#1850)
    
    Update sparsity printer test to use aliases, from latest LLVM bump

[33mcommit 47d62a34d166aa8e4acf1121df5bef88819fd813[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Nov 16 18:38:35 2023

    Rename StableHlo -> Stablehlo in methods (#1847)

[33mcommit 444c72d6de4e2b3c7738457708ea863d5fb7f0e4[m[33m ([m[1;33mtag: [m[1;33mv0.15.2[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Nov 16 01:00:54 2023

    Integrate LLVM at llvm/llvm-project@c5dd1bbcc37e (#1845)

[33mcommit 954905121b6cd44db8d12cae1cdb7299697a34ce[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Nov 16 00:44:15 2023

    Update license to be consistent with StableHLO project (#1844)
    
    Updating license per discussion in
    https://github.com/openxla/stablehlo/pull/1817#issuecomment-1813203843
    cc @ashay

[33mcommit 4a628d93545f2ee3e4a9a8e31d7b48fd0ea6bad0[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Thu Nov 16 00:18:26 2023

    linalg: copy subset of stablehlo-to-linalg passes from IREE (#1817)
    
    Jakub Kuderski did the wonderful work of migrating the MHLO-to-Linalg
    conversion passes into IREE so that they translate from StableHLO to
    Linalg, but these passes aren't available to non-IREE users of
    StableHLO.  This patch copies Jakub's passes, with some small changes,
    to enable a `stablehlo-legalize-to-linalg` pass in StableHLO.
    
    This patch does not copy the IREE passes for lowering StableHLO's
    control flow operations (`while`, `if`, and `case`) and the ones for
    lowering shape computations, since I'm not sure whether lowering them
    all in one pass (as opposed to separate passes) would impact IREE's
    flow.  Whichever way we decide to add them, copying these remaining
    translations should be trivial.

[33mcommit e978eea1112295081d0bbb12e816f64908275df9[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Nov 14 22:11:57 2023

    Add dependency on LLVM's not tool (#1829)
    
    Was working in a corrupt build area and got a "command `not` not found"
    error. CMake somehow works without it normally, but still better to add
    it for correctness.
    
    References:
    
    - [StableHLO test using
    `not`](https://github.com/openxla/stablehlo/blob/654c202babbab3f4bd50cdd276de02dc17ecc358/stablehlo/tests/vhlo_to_version_invalid.mlir#L4)
    - [StableHLO bazel dependency on
    `not`](https://github.com/openxla/stablehlo/blob/654c202babbab3f4bd50cdd276de02dc17ecc358/stablehlo/tests/BUILD.bazel#L150)
    - [MHLO
    example](https://github.com/openxla/xla/blob/fb67091154574430a8fd17b7eae7ceadd79f7139/xla/mlir_hlo/tests/CMakeLists.txt#L24)
    using not dependency in cmake

[33mcommit e5ad51715a11721c78b6748ab5de7945df24b1b8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 10 22:20:57 2023

    Bump patch version after integrate 0.15.1 -> 0.15.2 (#1838)

[33mcommit ea413721ed86db6514f748e6e1756d17b5c25413[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 10 20:57:08 2023

    Minor code style updates (#1834)
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/e2805725a7ec22b4393848bb1083ea663313c141.

[33mcommit cc0adef2406abbbccac8e42cc7c265bbff8295ad[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 10 20:56:55 2023

    Fix issues found during the recent downstream integrate (#1839)
    
    * Add a comment next to #endif in Base.h.
      * Sort dependencies alphabetically in BUILD.bazel.

[33mcommit c3e1ab5af5f4f3ab749072ffa960348f90d5e832[m[33m ([m[1;33mtag: [m[1;33mv0.15.1[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 10 17:53:52 2023

    Integrate LLVM at llvm/llvm-project@75d6795e4202 (#1837)

[33mcommit 2149a931d96ed267c25a3469e680e195be2d5124[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Nov 9 20:07:21 2023

    Bump patch version after integrate 0.15.0 -> 0.15.1 (#1836)

[33mcommit d4bcd97a2f8671aab969c9aa425365d013bd6a85[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Nov 9 19:44:25 2023

    Shorten some file and target names under reference/ (#1835)
    
    This PR shortens `reference/InterpreterFoo.{cpp,h}` to just
    `reference/Foo.{cpp,h}`, and applies a similar transformation to target
    names in both CMake and Bazel builds.
    
    The only name that's left untouched is `reference/InterpreterOps`
    because shortening that would conflict with the already existing
    `reference/Ops`.
    
    There are some C++ names that could potentially be shortened as well,
    e.g. InterpreterFallback and InterpreterConfiguration, but C++ names are
    out of scope of this PR.

[33mcommit a0a10af7f87e0c2b34e81538082af5c0ed00a15e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Nov 9 18:56:01 2023

    Fix issues found during the recent downstream integrate (#1833)
    
    * Add missing dependency to reference_interpreterconfiguration.
    * Update `/*config=*/` comments to `/*fallback=*/` following the recent
    rename.

[33mcommit 897ba37f4eb413477ab33ef6bcd4a71e5ffd19e2[m[33m ([m[1;33mtag: [m[1;33mv0.15.0[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 8 19:43:56 2023

    Integrate LLVM at llvm/llvm-project@f9404a1b57a5 (#1832)

[33mcommit e7c7b3afb9855ead26abeeb2d016f47269eec8e7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 8 15:55:53 2023

    Migrate InterpreterDialect and CheckDialect to use properties (#1830)
    
    [MLIR
    Properties](https://mlir.llvm.org/OpenMeetings/2023-02-09-Properties.pdf)
    are the new standard for non-discardable attributes. In the near future,
    all dialects must migrate to use properties by default. This PR migrates
    InterpreterDialect and CheckDialect, which turned out to be trivial.
    
    VHLO changes are approved, the changes for StableHLO and CHLO will be
    slightly more involved.
    
    Part of #1584.

[33mcommit dec03218e3eafa75ec0275adb3809d689b6cd5a2[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 8 15:55:32 2023

    Sort dependencies of the stablehlo_ci_tests target (#1826)
    
    Previously, these dependencies were not sorted which triggered a lint
    check during a downstream integrate. Looks like a useful thing to fix.

[33mcommit 903e3483ab76c97c274601751bcdd271c503d37c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 8 15:54:57 2023

    Migrate VHLO to use properties (#1827)
    
    [MLIR
    Properties](https://mlir.llvm.org/OpenMeetings/2023-02-09-Properties.pdf)
    are the new standard for non-discardable attributes. In the near future,
    all dialects must migrate to use properties by default. This PR migrates
    VHLO first, which is an important step since we need to ensure:
    1. Backward compatibility: VHLO with properties can read older portable
    artifacts
    2. Forward compatibility: VHLO with properties can generate portable
    artifacts that is readable in previous versions
    
    (1) Had no issues, after migrating VHLO to properties, only the
    FileCheck lines needed for the following:
    - Attrs print using `{...}` syntax and properties print using `<{...}>.
    - Attrs print after regions, properties print before regions
    
    Note no changes to `stablehlo_legalize_to_vhlo.0_9_0.mlir.bc` for
    example, meaning we can still read old bytecode files and it
    deserializes into properties successfully.
    
    (2) Worked fine for all versions of StableHLO **except for 0.14.0**.
    Note again that there were no changes to
    `stablehlo_legalize_to_vhlo.0_9_0.mlir.bc`, which means that bytecode
    files generated at HEAD with properties and targeting 0.9.0 generated
    _identical_ bytecode files. However, [MLIR Bytecode format
    v5+](https://github.com/llvm/llvm-project/blob/2e6f722b88e42bba72af5bafd8cc1e37e460d978/mlir/include/mlir/Bytecode/Encoding.h#L48)
    generates different bytecode for dialects that use properties, and
    property definitions must be present for deserialization of these. This
    means back-deployment from `HEAD->0.14.0` breaks, since 0.14.X
    (pre-properties) doesn't have the property struct definitions needed for
    deserialization.
    
    Given that backward compatibility is fully supported, (2) is resolved by
    changing 0.14.0 to target bytecode v4, which serializes attr-dict (can
    be deserialized by pre/post-property VHLO), and in addition bumping the
    StableHLO version to 0.15.0 which uses the latest bytecode format
    version v6. This leads to full forward/backward compatibility with no
    temporary breakages.
    
    Part of #1584

[33mcommit 654c202babbab3f4bd50cdd276de02dc17ecc358[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Nov 6 21:08:34 2023

    Print optional attr-dict on stablehlo.reduce (#1825)
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/b482677b8f00a9bd1a59f315d2176d5ee9b17b0b.

[33mcommit 66cd606f4c8188461834b086a5a54a8b46fcf085[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Nov 6 21:08:04 2023

    Fix an accidental modification to TOSA tests (#1824)
    
    In #1822, I accidentally removed a few CHECK lines from a TOSA
    conversion test. This PR restores them.

[33mcommit 95997b2a398abc111100a637491efbc131de6022[m
Author: luis <lpenagos@google.com>
Date:   Mon Nov 6 18:02:39 2023

    Introduce StableHLO interpreter API (#1797)
    
    Today, it isn't practical to call the StableHLO reference interpreter.
    Invoking the interpreter requires going through the MLIR translate
    infrastructure (i.e. via `interpretRegistration` in
    `StablehloTranslateMain.cpp`), and does not currently support specifying
    user inputs or doing any meaningful post processing on the model outputs
    short of printing them to screen. This makes it difficult to build
    tooling which uses the reference interpreter.
    
    This PR proposes a new reference interpreter API which facilitates the
    running the StableHLO reference interpreter through a simple, unified
    interface. At a high level, the proposed interface allows clients to
    specify a set of inputs (in `InterpreterValue` format), and an
    `InterpreterConfiguration` object (see below). The proposed interface
    invokes the reference interpreter and returns a set of outputs (in
    `InterpreterValue` format). The new interface would live in
    `stablehlo/reference/InterpreterApi.h` and would expose the following
    interface:
    
    ```
    /// Invoke the StableHLO reference interpreter with the given parsed MLIR
    /// module input and provided inputs. Returns a list of interpreter outputs.
    /// Can optionally pass a fallback interpreter callback which executes when no
    /// builtin kernels are matched.
    llvm::ErrorOr<SmallVector<InterpreterValue>> evalModule(
        ModuleOp module, ArrayRef<InterpreterValue> inputs,
        const InterpreterConfiguration &config);
    ```
    
    
    ### Interpreter configuration
    As part of this PR, there is an `InterpreterConfiguration` struct which
    can be used to configure the StableHLO interpreter. Encapsulating
    configuration state into a singular object allows the interface to
    remain stable as new configuration options are added. Initially, the
    configuration object will contain a handful of options:
    
    ```
    struct InterpreterConfiguration {
      InterpreterConfiguration()
          : fallback(std::make_unique<InterpreterFallback>()) {}
    
      /// If specified, the directory to which StableHLO interpreter tensors will
      /// be serialized to disk.
      std::string probeInstrumentationDir = "";
    
      /// Use the specified named function as the main entrypoint into a module.
      /// Defaults to `main` for modules with multiple functions. If a module only
      /// contains 1 function and the default `main` value is used, the singular
      /// function will be used as the entrypoint (irrespective of a function name
      /// match).
      std::string mainFunction = "main";
    
      /// If specified, use the callback to run on ops which do not have a
      /// registered kernel.
      std::unique_ptr<InterpreterFallback> fallback;
    };
    ```
    
    Note that all of these configuration values are optional, and it is
    possible to run the interpreter with a default initialized
    `InterpreterConfiguration` struct (which will by default, not provide a
    fallback callback).
    
    ### Interpreter fallback callback
    When defining an interpreter fallback callback, the
    `InterpreterFallback` struct can be extended:
    
    ```
    class InterpreterFallback {
     public:
      /// Custom op kernels for any user specified ops not found in the StableHLO
      /// op dialect or StableHLO interpreter dialect.
      virtual llvm::Error operator()(Operation &op, Scope &scope, Process *process);
    
      virtual ~InterpreterFallback() = default;
    };
    ```
    Note that the user specified fallback need not contain any
    `InterpreterOp` kernels, these will always run before the user specified
    callback.
    
    Currently, the proposed change exposes `InterpreterConfiguration` and
    the current `mlir::func::FuncOp` being evaluated to the fallback
    interface to support the existing flow. Consequently, there‚Äôs a bit of
    gymnastics in `stablehlo::runInterpreter` which I think could be
    improved on. I'm open to ideas on how we could better handle this.

[33mcommit 7bb2c7af5d73e68451e0afc54cd64a326947b392[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Nov 4 02:24:40 2023

    Bump patch version after integrate 0.14.29 -> 0.14.30 (#1823)

[33mcommit 04291aea6b50d9573e6f4de184938d83b9564cd0[m[33m ([m[1;33mtag: [m[1;33mv0.14.29[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 3 18:36:09 2023

    Integrate LLVM at llvm/llvm-project@46732e2abb34 (#1822)

[33mcommit eec1a58ca7cced2ed890c701094766dc6758adaa[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 3 18:34:30 2023

    Bump patch version after integrate 0.14.28 -> 0.14.29 (#1821)

[33mcommit 5e41e674af78da676652459c2dcf6a0d76e59ddb[m[33m ([m[1;33mtag: [m[1;33mv0.14.28[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 1 20:32:09 2023

    Add Documentation on Reference Interpreter (#1803)
    
    This PR adds documentation to the StableHLO interpreter's current status
    and provides a way to convert StableHLO ops without a direct
    implementation to ones that do via running a set of expander/transform
    passes within the [openxla/xla](https://github.com/openxla/xla) repo.
    
    In summary, this PR closes most of the remaining specced ops without an
    interpreter with a few exceptions noted in the doc: `FftOp`, `RngOp`,
    `RngBitGeneratorOp`, `UniformDequantizeOp`, and `UniformQuantizeOp`. It
    also documents interpretation of ops categorized as "Not In HLO" which
    currently does not have a formal spec in our StableHLO
    [spec.md](https://github.com/openxla/stablehlo/blob/main/docs/spec.md).
    
    This PR resolves most of the items in #1571 except ones mentioned above
    and those marked N/A in the comment (rationale for these are provided in
    the doc).
    
    Miscellaneous:
    closes #1121 batch_norm_grad
    closes #963 batch_norm_inference
    closes #1122 batch_norm_training
    closes #1123 cholesky
    closes #1138 triangular_solve
    
    Not In HLO:
    closes #1799 broadcast
    closes #1800 create_token
    closes #1801 cross-replica-sum
    closes #1225 dot
    closes #1226 einsum
    closes #1227 torch_index_select
    closes #1802 unary_einsum

[33mcommit c4ae40e15e0daf17773627dfd5b60fc196cf7e01[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 1 19:10:01 2023

    Integrate LLVM at llvm/llvm-project@6ae7b735dbd5 (#1819)

[33mcommit 16fb88d0ca1dd69125adfdbf0556a9a7c96a1b5b[m
Author: Sambhav Jain <sambhav.eee@gmail.com>
Date:   Tue Oct 31 15:28:54 2023

    Build changes to allow cleaner downstream integrations (#1810)
    
    Integrating `openxla/stablehlo` into our (Cruise's) downstream monorepo
    requires maintaining a patch file (like
    [this](https://gist.github.com/sjain-stanford/cbb2ffdc8d702abbb0dc62096895fdd8))
    which fixes references to `//` with `@stablehlo//` among other things.
    This PR explicitly defines a `stablehlo` workspace and uses that to
    populate the lit config replacements, thereby completely eliminating the
    need for a downstream patch that needs to constantly be updated during
    upgrades. We also add test_suites which allow better downstream CI
    integrations which can be run as follows:
    
    ```
    bazel test //:all_tests
    ```

[33mcommit 89ea02e4e67e4ac4199c6461ce9fc217450fe081[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Wed Oct 25 16:22:16 2023

    fix vhlo DictionaryAttr's printer with separated comma (#1813)

[33mcommit 43e51a7c2380b683c8437ca56a77918dbba0772a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Oct 20 18:21:25 2023

    Bump patch version after integrate 0.14.27 -> 0.14.28 (#1816)

[33mcommit a621c6df2b9129f976c8226bd547d4d42880d427[m[33m ([m[1;33mtag: [m[1;33mv0.14.27[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Oct 19 17:00:11 2023

    Integrate LLVM at llvm/llvm-project@c122b9727a27 (#1814)

[33mcommit e54f6c4be1654453646ebb2829a6f049b7587b76[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 18 17:07:27 2023

    Bump patch version after integrate 0.14.26 -> 0.14.27 (#1811)

[33mcommit 03216ba4f6ead279db5912828f8c94634589007d[m[33m ([m[1;33mtag: [m[1;33mv0.14.26[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Oct 17 00:17:49 2023

    Bump patch version after integrate 0.14.25 -> 0.14.26 (#1808)

[33mcommit 5a8bb985f50a679721292b14f97f270344ac64a3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 16 21:06:14 2023

    Integrate LLVM at llvm/llvm-project@97217d188469 (#1807)

[33mcommit f7f4d4e705c9399b88f1adc6e02116eb8c855a5e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 16 18:56:45 2023

    Fix clang tidy, remove unused dependencies (#1806)
    
    cc: @penagos

[33mcommit 801bb9c6a9f7499f3a6cacfb3de4c994897a5e03[m[33m ([m[1;33mtag: [m[1;33mv0.14.25[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 13 18:57:21 2023

    Integrate LLVM at llvm/llvm-project@b15b84610f63 (#1805)

[33mcommit 89fcab148eacce60f2d63cadab55b1b0e5574325[m
Author: luis <luis@penagos.co>
Date:   Tue Oct 10 19:26:12 2023

    Add reference interpreter probe instrumentation (#1784)
    
    Reopening #1763 due to a branch corruption issue. Original text below:
    
    ===
    
    As the number of ML frameworks continues to rapidly grow, it‚Äôs becoming
    increasingly desirable to be able to compare numerical results across
    different runtimes. This ensures that ML models run equivalently in
    different environments, and overall helps to instill confidence in ML
    frameworks. For example, given the recency of the StableHLO
    representation, it may be desirable to have a way to compare StableHLO
    interpreter results to other frameworks, such as Tensorflow, or other
    implementations
    
    This PR proposes the addition of a probe instruction to the new
    interpreter dialect in StableHLO. The probe instruction will enable us
    to extract intermediate tensor state from the StableHLO interpreter.
    Since this new op is intended to initially support debuggability, it
    will not be added to the StableHLO opset itself, but instead to the new
    [interpreter dialect](https://github.com/openxla/stablehlo/pull/1703).
    
    The probe instruction will take precisely 1 shaped tensor input, and
    produce the same shaped tensor output, without modifying it:
    
    ```
    %1 = "interpreter.probe"(%0) {probe_id="probe0"} : (tensor<*xf32) -> tensor<*xf32>
    ```
    
    The op will be modeled as having side effects, to prevent it from being
    moved or removed by the compiler. It will additionally contain a
    singular, unique string attribute, `probe_id`, which will allow us to
    later pair this tensor‚Äôs data with other runtimes. At compile time, the
    probe instruction may be inserted after every StableHLO operation
    (except constants), allowing runtime data to be serialized to disk.
    
    This PR also adds a new StableHLO interpreter configuration flag,
    `--probe_readout_dir`, which will be used to specify the directory in
    which to store all probed interpreter values. When executed, the probe
    instruction will associate the contents of its input tensor with the
    given probe_id, and serialize the tensor data to disk using the portable
    [NumPy](https://numpy.org/) format. The filename used will be in the
    form of `probe_readout_dir/probe_id.npy`. As the interpreter executes
    `interpreter.probe` operations, in addition to serializing tensor data
    to NumPy files, it will also append serialized filepaths to a metadata
    `index.csv` file in the form of:
    
    ```
    probe_id0,/probe_readout_dir/probe_id0.npy
    probe_id1,/probe_readout_dir/probe_id1.npy
    ‚Ä¶
    probe_id0,/probe_readout_dir/probe_id0_1.npy
    ```
    
    The metadata file can then be used by post processing tools to analyze
    the instrumented tensor data. Note that a `probe_id` can appear more
    than once in the metadata file (i.e. if a probe is executed within a
    loop).
    
    ### Validation
    
    To validate the new instruction, we propose extending the existing
    `stablehlo::check` MLIR dialect with a `expect_serialized_eq` operation,
    which can be utilized in the following way:
    
    ```
    // Where `T` is a statically shaped tensor
    %0 = ‚Ä¶
    %1 = interpreter.probe %0 { probe_id = "probe0" } : (tensor<T>) -> tensor<T>
    check.expect_serialized_eq %3 { probe_id = "probe0", iteration = 0 : i32 } : tensor<T>
    ```
    
    The `check.expect_serialized_eq` is expected to appear after the
    producing `interpreter.probe` instruction with the same `probe_id`. When
    executed, the instruction will search for an existing `index.csv`
    metadata file in the same directory as specified by the
    `--probe_readout_dir` flag, and then attempt to find the serialized
    filepath for the given `probe_id` and iteration. After which, the
    interpreter will load the serialized tensor from the NumPy file, and
    then proceed to compare it with its operand tensor. The operation will
    assert on an answer mismatch or failure to find/parse the associated
    NumPy file. The iteration attribute is 0-based and is optional and will
    default to 0.

[33mcommit e1e306adf3c75b676ac7c24afb98953061c11cea[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Oct 10 18:29:50 2023

    Bump patch version after integrate 0.14.24 -> 0.14.25 (#1804)

[33mcommit 78f57e34a25367ef3192cd35da36b01c763f4ecf[m[33m ([m[1;33mtag: [m[1;33mv0.14.24[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 9 22:48:13 2023

    Integrate remainder of llvm/llvm-project@2a1f1b5fde0a (#1798)

[33mcommit 347837218b7007a977014c2a856ae10ccc7d78af[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 9 21:29:41 2023

    Integrate LLVM at llvm/llvm-project@2a1f1b5fde0a (#1795)

[33mcommit f084a1a2d803c4155b44816d935140529a2e09f1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Oct 9 20:18:57 2023

    [RFC] Specification of quantized reduction operation (#1664)
    
    From the discussion emerged in
    https://github.com/openxla/stablehlo/pull/1538 about the specification
    of quantized `reduce` op, we zeroed in to a proposal which need some
    structural changes in the op (adding additional blocks). This proposed
    RFC is to further explore the proposal in detail.
    
    Please let me know your feedback. See also the [OpenXLA Discuss post](https://groups.google.com/a/openxla.org/g/openxla-discuss/c/iwE9is49SS4).

[33mcommit 6d662d00513aa2c9e74c930908a12d13b6ea78d0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Oct 5 18:26:37 2023

    Bump patch version after integrate 0.14.23 -> 0.14.24 (#1793)

[33mcommit 46a2506405a0c1ade5327b9e476a32781221b075[m[33m ([m[1;33mtag: [m[1;33mv0.14.23[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Oct 5 01:11:33 2023

    Integrate LLVM at llvm/llvm-project@173fd67a124d (#1791)

[33mcommit 0456a5fa3004915808dba94e014e46d7e608c5fa[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 3 22:10:45 2023

    Fixing order of tests introduced in #1782 (#1788)
    
    Corresponding MHLO
    change:https://github.com/tensorflow/mlir-hlo/commit/368ad52820900f56b606bf0fbc2758f36dcffa08

[33mcommit 9f4f5d556bc7e173eed2b31fe05e372bfd7f1cba[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Oct 3 22:07:56 2023

    Bump patch version after integrate 0.14.22 -> 0.14.23 (#1790)

[33mcommit fe5fa2690e33241b51cefdb27a1fff0fd0626047[m[33m ([m[1;33mtag: [m[1;33mv0.14.22[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Oct 3 18:44:04 2023

    Integrate LLVM at llvm/llvm-project@3661a48a8473 (#1789)

[33mcommit a544533bdc02876f8abdaa7d1f52d64dd047e80d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 3 00:00:17 2023

    Bump patch version after integrate 0.14.21 -> 0.14.22 (#1787)

[33mcommit 6bf3f5d7b8dd3fc67ee1236432606d9c414ffcbd[m[33m ([m[1;33mtag: [m[1;33mv0.14.21[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 29 20:36:00 2023

    Integrate LLVM at llvm/llvm-project@512739ebbb25 (#1786)

[33mcommit 82309e568caeef1c78a05383633963b8e6219a99[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 29 17:53:11 2023

    Bump patch version after integrate 0.14.20 -> 0.14.21 (#1785)

[33mcommit a8f074bda6462e515597abb495bc39d1a2684d0d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Sep 29 17:15:57 2023

    Add interpreter for SendOp and RecvOp (#1732)
    
    SendOp:
    We have the following constraints in the spec:
    
    ```
    (I1) `inputs` is a variadic number of tensors.
    (I2) `token` is a `token`.
    (I3) `channel_id` is a constant of type `si64`.
    (I4) `channel_type` is a enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST`.
    (I5) `is_host_transfer` is a constant of type `i1`.
    (C1) `channel_type` is defined as:
    * `DEVICE_TO_HOST` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `inputs` is not a variadic number of tensors. (Covered by ODS).
    I2: a) `token` is not a `token`. (Covered by ODS).
    I3: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    I4: a) `channel_type` is not a enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST`.
    I5: a) `is_host_transfer` is not a constant of type `i1`. (Covered by ODS).
    C1: a) `channel_type` is not defined as:
    * `DEVICE_TO_HOST` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I4: a) `channel_type` is not a enum of `DEVICE_TO_DEVICE` and `DEVICE_TO_HOST`.
    C1: a) `channel_type` is not defined as:
    * `DEVICE_TO_HOST` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    ```
    
    closes #1137
    
    RecvOp:
    We have the following constraints in the spec:
    
    ```
    (I1) `token` is a `token`.
    (I2) `channel_id` is a constant of type `si64`.
    (I3) `channel_type` is a enum of `DEVICE_TO_DEVICE` and `HOST_TO_DEVICE`.
    (I4) `is_host_transfer` is a constant of type `i1`.
    (C1) `channel_type` is defined as:
    * `HOST_TO_DEVICE` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    (C2) `0 < size(results)`.
    (C3) `is_empty(result[:-1])` or `is_tensor(type(results[:-1]))`.
    (C4) `is_token(type(results[-1]))`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `token` is not a `token`. (Covered by ODS).
    I2: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    I3: a) `channel_type` is not a enum of `DEVICE_TO_DEVICE` and `HOST_TO_DEVICE`.
    I4: a) `is_host_transfer` is not a constant of type `i1`. (Covered by ODS).
    C1: a) `channel_type` is not defined as:
    * `HOST_TO_DEVICE` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    C2: a) `size(results) <= 0`.
    C3: a) `is_empty(result[:-1]) = false` and `is_tensor(type(results[:-1])) = false`.
    C4: a) `is_token(type(results[-1])) = false`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I3: a) `channel_type` is not a enum of `DEVICE_TO_DEVICE` and `HOST_TO_DEVICE`.
    C1: a) `channel_type` is not defined as:
    * `HOST_TO_DEVICE` if `is_host_transfer = true`,
    * `DEVICE_TO_DEVICE` otherwise.
    C2: a) `size(results) <= 0`.
    C3: a) `is_empty(result[:-1]) = false` and `is_tensor(type(results[:-1])) = false`.
    C4: a) `is_token(type(results[-1])) = false`.
    ```
    
    Notes:
    * Also fixes bugs in the compatibility tests. The changes made here is
    an exception to the regular process (See
    [compatibility.md](https://github.com/openxla/stablehlo/blob/main/docs/compatibility.md#out-of-scope))
    since the verification was not in sync with the spec. Other than the bug
    fixes, the remaining changes were autogenerated using the script in
    [compatibility.md](https://github.com/openxla/stablehlo/blob/main/docs/compatibility.md).
    
    closes #1132

[33mcommit 77a707b54b7a7a03826dd6a4d58aefefb9c44f9d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 28 15:17:28 2023

    Fixing verifier for reshape, transpose, and broadcast_in_dim ops (#1782)
    
    fixes https://github.com/openxla/stablehlo/issues/1765
    
    The existing verifier for these ops was using the trait
    [SameOperandsAndResultElementType](https://github.com/openxla/stablehlo/blob/b2090e9774f68c81b65615d18872b99b718eedad/stablehlo/dialect/StablehloOps.td#L2763)
    to check the equality of the element types which will not work for the
    per-axis quantized variants of these ops as the quantization dimension
    and quantization parameters (scales and zero points) may differ between
    operand and result.
    
    The PR proposed to replace the trait with
    [HLO_CompatibleOperandsAndResultElementType
    ](https://source.corp.google.com/piper///depot/google3/third_party/stablehlo/stablehlo/dialect/Base.td;rcl=567468454;l=216)(which
    just checks the equality for storage type and expressed type).
    
    Note that the PR does not provide a comprehensive solution here. It just
    allows the representational capability to these ops thereby allowing
    writing meaningful quantized versions of the ops w/o verification error.
    A comprehensive verification solution should include verifying all the
    constraints related to quantization for these ops and is true for other
    quantized ops as well. That would need additional work and will be
    addressed collectively for the full set of quantized ops.

[33mcommit 680d44d1be19a74d3479c2906b3394389ccb1dec[m
Author: Aart Bik <39774503+aartbik@users.noreply.github.com>
Date:   Wed Sep 27 14:42:30 2023

    Minor edits to the sparsity RFC (#1783)
    
    A few edits to make the proposal more consistent with the updated MLIR
    doc

[33mcommit 587025f8c564aa91da4c97da80a8655b219bfc08[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Sep 26 23:47:13 2023

    Add interpreter for InfeedOp (#1722)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `token` is a `token`.
    (I2) `infeed_config` is a constant of type `string`.
    (C1) `0 < size(results)`.
    (C2) `is_empty(result[:-1])` or `is_tensor(type(results[:-1]))`.
    (C3) `is_token(type(results[-1]))`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `token` is not a `token`. (Covered by ODS).
    I2: a) `infeed_config` is not a constant of type `string`. (Covered by ODS).
    C1: a) `size(results) <= 0`.
    C2: a) `is_empty(result[:-1]) = false` and `is_tensor(type(results[:-1])) = false`.
    C3: a) `type(results[-1]) != token`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1: a) `size(results) <= 0`.
    C2: a) `is_empty(result[:-1]) = false` and `is_tensor(type(results[:-1])) = false`.
    C3: a) `type(results[-1]) != token`.
    ```
    
    closes #1128

[33mcommit 2eec6dbd23dc5bd9d51f98c5768edf5b1a470ff7[m[33m ([m[1;33mtag: [m[1;33mv0.14.20[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Sep 25 20:23:40 2023

    Integrate LLVM at llvm/llvm-project@f7bf99fb529f (#1781)

[33mcommit 8fbc1be397c3777e93f6cc1e264077396c8b7f9c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 19 21:04:02 2023

    Bump patch version after integrate 0.14.19 -> 0.14.20 (#1779)

[33mcommit 7a4b80ed8da88fda3f8a7da1f29d45cb5c00929e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 19 21:03:54 2023

    Fix clang tidy, change constness of rendezvous results (#1778)
    
    These results should not be modified by threads once returned, thus the
    pointer can be to const data. Making the shared pointer const doesn't
    add much, just means that the thing being pointed at cant be changed,
    which doesn't guarantee much.

[33mcommit 887e0692ae727555c6b0e00402897742a409de54[m[33m ([m[1;33mtag: [m[1;33mv0.14.19[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 19 17:41:42 2023

    Integrate LLVM at llvm/llvm-project@79e96b2457fe (#1776)

[33mcommit 9044a989819be39008c0e025814173de5a1ea386[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Sep 18 20:48:15 2023

    Fixing the access of scalar elements in `broadcast_in_dim`, `clamp`, and `select` operations (#1777)
    
    Semantics of some operations like `broadcast_in_dim`, `clamp`, and
    `select`, with one or more of the input operands potentially be scalars,
    needs implicit broadcasting. For example, `operand` for
    `broadcast_in_dim`, `min`/`max` for `clamp`, `pred` for `select`.
    
    The current PR fixes the access of those operands, to indicate proper
    deference of the tensor type to element type, in the specification of
    these operations.

[33mcommit 80c09b5660770351c361991abd479fe569edca04[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 15 17:18:13 2023

    Allow the specification of a scalar operand for broadcast_in_dim operation (#1774)
    
    The specification of broadcasting a scalar operand, using
    broadcast_in_dim, is unclear. The PR attempts to update the semantics
    for the operation to accommodate the case.
    
    Also, the update is limited to a spec change as the following program
    
    ```
    func.func @main() -> (tensor<1x750xi32>) {
    ¬†¬†%0 = stablehlo.constant dense<1> : tensor<i32>
    ¬†¬†%1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<1x750xi32>
    ¬†¬†return %1 : tensor<1x750xi32>
    }
    ```
    is currently a valid representation in StableHLO.

[33mcommit d17a137a5d48916f6f58ca3dd18b791b0d99a0c5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Sep 12 17:48:52 2023

    Fix TSAN violations for Distribution ops (#1768)
    
    The TSAN violation is due to an unprotected insert shown above and
    replacing `llvm::RefCountedBase` with its thread-safe alternative
    `llvm::ThreadSafeRefCountedBase`.
    
    In the original implementation of `rendezvous`, there were some nuances
    that were undesirable.
    For example, it's preferred to clear out the entry once all processes
    have read the data and clear out the map instead of letting the next
    distribution op to clear it out. The current implementation is also
    incorrect since there is no guarantee that processGroup size is the same
    as previous op calling `rendezvous`.
    ```
    if (channels_[channelKey].size() == processGroup.size())
      channels_[channelKey].clear();
    
    channels_[channelKey].insert(processId, operand);
    ```
    
    The proposed update enhances the `rendezvous` logic such that each
    process has a shared pointer to the result object once out of scope, it
    is automatically deleted.
    
    The PR also reenables tests for distribution ops.
    
    closes #1755

[33mcommit 371f5145c31a0210d9d400192d9c8d6d579431ae[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 8 23:34:33 2023

    Bump patch version after integrate 0.14.18 -> 0.14.19 (#1771)

[33mcommit 77a59815a82b34f7b08ed2d42a711d9920682d0e[m[33m ([m[1;33mtag: [m[1;33mv0.14.18[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 7 16:30:08 2023

    Integrate LLVM at llvm/llvm-project@4acc3ffbb0af (#1770)

[33mcommit 86f718d41bf86b5a333a03ddf25571d927024f31[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 7 16:30:00 2023

    Bump patch version after integrate 0.14.17 -> 0.14.18 (#1769)

[33mcommit bb47324b40392966fa0024c89bf643724e47194b[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Wed Sep 6 16:45:22 2023

    build: miscellaneous CMake fixes (#1766)
    
    This patch makes two small changes.  First, it adds the `MLIRParser`
    library as a dependency for the `StablehloPortableApi` and the
    `StablehloSerialization` library targets, since both PortableApi.cpp and
    Serialization.cpp include calls to `mlir::parseSourceString()`.
    Although the code _does_ build without this dependency, it seems like an
    accidental result, so this patch makes the dependency explicit.
    
    Second, the Windows shell does not use the same rules as Bash for
    setting environment variables, thus causing the `check-stablehlo-python`
    target to fail.  This patch uses the (cross-platform) CMake syntax for
    setting the `PYTHONPATH` environment variable before running the python
    script.

[33mcommit c4a2b74d44ac38e95664148342206278ef05e2ba[m[33m ([m[1;33mtag: [m[1;33mv0.14.17[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 1 17:22:14 2023

    Integrate LLVM at llvm/llvm-project@f0f395e00e2e (#1767)

[33mcommit 7f8eccb08f9512e500721cb0a043b561b4aa8e0a[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Thu Aug 31 22:04:20 2023

    tests: avoid sub-shells in lit commands for correct use on Windows (#1764)
    
    On Windows, command substitution fails when the `RUN` command uses
    sub-shells, which are often used when the lit test needs to use the
    output of commands, for example, `diff <(cmd1 arg1 arg2) <(cmd2 arg1)`.
    To avoid such incorrect substitution, it seems the canonical way to use
    the output of one command as an input for the subsequent command is to
    save the output of each sub-command into a temporary file before running
    the original (`diff`) command on the temporary files (see clang lit
    tests for examples).
    
    This patch applies this pattern to all tests in the "tests" and
    "testdata" directories. Although this patch is massive, it was generated
    using the following sed command:
    
    ```bash
    sed 's/\/\/ RUN: diff <(\(.*\)) <(\(.*\))/\/\/ RUN: \1 > %t.0\n\/\/ RUN: \2 > %t.1\n\/\/ RUN: diff %t.0 %t.1/' -i stablehlo/tests/*.mlir
    
    sed 's/\/\/ RUN: diff <(\(.*\)) <(\(.*\))/\/\/ RUN: \1 > %t.0\n\/\/ RUN: \2 > %t.1\n\/\/ RUN: diff %t.0 %t.1/' -i stablehlo/testdata/*.mlir
    ```
    
    As a result of this and previous two patches (#1760 and #1761), the
    StableHlo code builds and runs all lit tests successfully on Windows.

[33mcommit 31af9eec5de30b365f4a52cb19e9cc5ef5c07186[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Aug 31 21:38:37 2023

    Per-axis quantization specification for reshape, transpose, and broadcast_in_dim. (#1747)
    
    Based on the discussion in
    https://github.com/openxla/stablehlo/issues/1574, I propose the
    quantization specification for the following data movement ops:
    - **transpose**: The `quantization_dimension` of `result` is also
    transposed based on the input `permutation` order.
    - **reshape**: The `quantization_dimension` of the operand should not be
    merged or split.
    - **broadcast_in_dim**: For [degenerate dimension
    broadcasting](https://www.tensorflow.org/xla/broadcasting#broadcasting_similar-rank_arrays_with_degenerate_dimensions)
    of the operand's quantized dimension, the scales of the result are
    expanded.

[33mcommit d71458474ae523173d5ba37a51ec15bfa78bd9ff[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Thu Aug 31 21:12:23 2023

    build: use 64-bit integer literals across all platforms (#1760)
    
    The `long` type is 32 bits wide on Windows, either 32 or 64 bits wide on
    Linux (depending on x86 or x64), and 64 bits wide on macOS.
    Consequently, when we use bit-shift operators to construct 64-bit
    integers or when we check for splat tensors, using (signed or unsigned)
    `long` integers can produce incorrect values.
    
    This patch replaces signed and unsigned `long` integer literals with
    signed and unsigned `long long` values respectively, so that the
    they are always 64 bits wide on all platforms.  As a minor change, this
    patch uses the lower-case suffixes (`{u}ll` instead of `{U}LL`) for
    consistency.
    
    This patch is in anticipation of downstream users of StableHlo, like
    Torch-MLIR (which use Windows and macOS builds), swapping MLIR-HLO with
    StableHlo.

[33mcommit b29dd360e7751f7d34c8f35a27f911b0d2e1765f[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Thu Aug 31 21:09:54 2023

    tests: drop the use of "grep" from lit tests (#1761)
    
    The "grep" utility doesn't exist on Windows (except on MSYS2 systems),
    making it difficult for Windows users to run a handful of lit tests in
    the stablehlo/tests directory.  As a workaround, this patch replaces the
    use of grep in these tests with FileCheck.

[33mcommit b2090e9774f68c81b65615d18872b99b718eedad[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 30 20:27:06 2023

    Add interpreters for TupleOp and GetTupleElementOp (#1748)
    
    TupleOp:
    We have the following constraints in the spec:
    
    ```
    (I1) `val` is a variadic number of values.
    (C1) `result` has type `tuple<E0, ..., EN-1>` where `Ei = type(val[i])`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `val` is not a variadic number of values. (Covered by ODS).
    C1: a) `result` does not have type `tuple<E0, ..., EN-1>` where `Ei = type(val[i])`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1: a) `result` does not have type `tuple<E0, ..., EN-1>` where `Ei = type(val[i])`.
    ```
    
    closes #1139
    
    GetTupleElementOp:
    We have the following constraints in the spec:
    
    ```
    (I1) `operand` is a tuple.
    (I2) `index` is a constant of type `si32`.
    (C1) `0 <= index < size(operand)`.
    (C2) `type(result) = tuple_element_types(operand)[index]`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tuple. (Covered by ODS).
    I2: a) `index` is not a constant of type `si32`. (Covered by ODS).
    C1: a) `index < 0`.
    C1: b) `index >= size(operand)`.
    C2: a) `type(result) != tuple_element_types(operand)[index]`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1: a) `index < 0`.
    C1: b) `index >= size(operand)`.
    C2: a) `type(result) != tuple_element_types(operand)[index]`.
    ```
    
    closes #1127

[33mcommit bd167f43cf9595bb737544236ed3ecb5fab1aa27[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Aug 30 20:05:46 2023

    Bump patch version after integrate 0.14.16 -> 0.14.17 (#1762)

[33mcommit 7111fafc516e4bb657004902834d1bc0a548558a[m[33m ([m[1;33mtag: [m[1;33mv0.14.16[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 29 17:45:45 2023

    Bump patch version after integrate 0.14.15 -> 0.14.16 (#1759)

[33mcommit 6caddc21342ac00da8f53a3269851d4e79d76c42[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 28 19:18:03 2023

    Integrate LLVM at llvm/llvm-project@3823395c9d71 (#1757)

[33mcommit 1ce6d356b2b75fb88d161b99a4d9679d56e65162[m[33m ([m[1;33mtag: [m[1;33mv0.14.15[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 25 20:58:10 2023

    Integrate LLVM at llvm/llvm-project@1ff0bdb86dbf (#1753)

[33mcommit de3a897ece10a8fed3ef64e90b63ac511875759b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Aug 25 20:57:06 2023

    Disable multithreaded tests (#1756)
    
    There are TSAN violations for multithreaded tests. I've added #1755 to
    address it, and we should disable it in the meantime.

[33mcommit f7f4ca800b7fafcd2ef56df88e1ae968af97007c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 25 19:32:07 2023

    Bump patch version after integrate 0.14.14 -> 0.14.15 (#1754)

[33mcommit 067e8f5819a4fb44e47901ed96ec8f378d19b8f5[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Aug 25 17:49:46 2023

    Remove XLA attribute from test (#1752)
    
    Port of
    https://github.com/tensorflow/mlir-hlo/commit/c20cbf28f7963aed769c280aa7ab5f39ec4ba2f1

[33mcommit b9f56f6e2f71efe01eaaca352f10715d6eb249fd[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Aug 24 18:15:45 2023

    [NFC] Explicitly set dialects to usePropertiesForAttributes=0 (#1750)
    
    Port of
    https://github.com/tensorflow/mlir-hlo/commit/e8a6a7c2b51f7f7e27d050afbc115e805b8a9fc6

[33mcommit 05217d5db3a77c2322bf89b0a063e93c17a068a2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Aug 24 16:27:08 2023

    Clang tidy fix (#1751)

[33mcommit d9a17eb58eb5d2b788d1b6ec53e043f5b18659fe[m[33m ([m[1;33mtag: [m[1;33mv0.14.14[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 23 21:15:03 2023

    Integrate LLVM at llvm/llvm-project@f806be5eaae1 (#1749)

[33mcommit f914fc018df7ceaefd709b1d7ab8a463e582fecd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Aug 19 01:07:46 2023

    Add interpreter for ReduceScatterOp (#1731)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand` is a tensor.
    (I2) `scatter_dimension` is a constant of type `si64`.
    (I3) `replica_groups` is a 2-dimensional tensor constant of type `si64`.
    (I4) `channel_id` is a constant of type `si64`.
    (I5) `use_global_device_ids` is a constant of type `i1`.
    (I6) `computation` is a function.
    (C1) `dim(operand, scatter_dimension) % dim(process_groups, 1) = 0`.
    (C2) `0 <= scatter_dimension < rank(operand)`.
    (C3) `is_unique(replica_groups)`.
    (C4) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    (C5) `0 <= replica_groups < size(replica_groups)`.
    (C6) If `use_global_device_ids = true`, then `channel_id > 0`.
    (C7) `computation` has type `(tensor<E>, tensor<E>) -> (tensor<E>)` where
         `E = element_type(operand)`.
    (C8) `type(result) = type(operand)` except:
    * `dim(result, scatter_dimension) = dim(operand, scatter_dimension) /
      dim(process_groups, 1)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `scatter_dimension` is not a constant of type `si64`. (Covered by ODS).
    I3: a) `replica_groups` is not a 2-dimensional tensor constant.
    I3: b) `element_type(replica_groups) != si64`. (Covered by ODS).
    I4: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    I5: a) `use_global_device_ids` is not a constant of type `i1`. (Covered by ODS).
    I6: a) `computation` is not a function. (Covered by ODS).
    C1: a) `dim(operand, scatter_dimension) % dim(process_groups, 1) != 0`.
    C2: a) `scatter_dimension < 0`.
    C2: b) `scatter_dimension >= rank(operand)`.
    C3: a) `is_unique(replica_groups) = false`.
    C4: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C5: a) `replica_groups < 0`.
    C5: b) `replica_groups >= size(replica_groups)`.
    C6: a) If `use_global_device_ids = true`, then `channel_id <= 0`.
    C7: a) `computation` does not have type `(tensor<E>, tensor<E>) -> (tensor<E>)` where
         `E = element_type(operand)`.
    C8: a) `type(result) != type(operand)` except:
    * `dim(result, scatter_dimension) = dim(operand, scatter_dimension) /
      dim(process_groups, 1)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I3: a) `replica_groups` is not a 2-dimensional tensor constant.
    C1: a) `dim(operand, scatter_dimension) % dim(process_groups, 1) != 0`.
    C2: a) `scatter_dimension < 0`.
    C2: b) `scatter_dimension >= rank(operand)`.
    C3: a) `is_unique(replica_groups) = false`.
    C4: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C5: a) `replica_groups < 0`.
    C5: b) `replica_groups >= size(replica_groups)`.
    C6: a) If `use_global_device_ids = true`, then `channel_id <= 0`.
    C7: a) `computation` does not have type `(tensor<E>, tensor<E>) -> (tensor<E>)` where
         `E = element_type(operand)`.
    C8: a) `type(result) != type(operand)` except:
    * `dim(result, scatter_dimension) = dim(operand, scatter_dimension) /
      dim(process_groups, 1)`.
    ```
    
    Notes:
    * C1a verification is infeasible since process_groups is not known
    statically.
    * C4a verification is infeasible since `num_replicas` and
    `num_partitions` are not known statically.
    * C5b verification is infeasible since `size(replica_groups)` is not
    known statically.
    
    closes #1133

[33mcommit 56a693f4534f1a1123bee04d72869ce2b9ebf575[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Aug 19 00:14:01 2023

    Add interpreter for AllToAllOp (#1730)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand` is a tensor.
    (I2) `split_dimension` is a constant of type `si64`.
    (I3) `concat_dimension` is a constant of type `si64`.
    (I4) `split_count` is a constant of type `si64`.
    (I5) `replica_groups` is a 2-dimensional tensor constant of type `si64`.
    (I6) `channel_id` is a constant of type `si64`.
    (C1) `0 <= split_dimension < rank(operand)`.
    (C2) `dim(operand, split_dimension) % split_count = 0`.
    (C3) `0 <= concat_dimension < rank(operand)`.
    (C4) `0 < split_count`.
    (C5) `is_unique(replica_groups)`.
    (C6) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    (C7) `0 <= replica_groups < size(replica_groups)`.
    (C8) `dim(replica_groups, 1) = split_count`.
    (C9) `type(result) = type(operand)` except:
    * `dim(result, split_dimension) =
      dim(operand, split_dimension) / split_count`.
    * `dim(result, concat_dimension) =
      dim(operand, concat_dimension) * split_count`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `split_dimension` is not a constant of type `si64`. (Covered by ODS).
    I3: a) `concat_dimension` is not a constant of type `si64`. (Covered by ODS).
    I4: a) `split_count` is not a constant of type `si64`. (Covered by ODS).
    I5: a) `replica_groups` is not a 2-dimensional tensor constant.
    I5: b) `type(replica_groups) != si64`. (Covered by ODS).
    I6: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    C1: a) `split_dimension < 0`.
    C1: b) `split_dimension >= rank(operand)`.
    C2: a) `dim(operand, split_dimension) % split_count != 0`.
    C3: a) `concat_dimension < 0`.
    C3: b) `concat_dimension >= rank(operand)`.
    C4: a) `split_count <= 0`.
    C5: a) `is_unique(replica_groups) = false`.
    C6: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    C7: a) `replica_groups < 0`.
    C7: b) `replica_groups >= size(replica_groups)`.
    C8: a) `dim(replica_groups, 1) != split_count`.
    C9: a) `type(result) != type(operand)` except:
    * `dim(result, split_dimension) =
      dim(operand, split_dimension) / split_count`.
    * `dim(result, concat_dimension) =
      dim(operand, concat_dimension) * split_count`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I5: a) `replica_groups` is not a 2-dimensional tensor constant.
    C1: a) `split_dimension < 0`.
    C1: b) `split_dimension >= rank(operand)`.
    C2: a) `dim(operand, split_dimension) % split_count != 0`.
    C3: a) `concat_dimension < 0`.
    C3: b) `concat_dimension >= rank(operand)`.
    C4: a) `split_count <= 0`.
    C5: a) `is_unique(replica_groups) = false`.
    C6: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    C7: a) `replica_groups < 0`.
    C7: b) `replica_groups >= size(replica_groups)`.
    C8: a) `dim(replica_groups, 1) != split_count`.
    C9: a) `type(result) != type(operand)` except:
    * `dim(result, split_dimension) =
      dim(operand, split_dimension) / split_count`.
    * `dim(result, concat_dimension) =
      dim(operand, concat_dimension) * split_count`.
    ```
    
    Notes:
    * C6a verification is infeasible since `num_replicas` and
    `num_partitions` are not known statically.
    * C7b verification is infeasible since `size(replica_groups)` is not
    known statically.

[33mcommit 2a38fde3c79898bda9ea1d124244974ea01dce21[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Aug 19 00:01:02 2023

    Add interpreter for AllGatherOp (#1727)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor or per-tensor quantized tensor.
    (I2) `all_gather_dim`: constant of type `si64`.
    (I3) `replica_groups`: 2-dimensional tensor constant of type `si64`.
    (I4) `channel_id`: constant of type `si64`.
    (I5) `use_global_device_ids`: constant of type `i1`.
    (C1) `0 <= all_gather_dim < rank(operand)`.
    (C2) `is_unique(replica_groups)`.
    (C3) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    (C4) `0 <= replica_groups < size(replica_groups)`.
    (C5) If `use_global_device_ids = true`, then `channel_id > 0`.
    (C6) `type(result) = type(operand)` except:
    * `dim(result, all_gather_dim) =
      dim(operand, all_gather_dim) * dim(process_groups, 1)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `all_gather_dim` is not a constant of type `si64`. (Covered by ODS).
    I3: a) `replica_groups` is not a 2-dimensional tensor constant.
    I3: b) `element_type(replica_groups) != si64`. (Covered by ODS).
    I4: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    I5: a) `use_global_device_ids` is not a constant of type `i1`. (Covered by ODS).
    C1: a) `all_gather_dim < 0`.
    C1: b) `all_gather_dim >= rank(operand)`.
    C2: a) `is_unique(replica_groups) = false`.
    C3: a) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C4: a) `replica_groups < 0`.
    C4: b) `replica_groups >= size(replica_groups)`.
    C5: a) `use_global_device_ids = true` and `channel_id <= 0`.
    C6: a) `type(result) != type(operand)` except:
    * `dim(result, all_gather_dim) =
      dim(operand, all_gather_dim) * dim(process_groups, 1)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I3: a) `replica_groups` is not a 2-dimensional tensor constant.
    C1: a) `all_gather_dim < 0`.
    C1: b) `all_gather_dim >= rank(operand)`.
    C2: a) `is_unique(replica_groups) = false`.
    C3: a) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C4: a) `replica_groups < 0`.
    C4: b) `replica_groups >= size(replica_groups)`.
    C5: a) `use_global_device_ids = true` and `channel_id <= 0`.
    C6: a) `type(result) != type(operand)` except:
    * `dim(result, all_gather_dim) =
      dim(operand, all_gather_dim) * dim(process_groups, 1)`.
    ```
    
    Notes:
    * C3a verification is infeasible since `num_replicas` and
    `num_partitions` are not known statically.
    * C4b verification is infeasible since `size(replica_groups)` is not
    known statically.
    
    closes #1118

[33mcommit a2375684b60f1a3dcca51a926767f5f8d77cc2cc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Aug 18 21:52:25 2023

    Add specification for per-axis quantization scheme (#1702)
    
    This PR **partially** solves #1574 in the sense that it just proposes a
    specification for per-axis quantitation scheme.
    In the current proposal we defined the semantics as part of
    quantize/dequantize meta functions avoiding text descriptions to provide
    more clarity.
    
    Please let me know your feedback.

[33mcommit fa92c2358d402d52e5e2133f0e057c3e822647be[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 17 22:02:17 2023

    Bump patch version after integrate 0.14.13 -> 0.14.14 (#1744)

[33mcommit 5aa1adeb2a412d1aca8972385cd9b26b1a524052[m[33m ([m[1;33mtag: [m[1;33mv0.14.13[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 17 02:39:20 2023

    Followup on port of tensorflow/mlir-hlo@5aa3df6 (#1742)
    
    This PR brings the remaining StableHLO changes from downstream changes
    from following #1724.

[33mcommit 2f3980f6d6966521038d4bb8ccb746868eb7b2a2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 16 20:46:48 2023

    Integrate LLVM at llvm/llvm-project@cad3130a23da (#1743)

[33mcommit 5651688a65993db85d88d5765181da2896c916aa[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 16 00:35:48 2023

    Fix typo in `vhlo.md` instructions (#1739)

[33mcommit 3365d5465fdbf96e8e932d09f532ac6c80fcaeba[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 16 00:02:17 2023

    Bump patch version after integrate 0.14.12 -> 0.14.13 (#1740)

[33mcommit 87e210763167c34a22338a46af4767fdac6fa92d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 15 20:21:12 2023

    Use `ChannelId` alias instead of `int64_t` (#1738)

[33mcommit 9232c90394fa75ccd00f4551a5b3506f8561485d[m[33m ([m[1;33mtag: [m[1;33mv0.14.12[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 15 19:13:49 2023

    Fix race condition in `ProcessGrid::rendezvous` (#1737)
    
    There is a race condition when two processes try to access
    `channelLocks_[channelKey]` at the same time when that key doesn't
    exist. The two processes will both create a mutex at the same time,
    acquiring different mutex instead of sharing the same one. It is also
    unsafe to read from `channels_[channelKey]` without a lock since this
    value can also be modified by another caller to `rendezvous` with a
    duplicate `processGroup` and `channelId` by another distribution op.

[33mcommit 8b366fddfb15b8bdc49a94b6fa6c2f4ab3e4827c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 15 01:39:59 2023

    Integrate LLVM at llvm/llvm-project@913f21ae5c46 (#1736)

[33mcommit dfa6eeb757b724bbb179e3129216635a3076c253[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 15 00:47:20 2023

    Bump patch version after integrate 0.14.11 -> 0.14.12 (#1735)

[33mcommit 92f87f3c632d3dca37240a7fa4d11c22f4d455f8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 15 00:29:21 2023

    Cleanup after a downstream integrate (#1734)
    
    This PR addresses clang-tidy issues found during the ongoing downstream
    integrate (we don't run clang-tidy on our PRs yet, so these issues have
    deferred detection).

[33mcommit f368a68a34eccd8239fef19274523e119ed622a2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Aug 10 21:25:26 2023

    Port of tensorflow/mlir-hlo@5aa3df6 to avoid invalid access (#1724)
    
    Also fixed strange naming mix of snake/camel case `non_spatialDim` to
    match mlir-hlo. Fixed for MHLO in tensorflow/mlir-hlo@5aa3df6 but forgot
    to make the stopgap fix in StableHLO as well.

[33mcommit 9351c747f78d60dbc0da21d59d445b2cb272598b[m[33m ([m[1;33mtag: [m[1;33mv0.14.11[m[33m)[m
Author: Yuanqiang Liu <liuyuanqiang.yqliu@bytedance.com>
Date:   Thu Aug 10 17:39:23 2023

    fix BUILD.bazel (#1729)
    
    This will cause that llvm/torch-mlir failed to integrate mlir-hlo with
    bazel.

[33mcommit b07191b36f2c558e87fdd9e3f4565f67b1de6df3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 10 17:24:10 2023

    Fix typo in Passes.td (#1728)

[33mcommit 69753dcd6cff65c7242d3996ea85341360105030[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 10 01:07:24 2023

    Integrate LLVM at llvm/llvm-project@f9a609c555be (#1726)
    
    Importing changes from downstream patch:
    https://github.com/tensorflow/tensorflow/blob/631d5829e51d7401df514ab6ea20128711708d80/third_party/stablehlo/temporary.patch#L184-L210

[33mcommit 17f5830349f4ef9446c501b5e6e80fb96ecd0472[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 9 20:29:20 2023

    Add interpreter for AllReduceOp (#1721)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand` tensor.
    (I2) `replica_groups` variadic number of 1-dimensional tensor constant of type `si64`.
    (I3) `channel_id` constant of type `si64`.
    (I4) `use_global_device_ids` constant of type `i1`.
    (I5) `computation` function.
    (C1) `is_unique(replica_groups)`.
    (C2) `size(replica_groups)` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    (C3) `0 <= replica_groups < size(replica_groups)`.
    (C4) If `use_global_device_ids = true`, then `channel_id > 0`.
    (C5) `computation` has type `(tensor<E>, tensor<E>) -> (tensor<E>)` where
         `E = element_type(operand)`.
    (C6) `type(result) = type(operand)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `replica_groups` is not a variadic number of 1-dimensional tensor.
    I2: b) `element_type(replica_groups) != si64`. (Covered by ODS).
    I3: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    I4: a) `use_global_device_ids` is not a constant of type `i1`. (Covered by ODS).
    I5: a) `computation` is not a function. (Covered by ODS).
    C1: a) `is_unique(replica_groups)` = false.
    C2: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C3: a) `replica_groups < 0`.
    C3: b) `replica_groups >= size(replica_groups)`.
    C4: a) If `use_global_device_ids = true` and `channel_id <= 0`.
    C5: a) `computation` does not have type `(tensor<E>, tensor<E>) -> (tensor<E>)`
           where `E = element_type(operand)`.
    C6: a) `type(result) != type(operand)`. (Covered by ODS).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2: a) `replica_groups` is not a variadic number of 1-dimensional tensor.
    C1: a) `is_unique(replica_groups)` = false.
    C2: a) `size(replica_groups)` is not defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_replicas` if `cross_replica_and_partition` is used.
    * `num_processes` if `flattened_ids` is used.
    C3: a) `replica_groups < 0`.
    C3: b) `replica_groups >= size(replica_groups)`.
    C4: a) If `use_global_device_ids = true` and `channel_id <= 0`.
    C5: a) `computation` does not have type `(tensor<E>, tensor<E>) -> (tensor<E>)`
           where `E = element_type(operand)`.
    ```
    
    Notes:
      * I2 is not tested due to #498.
    * C2a verification is infeasible since `num_replicas` and
    `num_partitions` are not knwon statically.
    * C3b verification is infeasible since `size(replica_groups)` is not
    known statically.
    
    closes #1119

[33mcommit 4993cf7612c547a7f94f9657a3fef04e509f3e25[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Aug 9 20:02:47 2023

    Add interpreter for OutfeedOp (#1723)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `inputs`: is a variadic number of tensors.
    (I2) `token`: is a `token`.
    (I3) `outfeed_config`: is a constant of type `string`.
    (C1) `type(result) = token`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `inputs`: is not a variadic number of tensors. (Covered by ODS).
    I2: a) `token`: is not a `token`. (Covered by ODS).
    I3: a) `outfeed_config` is not a constant of type `string`. (Covered by ODS).
    C1: a) `type(result) != token`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1: a) `type(result) != token`.
    ```
    
    closes #1130

[33mcommit f1733e9200872ad02cd1ff052c4a61946898d2f9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 8 17:17:41 2023

    Add interpreter for CollectivePermuteOp (#1715)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor.
    (I2) `source_target_pairs`: 2-dimensional tensor constant of type `si64`.
    (I3) `channel_id`: constant of type `si64`.
    (C1) `dim(source_target_pairs, 1) = 2`.
    (C2) `is_unique(source_target_pairs[:, 0])`.
    (C3) `is_unique(source_target_pairs[:, 1])`.
    (C4) `0 <= source_target_pairs < N`, where N is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    (C5) `type(result) = type(operand)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `source_target_pairs` is not a 2-dimensional tensor constant of type `si64`.
    I3: a) `channel_id` is not a constant of type `si64`. (Covered by ODS).
    C1: a) `dim(source_target_pairs, 1) != 2`.
    C2: a) `is_unique(source_target_pairs[:, 0]) = false`.
    C3: a) `is_unique(source_target_pairs[:, 1]) = false`.
    C4: a) `source_target_pairs < 0`.
    C4: b) `source_target_pairs >= N`, where `N` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    C5: a) `type(result) != type(operand)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2a: `source_target_pairs` is not a 2-dimensional tensor constant of type `si64`.
    C1a: `dim(source_target_pairs, 1) != 2`.
    C2a: `is_unique(source_target_pairs[:, 0]) = false`.
    C3a: `is_unique(source_target_pairs[:, 1]) = false`.
    C4a: `source_target_pairs < 0`.
    C4b: `source_target_pairs >= N`, where `N` is defined as:
    * `num_replicas` if `cross_replica` is used.
    * `num_partitions` if `cross_partition` is used.
    C5a: `type(result) != type(operand)`.
    ```
    
    Notes:
    * C4b verification is infeasible since `num_replicas` and
    `num_partitions` are not known statically at the moment (see #650).
    
    closes #1124

[33mcommit a9ab84b453c26ecf179bd6e845ee8bbcc9f7cac0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 8 06:10:08 2023

    Bump patch version after integrate 0.14.10 -> 0.14.11 (#1719)

[33mcommit 19f8eaf8c4222603252403c268c2e60c6568348f[m[33m ([m[1;33mtag: [m[1;33mv0.14.10[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Aug 7 21:58:16 2023

    Integrate LLVM at llvm/llvm-project@9b6aaf1dcaf5 (#1718)

[33mcommit 5eac531caf3ab29c36c68c89500454a60380bbc3[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 7 18:40:14 2023

    Typo fixes in the example provided for the spec of CollectivePermute (#1717)

[33mcommit a58fd230139b8bcff67b91c4fb8da6d74ff6d53e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Aug 6 16:49:59 2023

    Bump patch version after integrate 0.14.9 -> 0.14.10 (#1714)

[33mcommit 0f888242957d25896b52efa40e94176083f10b1b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Aug 5 01:37:56 2023

    Using dequant-op-quant for convolution and dot_general (#1700)
    
    fixes #1628
    
    The idea behind the PR is to treat the quantization of `dot_general` and
    `convolution` similar to how various other element wise ops are
    specified. Other than the obvious parity of the specification with other
    ops, there are other benefits too.
    
    1. `dot_general` and `convolution` are potential candidates for per-axis
    quantization. With this solution we can define per-axis scheme
    uniformaly in the meta-functions `dequantize` and `quantize` and thereby
    do not have to explicitly handle the per-axis scheme for these ops.
    2. `dot_general` and `convolution` uses some data_movement ops (like
    `reshape`, `transpose`, `pad`, `slice`) for the underlying
    specification. There is a concern where the fact that `dot_general` and
    `convolution` can potentially use per-axis quantization necessitates
    handling of the per-axis scheme for the data movement ops just because
    the later are used in the specification of the former. With the current
    solution, we wouldn't need to worry about per-axis quantization for the
    underlying data movement ops.

[33mcommit 5c95053a2d159ae661094677908f95632055402e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Aug 4 02:35:17 2023

    Patch BUILD.bazel after integrate (#1713)

[33mcommit 8b49e5f4e5c69d52e4c59092e7a49f0fcfa48d21[m[33m ([m[1;33mtag: [m[1;33mv0.14.9[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 3 19:51:30 2023

    Add interpreter for PartitionIdOp (#1712)
    
    closes #1131

[33mcommit c5daa3d448c7128b243c279504691ad63dad442f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Aug 3 17:52:52 2023

    Add interpreter for ReplicaIdOp (#1709)
    
    This PR also implements the `RunParallelOp` in the `Interpreter` dialect
    to enable parallel execution of distribution ops. The description of
    semantics and example for `RunParallelOp` is added.
    
    closes #1134

[33mcommit 0dc5322c1e62c0d71ed8bf5fb291874b58e5ba54[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Aug 3 17:41:19 2023

    Integrate LLVM at llvm/llvm-project@e68ffa373f7f (#1711)

[33mcommit c99ba0d13dcf46ca9581ce4aa69cfb45262df3da[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Aug 2 18:30:35 2023

    Bump patch version after integrate 0.14.8 -> 0.14.9 (#1710)

[33mcommit 7b8ee090d90a64f64774beeaf435eb83e443aa7b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Aug 1 20:51:18 2023

    Patch BUILD.bazel files after integrate (#1708)
    
    These include some lint related changes on the `BUILD.bazel` file.

[33mcommit 9ae6c373a6e2941ff84a8831bb3724728cb2b49a[m[33m ([m[1;33mtag: [m[1;33mv0.14.8[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Aug 1 16:49:45 2023

    Add Interpreter dialect (#1703)
    
    The StableHLO reference implementation is nearing its final phase to
    implement most of the specced operations in the StableHLO opset, and one
    major remaining piece is the Distribution ops. This PR creates the
    `Interpreter` dialect specifically targeted for interpreter-related ops
    that enable running parallel execution of Distribution ops. In the
    future, this dialect may also be used to add additional functionality to
    the interpreter.

[33mcommit 0a750295c28549282b692cf2c34af168c5708e2c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 1 16:29:01 2023

    Patch changes from previous integrate (#1707)
    
    Port of
    https://github.com/tensorflow/mlir-hlo/commit/5aa3df6e13ea76f3290414e011f60113d5d26e65
    and
    https://github.com/tensorflow/mlir-hlo/commit/f7fa8f5b6e0f9a058913000ab9c8966ae730a343

[33mcommit 2fa32ac5d23ea7cfb101d79ede47a4fd6a233e75[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 1 15:56:58 2023

    Bump patch version after integrate 0.14.7 -> 0.14.8 (#1706)

[33mcommit 4a00e2ff399fe7da7be75309150f4f04d3b11fec[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jul 31 16:15:47 2023

    Integrate LLVM at llvm/llvm-project@41895843b591 (#1705)

[33mcommit c90abca5ac64b571747c96b2c42976590bfad2c6[m[33m ([m[1;33mtag: [m[1;33mv0.14.7[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jul 28 19:45:58 2023

    Integrate LLVM at llvm/llvm-project@cb924ddca514 (#1704)
    
    Includes bytecode update from fix in https://reviews.llvm.org/D156198
    
    Manually confirmed these work on previous releases

[33mcommit 9c9441f01bc3e5dc7b1a2674302ebeb39bbb7e99[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jul 26 18:47:12 2023

    Alphanumerically order ops (#1701)
    
    Some functions were alphabetically ordered up to their op name
    disregarding the suffix "Op". This now orders all ops to
    alphanumerically order on the entire Operation name (e.g. `LogOp` should
    come first before `LogisticOp` since `O` < `i`).

[33mcommit d3d73d483e5364d012a1026a04b626b3d6a6b9ea[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 25 21:51:14 2023

    Bump patch version after integrate 0.14.6 -> 0.14.7 (#1699)

[33mcommit 8fc2265d49e59e04055261c76943b4b6283d1513[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 25 21:01:36 2023

    Add `HLO_Commutative` trait to avoid inheriting folders (#1698)
    
    After https://reviews.llvm.org/D155687 the builtin Commutative trait has
    folders by default. StableHLO does not want to have any default folding
    or canonicalization. Adding a `hlo::OpTrait::IsCommutative` and
    `HLO_Commutative` trait to avoid these folders.

[33mcommit 8816d0581d9a5fb7d212affef858e991a349ad6b[m[33m ([m[1;33mtag: [m[1;33mv0.14.6[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jul 25 18:13:12 2023

    Typo in convolution constraint (#1697)
    
    Missing closing parenthesis

[33mcommit db7a61b012bc7ae33f704c927cbcb9dea8de443c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 25 18:06:00 2023

    Integrate LLVM at llvm/llvm-project@fc3b7874b6c9 (#1696)
    
    Fixes bazel build for
    https://github.com/llvm/llvm-project/commit/67a910bbff772ebf4c47e8b434b59cdc4820bb68
    
    MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/eefae33d95aefb857b1c7fc1204c2de53acb6b8b.

[33mcommit 3f51a7fd4b7986c3c0006db0f8465bb87a099994[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jul 24 20:32:52 2023

    Bump patch version after integrate 0.14.5 -> 0.14.6 (#1695)

[33mcommit d0b779d1af62a2239eaef4059ef62c7787d1e31e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jul 24 16:07:50 2023

    Specification of quantized ops from `Control-flow`, `Distribution`, `Extensibility`, and `Miscellaneous` categories.  (#1647)
    
    ### Summary
    
    This PRs provides the specification of the ops from the following
    categories:
    
    * **Control-flow ops**: `after_all, case, if, optimization_barrier,
    while`
    - `afer_all`: Not applicable for quantization as the ops deals with
    setting data-flow ordering using tokens.
    - `case, if`: These ops implicitly captures operands from the parent
    scope. The result of these operations and the corresponding blocks can
    be quantized types.
    - `optimization_barrier`: The `operand` and `result` can both be
    quantized tensors.
    - `while`: The `operand` and `result` can both be quantized tensors and
    of same type. Per the semantics of `while` op, the output of the
    associated block is used for the next iteration with the `operand` used
    for the very first iteration. With that, I do not think we can use the
    `operand` and `result` with different quantization types.
    
    * **Distribution**: `all_gather, all_reduce, all_to_all,
    collective_permute, infeed, outfeed, partition_id, recv, reduce_scatter,
    replica_id, send`
    All the ops in this category except for the following ones will support
    quantized types:
    - `all_reduce, reduce_scatter`: These involve quantization of reduction
    operation https://github.com/openxla/stablehlo/pull/1538. I would
    propose to address these two after the quantization of reduction ops are
    addressed.
      - `partition_id, replica_id`: Not applicable
    
    * **Extensibility**: `custom_call, get_tuple_element, tuple`
    All the ops in this category supports all the `ValueType`s including
    `QuantizedTensorType`. No change is needed in the existing spec.
    
    * **Miscellaneous**: `batch_norm_grad, batch_norm_inference,
    batch_norm_training, cholesky, constant, fft, iota, rng,
    rng_bit_generator, triangular_solve`
    - The specification of `batch_norm_inference`, `cholesky`,
    `triangular_solve` ops follow the same `dequantize-op-quantize` strategy
    as we followed for the elementwise ops.
    - `batch_norm_grad, batch_norm_training`: These involve quantization of
    reduction operation https://github.com/openxla/stablehlo/pull/1538. I
    would propose to address these two after the quantization of reduction
    ops are addressed.
    - `constant, iota`: The output type of these ops can be a quantized type
    of user-defined quantization parameters.
    - `fft`: This op supports `element_type(operand) = element_type(result)
    = complex type`, when the `fft_type` is `FFT` or `IFFT`. For `fft_type =
    RFFT`, the op supports floating-point `operand` and complex type
    `result` and for `fft_type = IRFFT`, it supports complex type `operand`
    and floating-point `result`. IMO, only for `fft_type = RFFT`, the
    operand can support quantized type and the semantics could be to
    quantize the operand to floating-point before doing the computation.
    - `rng`: The `operand` and `result` can be of quantized type only when
    the `rng_distribution` is `NORMAL` in which case the semantics follows
    the `dequantize_op_quantize` strategy. The motivation for this choice
    comes from the fact the op supports floating point type only when
    `rng_distribution` is `NORMAL` and we propose to treat this op, on
    floating-point tensors, as an op on quantized tensor, along the lines of
    dequantize -> float computation -> quantize.
    - `rng_bit_generator`: To be discussed in "open questions" section
    below.
    
    ### Open questions
    - `rng_bit_generator`: This op generates a randomized output tensor
    based on a `rng_algorithm` algorithm (`DEFAULT`, `THREE_FRY`, and
    `PHILOX`). It is not clear how there algorithm should handle generating
    quantized output. My proposal here is to skip quantization of this op
    unless we have a use case for it. Please let me know your opinion on
    this.
    
    Please review and let me know your feedback.
    
    
    
    **update 06/26/2023**
    Scoping `rng` out from quantization based on
    https://github.com/openxla/stablehlo/pull/1647#issuecomment-1607857449
    
    **update 07/13/2023**
    Per
    https://github.com/openxla/stablehlo/pull/1647#discussion_r1251233481, I
    proposed not to fuse de-quantization of operand and quantization of
    result for `fft_type = RFFT` and `fft_type = IRFFT` respectively for fft
    op. With that a user can explicit de-quantize of operand or quantize of
    result as needed.
    
    In the current proposal, the `batch_norm_inference` op, do not consider
    the constant operand `epsilon` to be of quantized type. IMO, considering
    that operand with quantized type unnecessary complicate the
    expressiveness of the op.

[33mcommit f4c43e121aa94a27eb1b846f2e1d776732203400[m[33m ([m[1;33mtag: [m[1;33mv0.14.5[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jul 21 20:05:34 2023

    Integrate LLVM at llvm/llvm-project@37937b8a040c (#1693)

[33mcommit 07a648cc81ef599bfba9b947c75657cdf9662b91[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Jul 19 19:53:25 2023

    Fix --stablehlo-refine-shapes for i1 => ... conversions (#1692)
    
    MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/35f099cb3b784bf2b5130c526f7538ab01f7bc22

[33mcommit 9c17c93d5597df34f0f27defa9b8577f1bf99382[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Jul 19 16:27:36 2023

    Integrate LLVM at llvm/llvm-project@f0dfe682bca0 (#1690)

[33mcommit bf4ff668528f789bd12480de2dfbe68cab73dcc0[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jul 18 14:55:38 2023

    Only run scheduled build and test CI on main repo (#1689)
    
    Currently this CI runs in all forked repos, which provides little added
    benefit to the runs that occur in main, and when this job fails, it ends
    up doubling the amount of notifications.

[33mcommit aa6da81dab7338fb27e17ce3c1d0c998e10a022c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jul 17 17:16:38 2023

    Add header for Forward Compat Testing RFC, fix header in fp8 RFC. (#1688)

[33mcommit ae8fbde883a330e9dea78a5c1cd3dc3697f2bee8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jul 13 17:41:44 2023

    Bump patch version after integrate 0.14.4 -> 0.14.5 (#1686)

[33mcommit 41bad512515d609ccd3896d74bf697e7d456e1d3[m[33m ([m[1;33mtag: [m[1;33mv0.14.4[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jul 12 20:37:57 2023

    Integrate LLVM at llvm/llvm-project@b10899d86995 (#1685)

[33mcommit b18e50b56bf4ee337e78216e720b75711270a5f1[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jul 12 20:13:00 2023

    Bump patch version after integrate 0.14.3 -> 0.14.4 (#1684)

[33mcommit 3c93a12e2cf8ccf74e012d4a8a6da4e664429122[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jul 12 16:11:26 2023

    [RFC] Forward Compatibility Testing (#1498)
    
    This is a proposal for forward compatibility testing in StableHLO. This
    ensures that payloads serialized at HEAD can be deserialized at a
    previous release.
    
    Note, this plan is built on top of our existing backward compatibility
    testing infrastructure, and as such does not include semantic forward
    compatibility testing. Once we have addressed semantic compatibility
    testing in #1416 we can expand this machinery to include semantic
    forward compatibility testing.
    
    - **Proposed design prototype:**
    https://github.com/GleasonK/stablehlo/pull/35
    - **Alternate design prototype:**
    https://github.com/GleasonK/stablehlo/pull/33
    
    Notification of this PR will be sent to the
    [openxla-discuss](https://groups.google.com/a/openxla.org/g/openxla-discuss)
    group.
    
    Related to #1318.
    
    UPDATE 5/25: While this RFC is in review, some form of forward
    incompatibility detection is still required by StableHLO. In
    https://github.com/openxla/stablehlo/pull/1525 a stop-gap solution based
    on the preferred design was implemented. It will be modified in
    accordance with feedback on this RFC.

[33mcommit 4add5f0e890bc66b333e86961978f066325f8a86[m[33m ([m[1;33mtag: [m[1;33mv0.14.3[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Jul 10 18:39:44 2023

    Integrate LLVM at llvm/llvm-project@dbaa5838c13e (#1682)

[33mcommit 2f86ea0a60c8e31ecebbbddc9d2abb7ba4f06fd8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jul 8 17:29:47 2023

    Integrate LLVM at llvm/llvm-project@986001c8274a (#1681)

[33mcommit f836d8a52f352c0eaa2872ba67173f18419fcfd2[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jul 7 18:12:06 2023

    Bump patch version after integrate 0.14.2 -> 0.14.3 (#1680)

[33mcommit 4b3e9d66e08a290f462ab1e0b6b8e7ffe71656fe[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jul 6 18:12:52 2023

    Fix a build warning in conversion to TOSA (#1677)
    
    Apparently, tosa.logical_not doesn't implement type inference, so the
    build warning was asking to explicitly provide a return type in the PDLL
    pattern, which this PR does.

[33mcommit 5a82a7b88cdaffbfcb1c25699b5cc06500cc8012[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jul 6 18:12:27 2023

    Cleanup after a downstream integrate (#1676)
    
    This PR addresses clang-tidy issues found during the ongoing downstream
    integrate (we don't run clang-tidy on our PRs yet, so these issues have
    deferred detection).
    
    Also, I changed stablehlo_canonicalize_dynamism.mlir to explicitly
    specify operation types in CHECK directives. This pass is all about
    operation types, so let's check them explicitly.

[33mcommit 20b1da42266a1f351b8315bc195faabceaa74f3e[m[33m ([m[1;33mtag: [m[1;33mv0.14.2[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jul 6 05:45:13 2023

    Offer StableHLO portable artifact deserialization as a pass pipeline (#1651)
    
    Some users want to write APIs that accept portable artifacts _and_ other
    programs. Currently adding support for portable artifacts requires
    additional logic to test if input is a portable artifact, then
    deserialize it before running any other passes. This is not ergonomic,
    as the more MLIR way of doing this is to offer a pass that will
    deserialize _if_ the input is a portable artifact - for example, support
    for StableHLO was added to some TF APIs using a
    `createHloLegalizeToStablehlo` pass.
    
    To get to a point to permit this, we need two changes:
    
    1. Don't error on non-VHLO dialects when deserializing, currently only
    happens in `--vhlo-to-version`.
    This isn't a big issue, as `--stablehlo-legalize-to-vhlo` already
    validates that programs are stable. The current way VHLO to version is
    written is already more lenient than StableHLO->VHLO, as it only errors
    on Func and StableHLO ops, meaning, for example, that arith ops are
    permitted even without this change (would be caught by StableHLO->VHLO
    though).
    
    2. Add deserialization pipeline. This allows users of StableHLO to add
    these passes to their input pipeline to quickly support StableHLO
    portable artifacts.
    
    Part of https://github.com/openxla/stablehlo/issues/1530.

[33mcommit 5af7726bc009af6665b0294ced769fd61e7e05b8[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jul 6 05:31:55 2023

    Fix cspell spelling issues in MD files (#1674)
    
    Now cspell passes for `**/*.md`.

[33mcommit a9191504d0fb7bd4b02b543591418c769d037c6f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jul 6 04:22:12 2023

    Fix spelling errors in .cpp and .h files (#1671)
    
    Used `cspell` with a user dictionary. Will upload a lint script for this
    shortly.
    
    Fixes:
    - bitwidth -> bit width. With a space seems far more common. Even LLVM
    methods use `BitWidth`.
    - dnums -> dimNums. Could go either way on this one, but seems like a
    more descriptive name that passes spell check is fine.
    - The rest are typos or improvements to camelcase

[33mcommit 9ec824cf0a4783048215f484db1442d0de266103[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jul 6 04:17:38 2023

    Add lint to ensure that compatibility tests are implemented for new versions of StableHLO (#1670)
    
    This will fail if we bump the version to `Version(0, 15, 0)` without
    adding new compatibility tests.
    
    Fixes https://github.com/openxla/stablehlo/issues/1563

[33mcommit 3c3ce2714dcc329f5e8685bd13efd77e4ed81f08[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jul 6 03:55:45 2023

    Integrate LLVM at llvm/llvm-project@8910cc2742f9 (#1673)

[33mcommit 322019744e551713d69ea1d4a65f6b74c01cc00e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jul 6 01:55:35 2023

    Add interpreter for BitcastConvertOp (#1463)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor.
    (C1) Let `E` and `E'` be the `operand` and `result` element type,
    respectively and `R = rank(operand)`:
    * If `num_bits(E')` = `num_bits(E)`, shape(`result`) = shape(`operand`).
    * If `num_bits(E')` < `num_bits(E)`:
      * `rank(result) = R+1`.
      * dim(`result`, `i`) = dim(`operand`, `i`) for all `i` in [0, `R`-1].
      * `dim(result, R) = num_bits(E)/num_bits(E')`.
    * If `num_bits(E')` > `num_bits(E)`:
      * `rank(result) = R-1`.
      * dim(`result`, `i`) = dim(`operand`, `i`) for all `i` in [0, `R`-1).
      * `dim(operand, R-1) = num_bits(E')/num_bits(E)`.
    (C2) Conversion between complex and non-complex types is not permitted.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    C1: a) If `num_bits(E')` = `num_bits(E)`, shape(`result`) != shape(`operand`).
    C1: b) If `num_bits(E')` < `num_bits(E)`: `rank(result) != R+1`.
    C1: c) If `num_bits(E')` < `num_bits(E)`: dim(`result`, `i`) != dim(`operand`, `i`) for any `i` in [0, `R`-1].
    C1: d) If `num_bits(E')` < `num_bits(E)`: `dim(result, R) != num_bits(E)/num_bits(E')`.
    C1: e) If `num_bits(E')` > `num_bits(E)`: `rank(result) != R-1`.
    C1: f) If `num_bits(E')` > `num_bits(E)`: dim(`result`, `i`) != dim(`operand`, `i`) for all `i` in [0, `R`-1).
    C1: g) If `num_bits(E')` > `num_bits(E)`: `dim(operand, R-1) != num_bits(E')/num_bits(E)`.
    (C2) Conversion between complex and non-complex types is permitted.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: If `num_bits(E')` = `num_bits(E)`:
         shape(`result`) != shape(`operand`).
    C1b: If `num_bits(E')` < `num_bits(E)`:
         `rank(result) != R+1`.
    C1c: If `num_bits(E')` < `num_bits(E)`:
         dim(`result`, `i`) != dim(`operand`, `i`) for any `i` in [0, `R`-1].
    C1d: If `num_bits(E')` < `num_bits(E)`:
         `dim(result, R) != num_bits(E)/num_bits(E')`.
    C1e: If `num_bits(E')` > `num_bits(E)`:
         `rank(result) != R-1`.
    C1f: If `num_bits(E')` > `num_bits(E)`:
         dim(`result`, `i`) != dim(`operand`, `i`) for all `i` in [0, `R`-1).
    C1g: If `num_bits(E')` > `num_bits(E)`:
         `dim(operand, R-1) != num_bits(E')/num_bits(E)`.
    C2: Conversion between complex and non-complex types is permitted.
    ```
    
    Notes:
    * The interpreter assumes little endian representation, and we have
    #1460 to address adding support for big endian architectures.
    
    closes #1098

[33mcommit ac47650e939c4e93f758a11de742384319f83e52[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jul 6 01:55:02 2023

    Fix incorrect `window_end` calculation for ReduceWindowOp (#1665)
    
    Turns out https://github.com/openxla/stablehlo/pull/1492 didn't actually
    fix the bug since `window_end` is still calculated incorrectly.
    
    Shoutout to @pravnar for filing the bug :)
    
    closes #1662

[33mcommit 99b9304c450d646115d174d9b0ce032e72f14449[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jul 6 01:43:47 2023

    Elaborate on the semantics of tokens (#1669)
    
    closes #1577

[33mcommit bd8d708c237939debc8f189f8752083541b93ce9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jul 6 01:27:26 2023

    Avoid out of bound writes in the spec of ScatterOp (#1561)
    
    As a follow up to
    https://github.com/openxla/stablehlo/pull/1488#discussion_r1207464016,
    this PR updates the spec to clarify the behavior of OOB result indices
    and to match HLO semantics and the current reference implementation.

[33mcommit 87f76adc816c64e762beda9f92842fccb98ac8f6[m
Author: Aart Bik <39774503+aartbik@users.noreply.github.com>
Date:   Fri Jun 30 20:52:50 2023

    Update 20230210-sparsity.md (#1663)
    
    fixed typo

[33mcommit 6a44c628f1c42b2c83922116f93cd15c01571c9d[m
Author: Mehdi Amini <joker.eph@gmail.com>
Date:   Fri Jun 30 19:22:19 2023

    Add custom print for stablehlo.slice (#1657)
    
    Uses a syntax close to XLA HLO
    
    Subset of #1578

[33mcommit 499421265340d3c6375a0ef69d82739478afc2ee[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jun 30 19:21:25 2023

    Bump patch version after integrate 0.14.1 -> 0.14.2 (#1661)

[33mcommit 33349064bd8bd95b23f67555c85bccdd7e4c1098[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jun 30 16:06:40 2023

    Add HLO_CompatibleOperandsAndResultElementType to CHLO broadcast MinOp and MaxOp (#1659)
    
    Port of
    https://github.com/tensorflow/mlir-hlo/commit/65666664af3d400bd3ddc4fd581e3345e259468f

[33mcommit 1e339395e9e9fcd644a153672bf14cb6d91f7fdf[m[33m ([m[1;33mtag: [m[1;33mv0.14.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 29 20:03:04 2023

    Adding the missing quantized-type support in the ODS (#1608)
    
    Some of the stablehlo ops does not have support for quantized types in
    their tablegen specification, prohibits writing StableHLO quantized
    programs using those ops. The PR is about adding the missing support for
    the following ops. Also, I believe the ongoing specification
    [work](https://github.com/openxla/stablehlo/issues/588), should not
    deviate much from the proposed changes here.
    
    ```
    stablehlo.atan2
    stablehlo.divide
    stablehlo.power
    stablehlo.remainder
    stablehlo.subtract
    
    stablehlo.abs
    stablehlo.cbrt
    stablehlo.cosine
    stablehlo.exponential
    stablehlo.exponential_minus_one
    stablehlo.log
    stablehlo.log_plus_one
    stablehlo.logistic
    stablehlo.negate
    stablehlo.rsqrt
    stablehlo.sign
    stablehlo.sine
    stablehlo.sqrt
    stablehlo.tanh
    
    stablehlo.cholesky
    stablehlo.triangular_solve
    ```
    
    Other than these ops, we have `fft`, `rng`, and `rng_bit_generator` (or
    something else which I might be missing) which could be potential
    candidates for the support. I propose that we add the support after
    adding the specification of those op as adding the support might need
    some non-trivial discussion.

[33mcommit ce932dd84928c9c6c0f0895167069e9ab11559cd[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 29 19:38:49 2023

    Integrate LLVM at llvm/llvm-project@989879f8fded (#1655)

[33mcommit aa4c9a1abd143177ce6e0f34b17fa6ea2375aff6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 29 19:08:58 2023

    Clarify what "unspecced features" means (#1652)
    
    Based on a conversation with Kevin from earlier today. This is a nice
    catch and an important clarification.
    
    We do want to provide compatibility guarantees around new features (e.g.
    make sure that they cannot be serialized to portable artifacts with
    older target versions) - unspecced features mean something different,
    which is explicitly elaborated in the document now.

[33mcommit 7b9456e70fed9000e4c7bdf89f63de9c944f157e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 29 17:28:16 2023

    Bump patch version 0.14.0 -> 0.14.1 (#1654)

[33mcommit 2ae78dc73b80d0fa27a57e0cdb89eebcac1eada6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 28 18:49:38 2023

    Delete spurious files added accidentally (#1650)
    
    Deleted some spurious files added as part of #1566

[33mcommit 3d7fff8dd34dd1dc37f81c8fc592d7d12b7617f8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 28 14:55:29 2023

    Specification for quantized element-wise operations (#1566)
    
    ### Summary
    
    The PR proposes the specification for quantized element wise operations
    (total 47).
    
    ### Details
    
    Overall, we propose treating quantized elements as floating-point
    elements, and therefore ops on quantized tensors as ops on
    floating-point
    tensors, along the lines of dequantize -> float computation -> quantize.
    This
    principle works for most elementwise ops, although there are some
    exceptions
    discussed below.
    
    Furthermore, the proposal is to only support per-tensor quantization for
    elementwise ops. We haven't yet come across use cases for per-axis
    quantization
    for these ops, so let's start small. The story for per-axis quantization
    will
    be worked out in #1574.
    
    Finally, we propose to not support hybrid quantization (i.e. situations
    when
    some inputs/outputs are quantized and some are not) for now. The story
    for this
    will be worked out in #1575.
    
    - **Ops that support tensors of floating-point types (33)**: These ops
    support
    quantized tensors, with semantics following dequantize -> float
    computation ->
    quantize. We are using the `dequantize_op_quantize` function to express
    it:
    - Binary (10): `add, atan2, compare, divide, maximum, minimum, multiply,
        power, remainder, subtract`.
    - Unary (20): `abs, cbrt, ceil, cosine, exponential,
    exponential_minus_one,
    floor, is_finite, log, logistic, log_plus_one, negate, reduce_precision,
    round_nearest_afz, round_nearest_even, rsqrt, sign, sine, sqrt, tanh`.
      - Ternary (2): `clamp, select`.
      - Other (1): `map`.
    
    - **Ops that don't support tensors of floating-point types (9)**: These
    ops
    (`and, count_leading_zero, not, or, popcnt, shift_left,
    shift_right_arithmetic,
    shift_right_logical, xor`) don't support quantized tensors. If there is
    a need
    to perform computations on the underlying integer representation of
    these
    tensors, they can be bitcast_convert'ed to integers.
    
    - **Ops that involve complex types (3)**: These ops (`complex`, `imag`,
    `real`) don't support quantized
    tensors because quantization doesn't compose with complex types at the
    moment.
    
    - **Conversion ops (2)**:
    - `convert`: A convert from a quantized type to any type can be realized
    using
    `stablehlo.uniform_dequantize` followed by `stabhle.convert` to convert
    the
    dequantized floating-point type to type of choice. Similarly, a convert
    from
    any type to quantized type can be realized using `stablehlo.convert` to
    floating-point type followed by `stablehlo.uniform_quantize`. It's not
    necessarily great that we have 3 ops to represent something that could
        theoretically be represented by 1 op, and we're planning to explore
        a potential simplification in #1576.
    - `bitcast_convert`: Works with low-level representations, so it treats
        quantized elements as integer elements.

[33mcommit dbbaa81cf4be6d063427609c90c561907c6cdbfb[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 28 14:52:22 2023

    Specification of the remaining quantized data movement ops (#1567)
    
    The PR proposes the specification of remaining data movement ops.
    Specifically, the PR covers `concatenate`, `sort`, `broadcast_in_dim`,
    `gather`, `scatter`, `dynamic_slice`, and `dynamic_update_slice`. The
    rest are covered in a separate
    [PR](https://github.com/openxla/stablehlo/pull/1535).
    
    I note that
    - The specifications are very similar to each other and hence i thought
    about putting it in a single PR.
    - Following the discussion item in
    [link](https://groups.google.com/a/openxla.org/g/openxla-discuss/c/iwE9is49SS4/m/mi9AyvKZAQAJ),
    I proposed the specification to use using pex-tensor quantization
    granularity. Feel free to provide your input if any of the op has a
    use-case to support per-axis scheme.

[33mcommit 5cd80f2866f1f244374a2dd5b2cc28e92d10f32b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Jun 28 14:41:39 2023

    Add constraints to some quantized ops related to per-axis scheme (#1535)
    
    ### Summary
    
    The PR augments the a few ops, whose specs is already published, with
    constraints in the following way:
    
    Constraints added w.r.t the per-axis qunatization scheme,
     - **transpose**:  Transpose the `quantization_dimension` of `output`.
    - **reshape**: Avoid merging or splitting of elements along
    `quantization dimension` of `operand`.
     - **slice**: Slice the `zero_points(operand)` and `scales(operand)`.
    - **reverse**: reverse the `scales` and `zero_points` of the `result` if
    the `quantization_dimension(operand)` is in `dimensions` to reverse.
    - **pad**: The op is simplified to use only per-tensor scheme. The
    rationale for this is as follows:
    - If the `padding_value` is added in axis other than
    `quantization_dimension(operand)`, then the `zero_point(padding_value)`
    should match the `zero_points(operand)[i]` for all `i`. That means all
    the values in `zero_points(operand)` are the same, which is basically
    the per-tensor case.
    - Only meaningful case is if the `padding_value` is added __only__ along
    the `quantization_dimension(operand)`. In this case, the
    `zero_points(result)` and `scales(result)` need to be updated
    accordingly. I am not if there is a use case for this particular case
    and hence kept things simple by allowing on per-tensor scheme for this
    op.
    
    Constraints added to comply with other similar specs
    - **add**: For `QuantizedTensorType`, we introduced a new non-terminal
    `QuantizedElementType` which is different from the non-terminal
    `ElementType` used for `TensorType`. The proposed change accommodates
    that.
    
    Please let me know your feedback.
    
    PS: The PR involves shuffling the constraint labels (C*). Once the PR
    reaches a consensus, I will update the verification/type infenrence code
    to match the constraint labels.
    
    **update**: As per my
    [proposal](https://groups.google.com/a/openxla.org/g/openxla-discuss/c/iwE9is49SS4/m/mi9AyvKZAQAJ),
    I have updated the PR to specify the semantics using per-tensor scheme.
    We can find that it greatly simplifies the specification. With that my
    proposal is to add per-axis details on a need basis.
    
    **update 06/20/2023**
    
    It turns out that the current PRs becomes "empty" after a few PRs are
    merged after the introduction of the current PR.
    
    I note that:
    1. Some of the ops addressed in the current PRs (like `transpose, slice,
    reshape`) have already been introduced with quantized type in #1413
    which got merged after the PR was introduced.
    2. Other ops addressed in the current PRs (like `pad, reverse`) are also
    been introduced with quantized type in #1477 which got merged after the
    PR was introduced.
    3. The newly introduced constraints in the current PRs are all
    simplified while rebasing with
    https://github.com/openxla/stablehlo/pull/1629.

[33mcommit 23a5e8ab0ee041e0d19a57269d592b9dd42893a4[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 28 02:44:23 2023

    Patch bazel build dependency issues found during integrate (#1649)

[33mcommit d865f3f8a32cf50458bcf300be3392b3e2fcb95a[m[33m ([m[1;33mtag: [m[1;33mv0.14.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jun 26 20:25:33 2023

    Integrate LLVM at llvm/llvm-project@0e9384a6c6ca (#1648)

[33mcommit 4578178cefc194cf62237cb6736f21c8328e4dd5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jun 26 17:10:39 2023

    Further update compatibility.md (#1642)
    
    This PR aggregates missing information from multiple RFCs:
      1) Situations which are not covered by compatibility guarantees
         (numerical accuracy, bug compatibility and unspecced features).
      2) How our compatibility suite works (this part of compatibility.md
         is pretty outdates).

[33mcommit d75151f85036063464059a81a94c4700ad29e718[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Jun 24 06:38:41 2023

    Add interpreter for ReducePrecisionOp (#1479)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor of floating-point type.
    (I2) `exponent_bits`: constant of type `si32`.
    (I3) `mantissa_bits`: constant of type `si32`.
    (C1) `operand` and `output` have the same type.
    (C2) `exponent_bits` >= 1.
    (C3) `mantissa_bits` >= 0.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    (I1) `operand` is not a tensor of floating-point type. (Covered by ODS).
    (I2) `exponent_bits` is not a constant of type `si32`. (Covered by ODS).
    (I3) `mantissa_bits` is not a constant of type `si32`. (Covered by ODS).
    (C1) type(`operand`) != type(`output`). (Covered by ODS).
    (C2) `exponent_bits` < 1.
    (C3) `mantissa_bits` < 0.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    (C2) `exponent_bits` < 1.
    (C3) `mantissa_bits` < 0.
    ```
    
    closes #1109

[33mcommit b6e4b8c58cf0ec9b27b2244678e7a5967152fdbc[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Jun 24 01:25:02 2023

    Avoid out of bound reads in the spec of GatherOp (#1541)
    
    In a recent
    [discussion](https://github.com/openxla/stablehlo/pull/1058#discussion_r1203386357)
    on the GatherOp implementation, we pointed out that the semantics did
    not fully address what to do with out of bounds indexes. To make sure
    the bounds are always correct, clamping on the `start_index` is needed.
    This reflects the change needed in the spec and the reference
    implementation.

[33mcommit 980900a45b38ca30d7d205d1bd31941c67d2b3fc[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Jun 24 00:47:52 2023

    Specialize `getResult()` calls to specific `FooOp` classes (#1644)
    
    As pointed out in
    https://github.com/openxla/stablehlo/pull/1635#discussion_r1240498968,
    we can clean up some code by calling `getResult()` on the specific op
    class in favor of calling `getResult(0)` on the wrapper everywhere.

[33mcommit a1799b492a921d20f2017cc6f7780dbeb5ae2319[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jun 23 23:58:16 2023

    Add interpreter for OptimizationBarrierOp (#1636)
    
    There are no constraints for this op, so no additional tests were added
    besides renaming/moving tests.
    
    closes #1129

[33mcommit 468546f86232dc79e91d9a59146c5373ee35f9c5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jun 23 23:40:37 2023

    Add interpreter for AfterAllOp (#1635)
    
    This PR also introduces Token for the interpreter. Tokens represent
    opaque values produced and consumed by some StableHLO operations. Tokens
    are used for imposing execution orders on side-effecting operations, and
    #1577 will further elaborate on the semantics of Tokens.
    
    There are no constraints for this op, so no additional tests were added
    besides renaming/moving tests.
    
    closes #1117

[33mcommit a3fbbc3fa39667273f264a504a232d81fa30fdd3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jun 23 19:47:33 2023

    Further align StableHLO and MLIR-HLO builds (#1614)
    
    To follow up on #1612, this PR further aligns StableHLO and MLIR-HLO
    builds to facilitate the transition from mlir-hlo to stablehlo.
    
    Changes in this PR:
      1) Adds Ccache support.
      2) Adds support for LLVM_ENABLE_ZLIB.
      3) Removes STABLEHLO_STANDALONE_BUILD, to better align with MLIR-HLO
         terminology.
    
    Deltas between CMakeLists.txt in this PR and CMakeLists.txt in MLIR-HLO:
      a) Says "StableHLO" instead of "MHLO".
      b) Supports STABLEHLO_ENABLE_STRICT_BUILD (there is no equivalent
         in the MLIR-HLO build).
      c) Doesn't support ${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules yet,
         as well as some MLIR_HLO_FOO_DIR variables needed for that.
         (We have #1549 to follow up on this).

[33mcommit bb381994b2a6075caa9ed856242dd863127ccf73[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jun 23 08:01:33 2023

    Fix typo in a link in one of the RFC (#1639)

[33mcommit cf2341aeaf9e397f7ed1e3464afaa0ee00ae1c47[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 22 23:24:23 2023

    Integrate LLVM at llvm/llvm-project@213709e7be03 (#1640)

[33mcommit 72a0117bd3c83c3b8a0c6c84b91433def735cf93[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 22 21:09:45 2023

    Remove uses of ArgResultAliasAttr as it is not used, specced, or supported in VHLO (#1421)
    
    Was looking at VHLO code coverage for serialization and realized the
    only unused attribute is ArgResultAliasAttr, because it was unsupported
    in VHLO legalizations. Speaking with @burmako it sounds like this
    attribute is unused and should not be in StableHLO/VHLO currently
    without an RFC first.
    
    This is not an incompatibility since serialization would fail for
    programs that use this attribute currently.
    
    Adding @sdasgup3 as reviewer as well for spec implications (should be
    none?)

[33mcommit 5ae526d2fa444ec8e56639ad889e8e43b014a36b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 22 20:23:56 2023

    Fix a constraint in `bitcast_convert` op (#1637)
    
    As recommended in
    https://github.com/openxla/stablehlo/pull/1566/files/13a2c8b6e518d611fb7bd0747096392478291eef#r1237644676,
    the constraint in `bitcast_convert` is revised to make sure that the
    `num_bits(E) % num_bits(E') == 0` when `num_bits(E') < num_bits(E)` and
    `num_bits(E') % num_bits(E) == 0` when `num_bits(E) < num_bits(E')`.
    
    Note that this does not need a change in existing verification and
    testing as the the verifier for `bitcast_convert` is doing the exact
    [check](https://github.com/openxla/stablehlo/blob/d8f0c12e2a7541cfe6ba3a04c8a2fb350bdab43d/stablehlo/dialect/TypeInference.cpp#L3174)
    as proposed in the PR.

[33mcommit ff89c9338a4b617dcba71e17ac06e0f258c8a006[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 22 19:55:24 2023

    Sharpen wording in compatibility.md (#1638)
    
    This follows up on #1606, further removing the traces of the
    compatibility policy that got updated in a recent compatibility RFC.

[33mcommit d8f0c12e2a7541cfe6ba3a04c8a2fb350bdab43d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 21 17:34:47 2023

    Add scheduled CI for StableHLO builds against LLVM@HEAD (#1562)
    
    Felt inspired after seeing a presentation on JAX CI in
    [openxla/openxla-pjrt-plugin](https://github.com/openxla/openxla-pjrt-plugin)
    (like
    [run_jaxtests_cpu.yml](https://github.com/openxla/openxla-pjrt-plugin/blob/main/.github/workflows/run_jaxtests_cpu.yml)),
    decided to add some frequent (every 6hr) builds against the latest LLVM
    revision to detect compatibility / other issues as soon as possible.
    
    This is built on top of existing CI. The name of the buildAndTestCmake
    job now includes what StableHLO is being built against. Examples from my
    fork:
    - [cmake-build
    (llvm-project@HEAD)](https://github.com/GleasonK/stablehlo/actions/runs/5136120754/jobs/9242454311)
    - [cmake-build
    (llvm_version.txt)](https://github.com/GleasonK/stablehlo/actions/runs/5136234330/jobs/9242713655)
    
    _Note: This only builds scheduled CI against HEAD, meaning it should
    have no impact on CI for pull requests._
    
    Other things to consider:
    - If LLVM build fails (not StableHLO build) should we exit with success,
    indicating a likely upstream breakage?
    - How should these be monitored? Is subscribing to email notification
    enough?
    
    Closes #1489

[33mcommit d79d1ef044b7e2a7bbe61dbb7c12b77d125e4acb[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jun 20 21:32:27 2023

    Disable OOB gather tests (#1630)
    
    This PR disables `gather` related tests enabled by a script prematurely
    as part of #1597 (these were conditionally passing at the time of
    merge). They would be reenabled by #1541 which correctly passes these
    tests.

[33mcommit 1ad3c51859d3ed16978000cbbef9fd13952432a6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 20 19:20:28 2023

    Add license header to StablehloLspServerMain.cpp (#1625)
    
    Follows up on #1620 to consistently follow the convention that we're
    using in the codebase.

[33mcommit c49153a05f1ca31d858da8f36207bbe75fd9bbfb[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 20 19:10:21 2023

    Also build //... with Bazel in CI (#1623)
    
    We have some targets (e.g. stablehlo-lsp-server) which are not part of
    any tests, but I think it would be good to include them in CI in some
    way.

[33mcommit 3165471a4b9026e6912d4dd957ae8c02152a3b86[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 20 16:51:19 2023

    Refactor the grammar for element types (#1629)
    
    As brought up in
    https://github.com/openxla/stablehlo/pull/1567#discussion_r1217167342,
    there is a need to introduce a computation function, with name
    `element_type`, which can be applied for both `TensorElementType` and
    `QuantizedTensorElementType` in order to simplify the operation
    specification.
    
    The PR does the following:
    1. Rename `ElementType` to `TensorElementType`.
    2. Rename `QuantizedElementType` to `QuantizedTensorElementType`.
    3. Introduction a quantization computation function `element_type`.
    4. Use the function at (3) for `convolution` and `pad`.

[33mcommit 8993ad54839add6648b88801f1d223b7f9bc2e58[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 19 15:31:50 2023

     Specification for UniformQuantizeOp and UniformDequantizeOp (#1496)
    
    fixes #531
    fixes #530
    
    ## Summary
    The PR proposes the specification for `uniform.quantize` and
    `uniform.dequantize` ops.
    
    The specification of `uniform.quantize` also captures the
    re-quantization conversions from quantized tensor to quantized tensors.
    
    Please let me know your feedback.
    
    
    ### Working notes on following the `reference_checklist.md` and
    `spec_checkilist.md`
    
    #### uniform_dequantize
    
    We have the following constraints from the spec
    
    ```
    (I1)  `operand` is a  quantized tensor
    (C1) `shape(operand) = shape(result)`.
    (C2) `element_type(result) = expressed_type(operand)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand: quantized tensors`. (Covered by ODS).
    C1: a) `shape(operand) != shape(result)`. (Covered by ODS)
    C2: a) `element_type(result) != expressed_type(operand)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C2: a) `element_type(result) != expressed_type(operand)`.
    ```
    
    We already has a type inference test to cover the above.
    
    
    #### uniform_quantize
    
    We have the following constraints from the spec
    
    ```
    (I1)  `operand: tensor of floating-point or quantized¬†type`.
    (C1) `shape(operand) = shape(result)`.
    (C2) `expressed_type(result) = is_float(operand) ? element_type(operand) :
      expressed_type(operand)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand: quantized tensors`. (Covered by ODS).
    C1: a) `shape(operand) != shape(result)`. (Covered by ODS)
    C2: a) if_float(operand): `expressed_type(result) != element_type(operand)`.
          b) if_quantized(operand): `expressed_type(result) !=  expressed_type(operand)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C2: a) if_float(operand): `expressed_type(result) != element_type(operand)`.
          b) if_quantized(operand): `expressed_type(result) !=  expressed_type(operand)`.
    ```
    
    The above will be covered as part of
    https://github.com/openxla/stablehlo/issues/1603.

[33mcommit 556db3fcf2928a9fb8d2273b4dc1196716af0aad[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 19 00:31:27 2023

    Augment the `spec_checklist.md` (#1626)
    
    As brought up
    [here](https://github.com/openxla/stablehlo/pull/1496#discussion_r1230975223)
    that some parts of the `reference_checklist.md` are relevant enough to
    be included in the `spec_checklist.md`.

[33mcommit 496d10563224a24337619874f57b2ef9fd21e18e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jun 16 17:07:31 2023

    Actually bump MLIR Bytecode Format Version to 5 (#1624)
    
    Based on its title, #1559 was meaning to bump the bytecode version to 5,
    but it didn't include the actual bump, which is what this PR fixes :)

[33mcommit be4d19c701e9efc705e10482e6b2d69f51ab093d[m[33m ([m[1;33mtag: [m[1;33mv0.13.4[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 15 21:38:07 2023

    Further integrate stablehlo-lsp-server into our build (#1622)
    
    Add Bazel build, integrate building stablehlo-lsp-server into CI.

[33mcommit 7b27b0275a6549208c6c739525d6cca42e8ee837[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 15 20:49:25 2023

    Integrate LLVM at llvm/llvm-project@261b693afd82 (#1621)

[33mcommit 9ede2c662086d2928a90ddf2efb35d3ebbe8033c[m
Author: Mehdi Amini <joker.eph@gmail.com>
Date:   Thu Jun 15 20:33:14 2023

    Add stablehlo-lsp-server (#1620)
    
    This allows VSCode plugin integration as documented here:
    
    
    https://mlir.llvm.org/docs/Tools/MLIRLSP/#mlir-lsp-language-server--mlir-lsp-server

[33mcommit c1e0916a199886d9cd28741d7847e05e1916a9b9[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 15 20:17:38 2023

    Implement EvalMinOp and EvalClampOp in shape refinement (#1619)
    
    This complements the already existing EvalMaxOp to support shape
    computations which involve clamping.

[33mcommit 2fcdf9b25d622526f81cd1575c65d01a6db319d2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 15 14:32:27 2023

    Specification for quantized ConvolutionOp (#1477)
    
    ## Summary
    The PR proposes the specification for quantized convolution op along
    with the specifications for a few other ops on which the convolution
    specification depends on, for example, `pad`, `reverse`.
    
    ## A few details
    1. Given `q = tensor with uniformed quantized type`, the PR covers the
    semantics of static range quantized `convolution(q, q)`. The
    specification is very similar to the earlier proposed [quantized
    dot_general
    op](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dot_general).
    
    2. Given, that many ops support both per-tensor and per-axis
    quantization schemes, it might be redundant to specify for each op the
    semantics of the computation involved in both the scheme. To avoid that
    we decided that the op-level semantics will only specify the per-tensor
    semantics, whereas, semantics of per-axis computations will be described
    generally in one place.

[33mcommit 88cf77767a1c409317cb05c1c9af08e9b34fb9a1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 14 22:27:07 2023

    Make getMinVersion() and getMaxVersion() parse version numbers at compile time (#1618)
    
    `mlir::ConversionTarget::isLegal` call these two functions during
    StableHLO<->VHLO legalization, causing string version numbers to be
    repeatedly parsed by `mlir::vhlo::Version::fromString` at runtime. Our
    performance analysis indicates that such version number parsing can
    dominate the execution time of legalization pass.
    
    Since version numbers are available to TableGen, this CL uses TableGen
    functions to parse version numbers at compile time. With this change,
    `getMinVersion()` and `getMaxVersion()` use `mlir::vhlo::Version(x, y,
    z)` to avoid string parsing, which greatly improves the performance.
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/51ce2fa22bc9acff37332ad02ef39412f21ffc98

[33mcommit eb4cd3cbb98541d08d9d048a4070a0ac3ef6e506[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jun 14 22:26:31 2023

    [chlo] Add HLO_CompatibleOperandsAndResultElementType to validate quantized types (#1617)
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/bb7b1eb4565e02a822a754bb536577454ffc64b2
    
    Allows `CHLO::BroadcastAddOp` to support quantized types with compatible
    differences in representation.

[33mcommit 55e2bce13ead181d1f42742c6bb170df73622eaf[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Jun 14 19:49:44 2023

    Integrate LLVM at llvm/llvm-project@0258a53521cf (#1616)
    
    The commit hash is from Friday, since LLVM integrate is still working on
    ingesting recent changes in LLVM.

[33mcommit 52de2c0b4b7446a56e8985a4de6d50c1d8b47c3b[m
Author: Jakub Kuderski <kubakuderski@gmail.com>
Date:   Tue Jun 13 23:55:21 2023

    [CMake] Make StableHLO build consistent with MLIR-HLO (#1612)
    
    This applies when stablehlo is built by other projects enabled as an
    external llvm project.
    
    This change makes it match the mlir-hlo configuration more closely:
    
    https://github.com/tensorflow/mlir-hlo/blob/51ce2fa22bc9acff37332ad02ef39412f21ffc98/CMakeLists.txt#L113-L130,
    and ease the transition from mlir-hlo to stablehlo:
    https://discourse.llvm.org/t/sunsetting-the-mlir-hlo-repository/70536.
    
    Tested by embedding stablehlo into IREE.
    
    Issue: https://github.com/openxla/stablehlo/issues/1549

[33mcommit 0a8900ba1fdeecb350cebe6b95c070a1aa26abe5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 13 19:01:40 2023

    fix bug in verification in ReduceOp (#1613)
    
    It seems that there is a bug in the verifier of `reduce` op when all the
    inputs are un-ranked. Specifically,
    
    ```
    if ((!allInputsUnranked &&
             dimension >= inputTypes[rankedInputIdx].getRank()) ||
            dimension < 0)
          return emitOptionalError(
              location, "Out-of-bounds dimension ", dimension,
              " for input-tensor rank: ", inputTypes[rankedInputIdx].getRank());
    ```
    
    If `allInputsUnranked` is true and `dimension < 0`, then while
    evaluating the error message ` inputTypes[rankedInputIdx].getRank()` we
    will incur an invalid memory access as `rankedInputIdx = -1` when
    `allInputsUnranked = true`.
    
    Added a test to reproduce that.

[33mcommit 2caece0d0f2355d0dbc1bc73c5d5815ecbde7417[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 13 00:50:15 2023

    Unify references to discussion threads in RFCs (#1611)
    
    All RFCs now have a consistent "Discussion thread" header that points to
    the primary location where the discussion has been happening.
    
    We went through quite a few approaches over the last year (GitHub
    discussions, GitHub PRs, mailing lists), so I think it's good to make
    this uniform.
    
    Also, dates in some RFC documents were incorrectly referring to 2022
    instead of 2023. I fixed that too.

[33mcommit 2c7f3666c61779c9536bd9077e7fc3c9d1121e1c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 13 00:40:46 2023

    [RFC] Add StableHLO => Linalg lowering to openxla/stablehlo (#1610)
    
    This PR archives an approved RFC from
    [openxla-discuss](https://groups.google.com/a/openxla.org/g/openxla-discuss/c/KsRp9euuuB0).

[33mcommit 7b3f8d71b79f547d25cce8409ab0d88a19de1621[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Jun 11 17:39:51 2023

    Bump patch version after integrate 0.13.3 -> 0.13.4 (#1609)

[33mcommit e57f8ce530fcaa17d4ac58dbd6c87a823fd24390[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jun 9 17:57:19 2023

    Add interpreter for SelectAndScatterOp (#1518)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor.
    (I2) `source`: tensor.
    (I3) `init_value`: 0-dimensional tensor.
    (I4) `window_dimensions`: 1-dimensional tensor constant of type `si64`.
    (I5) `window_strides`: 1-dimensional tensor constant of type `si64`.
    (I6) `padding`: 2-dimensional tensor constant of type `si64`.
    (I7) `select`: function.
    (I8) `scatter`: function.
    (C1) `element_type(operand) = element_type(source)`.
    (C2) `shape(source) = num_windows` where:
    * `padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]`.
    * `is_empty_window = padded_operand_shape = 0 || window_dimensions > padded_operand_shape`.
    * `num_windows = is_empty_window ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1`.
    (C3) `element_type(init_value) = element_type(operand)`.
    (C4) `size(window_dimensions) = rank(operand)`.
    (C5) `0 < window_dimensions`.
    (C6) `size(window_strides) = rank(operand)`.
    (C7) `0 < window_strides`.
    (C8) `shape(padding) = [rank(operand), 2]`.
    (C9) `select` has type `(tensor<E>, tensor<E>) -> tensor<i1>` where
         `E = element_type(operand)`.
    (C10) `scatter` has type `(tensor<E>, tensor<E>) -> tensor<E>` where
          `E = element_type(operand)`.
    (C11) `type(operand) = type(result)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor. (Covered by ODS).
    I2: a) `source` is not a tensor. (Covered by ODS).
    I3: a) `init_value` is not a 0-dimensional tensor.
    I4: a) `window_dimensions` is not a 1-dimensional tensor.
        b)  element_type(`window_dimensions`) != `si64`. (Covered by ODS).
    I5: a) `window_strides` is not a 1-dimensional tensor constant of type `si64`.
        b)  element_type(`window_strides`) != `si64`. (Covered by ODS).
    I6: a) `padding` is not a 2-dimensional tensor constant of type `si64`.
        b)  element_type(`padding`) != `si64`. (Covered by ODS).
    I7: a) `select` is not a function. (Covered by ODS).
    I8: a) `scatter` is not a function. (Covered by ODS).
    C1: a) `element_type(operand) != element_type(source)`.
    C2: a) `shape(source) != num_windows` where:
    * `padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]`.
    * `is_empty_window = padded_operand_shape = 0 || window_dimensions > padded_operand_shape`.
    * `num_windows = is_empty_window ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1`.
    C3: a) `element_type(init_value) != element_type(operand)`.
    C4: a) `size(window_dimensions) != rank(operand)`.
    C5: a) `0 >= window_dimensions`.
    C6: a) `size(window_strides) != rank(operand)`.
    C7: a) `0 >= window_strides`.
    C8: a) `shape(padding) != [rank(operand), 2]`.
    C9: a) `select` does not have type `(tensor<E>, tensor<E>) -> tensor<i1>` where
           `E = element_type(operand)`.
    C10: a) `scatter` does not have type `(tensor<E>, tensor<E>) -> tensor<E>` where
            `E = element_type(operand)`.
    C11: a) `type(operand) != type(result)`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I3a: `init_value` is not a 0-dimensional tensor.
    I4a: `window_dimensions` is not a 1-dimensional tensor.
    I5a: `window_strides` is not a 1-dimensional tensor constant of type `si64`.
    I6a: `padding` is not a 2-dimensional tensor constant of type `si64`.
    C1a: `element_type(operand) != element_type(source)`.
    C2a: `shape(source) != num_windows` where:
    * `padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]`.
    * `is_empty_window = padded_operand_shape = 0 || window_dimensions > padded_operand_shape`.
    * `num_windows = is_empty_window ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1`.
    C3a: `element_type(init_value) != element_type(operand)`.
    C4a: `size(window_dimensions) != rank(operand)`.
    C5a: `0 >= window_dimensions`.
    C6a: `size(window_strides) != rank(operand)`.
    C7a: `0 >= window_strides`.
    C8a: `shape(padding) != [rank(operand), 2]`.
    C9a: `select` does not have type `(tensor<E>, tensor<E>) -> tensor<i1>` where
         `E = element_type(operand)`.
    C10a: `scatter` does not have type `(tensor<E>, tensor<E>) -> tensor<E>` where
          `E = element_type(operand)`.
    C11a: `type(operand) != type(result)`.
    ```
    
    Notes:
    * Removes duplicate constraint (C1) `rank(operand) =
    size(window_dimensions)` of (C5) `size(window_dimensions) !=
    rank(operand)` (in the original spec).
    
    closes #988

[33mcommit 74106415cdba761523c423e72f13574e9fefbf7b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jun 9 17:50:42 2023

    Add interpreter for DotGeneralOp (#748)
    
    We have the following non-quantization-related constraints (excluding
    C13, C15-C20) in the spec:
    
    ```
    (I1) lhs tensor.
    (I2) rhs tensor.
    (I3) lhs_batching_dimensions 1-dimensional tensor constant of type `si64`.
    (I4) rhs_batching_dimensions 1-dimensional tensor constant of type `si64`.
    (I5) lhs_contracting_dimensions 1-dimensional tensor constant of type `si64`.
    (I6) rhs_contracting_dimensions 1-dimensional tensor constant of type `si64`.
    (I7) precision_config variadic number of enum of `DEFAULT`, `HIGH`, and `HIGHEST`.
    (C1) size(`lhs_batching_dimensions`) = size(`rhs_batching_dimensions`).
    (C2) size(`lhs_contracting_dimensions`) =
    size(`rhs_contracting_dimensions`).
    (C3) `lhs_batching_dimensions` and `lhs_contracting_dimensions` combined are
    unique.
    (C4) `rhs_batching_dimensions` and `rhs_contracting_dimensions` combined are
    unique.
    (C5) 0 <= `lhs_batching_dimensions[i]` < rank(`lhs`) for all `i`
    in [0, size(`lhs_batching_dimensions`)).
    (C6) 0 <= `lhs_contracting_dimensions[i]` < rank(`lhs`) for all `i`
    in [0, size(`lhs_contracting_dimensions`)).
    (C7) 0 <= `rhs_batching_dimensions[i]` < rank(`rhs`) for all `i`
    in [0, size(`rhs_batching_dimensions`)).
    (C8) 0 <= `rhs_contracting_dimensions[i]` < rank(`rhs`) for all `i`
    in [0, size(`rhs_contracting_dimensions`)).
    (C9) dim(`lhs`, `lhs_batching_dimensions[i]`) =
    dim(`rhs`, `rhs_batching_dimensions[i]`) for all `i` in [0,
    size(`lhs_batching_dimensions`)).
    (C10) dim(`lhs`, `lhs_contracting_dimensions[i]`) =
    dim(`rhs`, `rhs_contracting_dimensions[i]`) for all `i` in [0,
    size(`lhs_contracting_dimensions`)).
    (C11) size(`precision_config`) = 2.
    (C12) shape(`result`) = dim(`lhs`, `lhs_batching_dimensions`) +
    dim(`lhs`, `lhs_result_dimensions`) + dim(`rhs`, `rhs_result_dimensions`).
    (C14) element_type(`lhs`) = element_type(`rhs`).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) lhs is not a tensor. (Covered by ODS).
    I2: a) rhs is not a tensor. (Covered by ODS).
    I3: a) lhs_batching_dimensions is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(lhs_batching_dimesnions) != `si64`. (Covered by ODS).
    I4: a) rhs_batching_dimensions is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(rhs_batching_dimesnions) != `si64`. (Covered by ODS).
    I5: a) lhs_contracting_dimensions is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(lhs_contracting_dimensions) != `si64`. (Covered by ODS).
    I6: a) rhs_contracting_dimensions is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(rhs_contracting_dimensions) != `si64`. (Covered by ODS).
    I7: a) precision_config does not have variadic number of enum of `DEFAULT`, `HIGH`, and `HIGHEST`. (Covered by ODS).
    C1: a) size(lhs_batching_dimensions) != size(rhs_batching_dimensions).
    C2: a) size(lhs_contracting_dimensions) != size(rhs_contracting_dimensions).
    C3: a) lhs_batching_dimensions and lhs_contracting_dimensions combined are not unique.
    C4: a) rhs_batching_dimensions and rhs_contracting_dimensions combined are not unique.
    C5: a) lhs_batching_dimensions[i] < 0 for any i.
        b) lhs_batching_dimensions[i] >= rank(lhs) for any i.
    C6: a) lhs_contracting_dimensions[i] < 0 for any i.
        b) lhs_contracting_dimensions[i] >= rank(lhs) for any i.
    C7: a) rhs_batching_dimensions[i] < 0 for any i.
        b) rhs_batching_dimensions[i] >= rank(rhs) for any i.
    C8: a) rhs_contracting_dimensions[i] < 0 for any i.
        b) rhs_contracting_dimensions[i] >= rank(rhs) for any i.
    C9: a) dim(lhs, lhs_batching_dimensions[i]) != dim(rhs, rhs_batching_dimensions[i]) for any i.
    C10: a) dim(lhs, lhs_contracting_dimensions[i]) != dim(rhs, rhs_contracting_dimensions[i]) for any i.
    C11: a) size(precision_config) != 2.
    C12: no negative test needed since it's just inferring the shape.
    C14: a) element_type(lhs) != element_type(rhs).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: size(lhs_batching_dimensions) != size(rhs_batching_dimensions).
    C2a: size(lhs_contracting_dimensions) != size(rhs_contracting_dimensions).
    C3a: lhs_batching_dimensions and lhs_contracting_dimensions combined are not unique.
    C4a: rhs_batching_dimensions and rhs_contracting_dimensions combined are not unique.
    C5a: lhs_batching_dimensions[i] < 0 for any i.
    C5b: lhs_batching_dimensions[i] >= rank(lhs) for any i.
    C6a: lhs_contracting_dimensions[i] < 0 for any i.
    C6b: lhs_contracting_dimensions[i] >= rank(lhs) for any i.
    C7a: rhs_batching_dimensions[i] < 0 for any i.
    C7b: rhs_batching_dimensions[i] >= rank(rhs) for any i.
    C8a: rhs_contracting_dimensions[i] < 0 for any i.
    C8b: rhs_contracting_dimensions[i] >= rank(rhs) for any i.
    C9a: dim(lhs, lhs_batching_dimensions[i]) != dim(rhs, rhs_batching_dimensions[i]) for any i.
    C10a: dim(lhs, lhs_contracting_dimensions[i]) != dim(rhs, rhs_contracting_dimensions[i]) for any i.
    C11a: size(precision_config) != 2.
    C14a: element_type(lhs) != element_type(rhs).
    ```
    
    Notes:
    * (C14) currently does not have a test and consider removing it #755
    * (C11) currently does not have a test due to #755 and #879.
    
    closes #336

[33mcommit 0feb9842462ad5219f05154ac2b545a6fe440b45[m[33m ([m[1;33mtag: [m[1;33mv0.13.3[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jun 9 15:27:31 2023

    Integrate LLVM at llvm/llvm-project@309023263dba (#1604)

[33mcommit 2be4a820527392a57f51b77670343178284133a7[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jun 9 15:08:41 2023

    Update compatibility.md (#1606)
    
    We have recently approved #1600 - an RFC about compatibility. This PR
    updates compatibility.md - the source of truth for StableHLO's
    compatibility guarantees - to the latest plan of record.
    
    Also, this PR rolls back changes to the original compatibility RFC done
    in #1306. I now think that it was a mistake to retroactively update it,
    even if the plan of record changed - it is much easier to reason about
    RFCs if they are immutable.

[33mcommit b1bca6e49ffff58276c32929a2cbd8fe428877ac[m
Author: Scott Griffiths <dr.scottgriffiths@gmail.com>
Date:   Fri Jun 9 14:46:39 2023

    Correction to Min Normal bitstring for Float8E5M2FNUZ (#1605)
    
    The set bit was in the same place as for the E4M3 type - it should be
    the same as for Float8E5M2.

[33mcommit 5dd6550c3abcbe2b14a814a993b13efacd961f70[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 8 22:48:19 2023

    Add the 6 months compatibility RFC to rfcs/ for future reference (#1601)
    
    This PR adds "[RFC] Increase backward compatibility guarantees to 6
    months" (#1306) to rfcs/ to be consistent with other approved RFCs and
    preserve it for future reference.

[33mcommit f1ca38fa5c9294485fae0b90011cd39a785be893[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Jun 8 22:30:53 2023

    [RFC] Reduce StableHLO v1.0 Compatibility Guarantees (#1600)
    
    Cloned from OpenXLA Discuss [[RFC] Reduce StableHLO v1.0 Compatibility
    Guarantees](https://groups.google.com/a/openxla.org/g/openxla-discuss/c/yYjTDAsoygQ)

[33mcommit a5bce5ea7ac348a26c685e614dd70160df617776[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jun 8 20:42:55 2023

    Fail interpretation on Not in HLO and Miscellaneous ops (#1573)
    
    As we're getting closer and closer to completion of the intepreter
    thanks to great work by Gunhyun, our corner cases are becoming more and
    more prominent.
    
    One of these corner cases is support for Not in HLO ops, which we are
    planning to remove from the StableHLO opset but which are still part of
    the StableHLO dialect.
    
    Another corner case is support for Miscellaneous ops, all of which
    except for FftOp have decomposition passes implemented either on MHLO or
    HLO. Some of these decompositions are really tricky, and it would be a
    shame to duplicate their code and to maintain this duplicated code in
    the interpreter.
    
    As a result, I propose that we fail when encountering these ops and
    leave decomposition of these ops to be the responsibility of the users
    of the interpreter for now.
    
    In the future, we should provide for these ops in this repository (or
    move these ops out of the StableHLO opset). We can do this via a a
    combination of moving the already existing MLIR-based passes to this
    repository (and running them before the interpreter runs) and/or porting
    the already existing HLO-based passes to MLIR in this repository.
    
    This proposal also involves deleting the already implemented eval
    functions and tests for BatchNormGradOp, BatchNormInferenceOp,
    BatchNormTrainingOp and CholeskyOp. A lot of work went into these
    implementations (which will not be forgotten!), and I'm hesitating to
    remove them. However, this is a non-trivial amount of code and it may
    need non-trivial amount of maintenance down the line, so my
    recommendation would be to remove it.

[33mcommit e56a85b6744b4343dad645842b16a212300fcfb5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jun 8 20:33:32 2023

    Add `QuantizedElementType` to `NonValueType` grammar rule (#1599)
    
    Based on
    https://github.com/openxla/stablehlo/pull/1566#discussion_r1220886877,
    `QuantizedElementType`, being not a first class type in StableHLO,
    should be a part of `NonValueType` production rule.

[33mcommit 89e8bad5fac830bd799d7deac5fd33c5459b5593[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jun 8 01:59:58 2023

    Update spec and interpreter for ReduceWindowOp to properly calculate window_end (#1492)
    
    The current spec of ReduceWindowOp does not use `slice` with the same semantics
    as what `stablehlo.slice` does. `stablehlo.slice` version of `limit_indices`
    accounts for dilation and also complies with SliceOp's constraint: `(C3)
    0 <= start_indices[d] <= limit_indices[d] <= dim(operand, d) for all
    dimension d.`
    
    ReduceWindowOp's `limit_indices` calculation does not account for the
    dilation and (C3), so we need to add dilation (minus 1 since
    limit_indices is exclusive).
    
    This was discussed previously in the interpreter PR
    https://github.com/openxla/stablehlo/pull/1336#discussion_r1142826672.
    We kept the formulation as-is to assume that the program provides
    correct values, but we may not assume this is the case when we are
    explicitly constructing the input values.

[33mcommit 1df04bb1038e747ee5d2a0953b37f78ad89c758f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jun 7 23:24:28 2023

    Fix table spacing in the spec (#1598)

[33mcommit ff1e62cc9cdbc0f709fae39863896277722dec35[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jun 7 23:20:24 2023

    Enable passing testdata tests and tag failing floating-point precision tests (#1597)

[33mcommit f567eff199627599cdb19da199e761c790561126[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jun 6 23:27:02 2023

    Bump patch version after integrate 0.13.2 -> 0.13.3 (#1592)

[33mcommit 6f0d25ef2e05f4549c5e4ffc506436a4a13af51f[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 6 02:43:29 2023

    Compress a few more formulas using the ellipsis syntax (#1591)

[33mcommit ba2e7f224e4768abd32fa35795bdc0f9d8e95859[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jun 6 00:20:36 2023

    Tighten the formal notation that we're using in the spec (#1556)
    
    This PR proposes a more consistent notation for spec.md and describes it
    in the newly introduced "Notation" section. It doesn't address all
    feedback from #289, and some areas are deliberately left for future work
    but I think it's already an improvement.
    
    1) Introduces the ellipsis notation, to replace previously inconsistent
    use of colons in a similar role, to allow writing things like: `dim(lhs,
    lhs_batching_dimensions...) = dim(rhs, rhs_batching_dimensions...)`.
    
    As a side effect, this also gets rid of most of the `i0`, `i1`, etc,
    except in two places: a) when we mix scalar indices with colons, e.g. in
    GatherOp or ScatterOp, b) when we create function types from individual
    input/output types. In these situations, I believe that scalar indices
    represent the best tradeoff between conciseness and simplicity of
    notation.
    
    Rationale: A lot of formulas become a lot more concise thanks to the
    ellipsis notation, with most of "`<formula with some scalar>` for all
    `scalar` in `something`" formulas replaced with just `<formula with
    ellipses>`.
    
    2) Consistently switches to using programming syntax rather than LaTeX
    syntax, as discussed offline.
    
    Rationale: LaTeX syntax looks pretty, but it is not copypasteable when
    rendered on GitHub (huge problem) and is harder to read and write in
    Markdown (considerable problem).
    
    3) Consistently switches to the "`size(foo)`" rather than alternatives
    along the lines of "size(`foo`)".
    
    Rationale: This makes it considerably easier to read and write Markdown
    representation of formulas because fewer special symbols are involved.
    
    4) Prefers formulas like "`type(foo) = type(bar)`" to prose like "`foo`
    and `bar` have the same type" wherever practical. There are just a
    handful of exceptions where formulas would be very heaviweight.
    
    Rationale: This is the direction that the spec has already been leaning
    towards, I just helped that a little bit.
    
    5) Adds some new non-terminals and renames some existing ones to make
    sure that functions used in formulas derive cleanly from them.
    
    Rationale: I think that we need blanket rules along the lines of "if
    type's non-terminal is called FooType, then there is automatically a
    function called is_foo which is available for types and values". To make
    that happen, I tweaked the names a little bit.
    
    6) Retires the "`foo in [lo, hi)`" syntax in favor of the `lo <= foo <
    hi` syntax.
    
    Rationale: The former syntax only works for scalars, the latter syntax
    consistently applies both to scalars and tensors (via implicit
    broadcasting).
    
    7) Consistently switches all uses of `==` to use `=`.
    
    Rationale: This is the direction that the spec has already been leaning
    towards, I just helped that a little bit. Even though we use `=` both
    for comparisons and for assignment, these usages can be unambiguously
    discerned, so I propose we keep `=` since it's more concise.
    
    8) Introduces uniform syntax for an equivalent of switch expressions:
    "`foo` is defined as: * `value1` if `condition1 * `value2` otherwise".
    
    Rationale: There are several ways of writing this in the current version
    of the spec, so I figured it would be useful to unify.
    
    9) (This is a controversial one, and maybe I'm wrong here). Switches
    most of formulas like `foo > lo` to say `lo < foo` to be consistent with
    many formulas like `lo < bar < hi`.

[33mcommit 4344550b2a239705a0c5b236b1e6251822b5ead8[m[33m ([m[1;33mtag: [m[1;33mv0.13.2[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jun 5 15:28:05 2023

    Integrate LLVM at llvm/llvm-project@9d531c2dcfa7 (#1579)

[33mcommit 1677e2e1b2559236ba64fd9b22ed942569289457[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jun 2 18:04:50 2023

    Align the notation of spec and interpreter for Data Movement operations (#1554)
    
    Data Movement operations were one of the first operations specced, so we
    were still working out the notation back then.
    
    As mentioned in #484, inspired by our experience with the interpreter,
    it appears to be useful to express semantics of these operations in
    terms of result_index / operand_index rather than in terms of individual
    components of those indices.
    
    I took a stab at tweaking the specification for these operations, and I
    think that going from scalars to tensors to represent indices results in
    more readable specification. Furthermore, descriptive names like
    result_index are more conducive to readability than i or j. Also, we can
    retire the somewhat weird "id" / "jd" notation for these ops.
    
    Overall, I think that the specification and the implementation of data
    movement ops are well-aligned with each other. A few quirks:
    * ConcatenateOp, PadOp and TransposeOp are formulated in terms of
    operand's iteration space because they are much easier to implement this
    way. Everything else is formulated in terms of result's iteration space.
    * SortOp doesn't have a low-level 1:1 mapping, but there's a 1:1 mapping
    of high-level pieces and comments help with understanding.
    
    Re: the rest of the opset (see the list of categories in
    https://docs.google.com/spreadsheets/d/1rvhxQMFUtCZ5DsY6X0_lJOCg9rVO2MdyeZlRorsc0UI/edit?resourcekey=0-5gMjnlkXDL6hCntv2yltaQ#gid=0):
    * Control Flow ops look good as well (modulo a minor simplification in
    #1553).
      * Data Movement ops are discussed in this PR.
      * Distribution ops are in the works and will be reviewed afterwards.
    * Dynamism ops are in the works as well, and we'll first need to spec
    them before worrying about the interpreter. Overall, I'm not worried
    because at runtime there's not going to be a difference between static
    ops and their dynamic counterparts.
      * Elementwise ops are straightforward as far as notation goes.
    * Extensibility ops are out of scope for now because we may decide to
    remove tuples altogether (#598).
    * Most of the Miscellaneous ops are going to be handled via passes in
    the long run, so I haven't reviewed those. The rest, i.e. constant and
    iota, are fine with a minor fix to iota.
    * Not in HLO ops are going to be out of the opset in the long run, so we
    haven't specced them and aren't planning to spec or implement them.
    * Quantization ops are elementwise, so they are straightforward too as
    far as notation goes.
    * Finally, Reduction ops are going to be tricky because their specs have
    been tricky, their implementations have been tricky, and their notations
    have been drifting apart. I opened #1551 to revisit this once we're done
    with the first round of implementations.

[33mcommit d67da5d85045e03893684c35ae5e730695289ae2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jun 1 19:27:06 2023

    Bump patch version after integrate 0.13.1 -> 0.13.2 (#1568)

[33mcommit ce95c838096110f604437f323791b476cbde96d9[m[33m ([m[1;33mtag: [m[1;33mv0.13.1[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jun 1 18:18:20 2023

    Integrate LLVM at llvm/llvm-project@b9e328fd9113 (#1565)

[33mcommit 7ea0d7d20ade7746708137e16debf0dda73f8e5a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jun 1 15:55:08 2023

    Bump patch version after integrate 0.13.0 -> 0.13.1 (#1564)
    
    Integrated commit:
    https://github.com/openxla/xla/commit/767423c2c190ce25ece07a7ea4367263f2d6d6e9

[33mcommit 75c7095a97c6aaaee15dfab1fac529ce695e1d4a[m[33m ([m[1;33mtag: [m[1;33mv0.13.0[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 31 18:45:56 2023

    Bump MLIR Bytecode Format Version to 5 (#1559)
    
    Increment to use latest bytecode format per guidelines in: [vhlo.md >
    MLIR Bytecode Format
    Versions](https://github.com/openxla/stablehlo/blob/main/docs/vhlo.md#mlir-bytecode-format-versions).
    
    Includes version bump to v0.13.0 and test file generated using commands
    in: [vhlo.md > Add Versioned Serialization
    Test](https://github.com/openxla/stablehlo/blob/main/docs/vhlo.md#add-versioned-serialization-test).

[33mcommit 85926d8285df319e3822910043849b21126f0a9d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 31 14:51:03 2023

    Update compatibility.md to call out when minor version are bumped (#1542)
    
    Since recently, we also bump the minor version upon changes to the
    upstream bytecode version.

[33mcommit 85778ce41ceff56a09538f590d9bfbaca1cdf1c5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 31 14:50:22 2023

    Use -DLLVM_TARGETS_TO_BUILD=host in build_mlir.sh (#1545)
    
    Cuts down LLVM build time by 17% on my machine.
    
    Our CI scripts already used this option.

[33mcommit 7d797a27a7f949571f7a022d6d25f6f5f5a5e303[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 30 22:45:11 2023

    Update vhlo.md to include specific RUN lines for testing (#1560)
    
    We have slightly different tests in
    `stablehlo_legalize_to_vhlo.0_X_0.mlir` and
    `stablehlo_legalize_to_vhlo.mlir`, so this provides extra clarity on
    what the run lines should be.
    
    This was noticed by @ghpvnist.

[33mcommit 09bab1e05025394fa1d7720f6abc3ef903594636[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 30 20:14:35 2023

    Remove an implementation-defined test for GatherOp (#1550)
    
    In the removed test, one of the start_indices is 10 which is out of
    bounds for the operand, and that's currently specced as
    implementation-defined. Per interpreter checklist, we should remove this
    test.

[33mcommit e61d473f94c5e3c377e33c72e0c254fc663deed9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 30 18:38:02 2023

    Integrate LLVM at llvm/llvm-project@f81f32adc9a8 (#1558)

[33mcommit 6758a9c8fccafabd113b8e964843a298c51b1e83[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 30 18:12:24 2023

    Simplify evalWhileOp (#1553)
    
    This PR drops checking that evaluating WhileOp::cond produces exactly
    one result. This is guaranteed by the verifier, so this is not something
    that we should be checking in the interpreter.

[33mcommit e6ef63c741378281d032715aee3a3fd2759a9f4b[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 30 15:34:20 2023

    Use func.func and func.return in the spec (#1555)
    
    The stablehlo.func and stablehlo.return syntax in the opening example of
    the spec was aspirational, and at the time we expected that we'll soon
    adopt it.
    
    A lot of the time has passed, and we still haven't gotten around to
    doing this, while we've been getting regular questions about this. Let's
    reflect reality in the example for now - we can always change it back
    once we address #425.

[33mcommit 4f5451533bbe5a87731fdd1cc866a81076377ecd[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 30 15:30:13 2023

    Bump patch version after integrate 0.12.0 -> 0.12.1 (#1548)

[33mcommit b8c6b90b692d5552742a9167adb1a5bb6858f693[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 26 23:57:22 2023

    Compress the code a little bit (#1543)
    
    When reviewing the ScatterOp PR, I saw a few opportunities for making
    the code a bit more compressed.

[33mcommit e169d26ccfce28ca45d5b8d5a39734eca81427e3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri May 26 23:39:18 2023

    Add interpreter for ScatterOp (#1488)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `inputs` variadic number of tensors.
    (I2) `scatter_indices` tensor of integer type.
    (I3) `updates` variadic number of tensors.
    (I4) `update_window_dims` 1-dimensional tensor constant of type `si64`.
    (I5) `inserted_window_dims` 1-dimensional tensor constant of type `si64`.
    (I6) `scatter_dims_to_operand_dims` 1-dimensional tensor constant of type `si64`.
    (I7) `index_vector_dim` constant of type `si64`.
    (I8) `indices_are_sorted` constant of type `i1`.
    (I9) `unique_indices` constant of type `i1`.
    (I10) `update_computation` function.
    (C1) All `inputs` have the same shape.
    (C2) rank(`inputs`[0]) = size(`update_window_dims`) +
         size(`inserted_window_dims`).
    (C3) All `updates` have the same shape.
    (C4) `shape(updates[0])` =
          `combine(update_scatter_dim_sizes, update_window_dim_sizes)` where:
    * `update_scatter_dim_sizes` = `shape(scatter_indices)` except that
      the dimension size of `scatter_indices` corresponding to
      `index_vector_dim` is not included.
    * `update_window_dim_sizes` <= `shape(inputs[0])` except that
      the dimension sizes in `inputs[0]` corresponding to `inserted_window_dims`
      are not included.
    * `combine` puts `update_scatter_dim_sizes` at axes corresponding to
     `update_scatter_dims` and `update_window_dim_sizes` at axes corresponding
     to `update_window_dims`.
    (C5) N = size(`inputs`) = size(`updates`) and N >= 1.
    (C6) `element_type(updates[k]) = element_type(inputs[k])` for all k $\in$
         [0, N).
    (C7) All dimensions in `update_window_dims` are unique and sorted.
    (C8) For all i in [0, size(`update_window_dims`)), 0 <=
    `update_window_dims`[i] < rank(`updates`[0]).
    (C9) All dimensions in `inserted_window_dims` are unique and sorted.
    (C10) For all i in [0, size(`inserted_window_dims`)), 0 <=
    `inserted_window_dims`[i] < rank(`inputs`[0]).
    (C11) size(`scatter_dims_to_operand_dims`) =
         `index_vector_dim` < rank(`scatter_indices`) ?
         dim(`scatter_indices`, `index_vector_dim`) : 1.
    (C12) All dimensions in `scatter_dims_to_operand_dims` are unique.
    (C13) For all i in [0, size(`scatter_dims_to_operand_dims`)), 0 <=
        `scatter_dims_to_operand_dims`[i] < rank(`inputs`[0]).
    (C14) 0 <= `index_vector_dim` <= rank(`scatter_indices`).
    (C15) `update_computation` has type
          `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
          where `Ek = element_type(inputs[k])` for all k in [0, N).
    (C16) `inputs[k]` and `result[k]` have the same type for all k in [0, N).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `inputs` variadic number of tensors. (Covered by ODS).
    I2: a) `scatter_indices` tensor of integer type. (Covered by ODS).
    I3: a) `updates` variadic number of tensors. (Covered by ODS).
    I4: a) `update_window_dims` 1-dimensional tensor constant of type `si64`. (Covered by ODS).
    I5: a) `inserted_window_dims` 1-dimensional tensor constant of type `si64`. (Covered by ODS).
    I6: a) `scatter_dims_to_operand_dims` 1-dimensional tensor constant of type `si64`. (Covered by ODS).
    I7: a) `index_vector_dim` constant of type `si64`. (Covered by ODS).
    I8: a) `indices_are_sorted` constant of type `i1`. (Covered by ODS).
    I9: a) `unique_indices` constant of type `i1`. (Covered by ODS).
    I10: a) `update_computation` function. (Covered by ODS).
    C1: a) Not all `inputs` have the same shape.
    C2: a) rank(`inputs`[0]) != size(`update_window_dims`) +
           size(`inserted_window_dims`).
    C3: a) Not all `updates` have the same shape.
    C4: a) `shape(updates[0])` !=
          `combine(update_scatter_dim_sizes, update_window_dim_sizes)`.
        b) `update_scatter_dim_sizes` != `shape(scatter_indices)` except that the
           dimension size of `scatter_indices` corresponding to `index_vector_dim`
           is not included.
        c) `update_window_dim_sizes` <= `shape(inputs[0])` except that the dimension
           sizes in `inputs[0]` corresponding to `inserted_window_dims` are not
           included.
        where `combine` puts `update_scatter_dim_sizes` at axes corresponding to
        `update_scatter_dims` and `update_window_dim_sizes` at axes corresponding to
        `update_window_dims`.
    C5: a) N != size(`inputs`). (Covered by ODS).
        b) N != size(`updates`). (Covered by ODS).
        c) N < 1. (Covered by ODS).
    C6: a) `element_type(updates[k]) != element_type(inputs[k])` for any k in [0, N).
    C7: a) Dimensions in `update_window_dims` are not unique.
        b) Dimensions in `update_window_dims` are not sorted.
    C8: a) For any i in [0, size(`update_window_dims`)), `update_window_dims`[i] < 0.
        b) For any i in [0, size(`update_window_dims`)), `update_window_dims`[i] >= rank(`updates`[0]).
    C9: a) Dimensions in `inserted_window_dims` are not unique.
        b) Dimensions in `inserted_window_dims` are not sorted.
    C10: a) For any i in [0, size(`inserted_window_dims`)), `inserted_window_dims`[i] < 0.
         b) For any i in [0, size(`inserted_window_dims`)), >= rank(`inputs`[0]).
    C11: a) size(`scatter_dims_to_operand_dims`) !=
         `index_vector_dim` < rank(`scatter_indices`) ?
         dim(`scatter_indices`, `index_vector_dim`) : 1.
    C12: a) Dimensions in `scatter_dims_to_operand_dims` are not unique.
    C13: a) For any i in [0, size(`scatter_dims_to_operand_dims`)), `scatter_dims_to_operand_dims`[i] < 0.
         b) For any i in [0, size(`scatter_dims_to_operand_dims`)), `scatter_dims_to_operand_dims`[i] >= rank(`inputs`[0]).
    C14: a) `index_vector_dim` < 0.
         b) `index_vector_dim` > rank(`scatter_indices`).
    C15: a) `update_computation` does not have type
            `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
            where `Ek = element_type(inputs[k])` for any k $\in$ [0, N).
    C16: a) type(`inputs[k]`) != type(`result[k]`) for any k $\in$ [0, N).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: Not all `inputs` have the same shape.
    C2a: rank(`inputs`[0]) != size(`update_window_dims`) + size(`inserted_window_dims`).
    C3a: Not all `updates` have the same shape.
    C4a: `shape(updates[0])` !=
         `combine(update_scatter_dim_sizes, update_window_dim_sizes)`.
    C4b: `update_scatter_dim_sizes` != `shape(scatter_indices)` except that the
         dimension size of `scatter_indices` corresponding to `index_vector_dim`
         is not included.
    C4c: `update_window_dim_sizes` <= `shape(inputs[0])` except that the dimension
         sizes in `inputs[0]` corresponding to `inserted_window_dims` are not
         included.
         where `combine` puts `update_scatter_dim_sizes` at axes corresponding to
         `update_scatter_dims` and `update_window_dim_sizes` at axes corresponding to
         `update_window_dims`.
    C6a: `element_type(updates[k]) != element_type(inputs[k])` for any k in [0, N).
    C7a: Dimensions in `update_window_dims` are not unique.
    C7b: Dimensions in `update_window_dims` are not sorted.
    C8a: For any i in [0, size(`update_window_dims`)), `update_window_dims`[i] < 0.
    C8b: For any i in [0, size(`update_window_dims`)), `update_window_dims`[i] >= rank(`updates`[0]).
    C9a: Dimensions in `inserted_window_dims` are not unique.
    C9b: Dimensions in `inserted_window_dims` are not sorted.
    C10a: For any i in [0, size(`inserted_window_dims`)), `inserted_window_dims`[i] < 0.
    C10b: For any i in [0, size(`inserted_window_dims`)), >= rank(`inputs`[0]).
    C11a: size(`scatter_dims_to_operand_dims`) !=
          `index_vector_dim` < rank(`scatter_indices`) ?
          dim(`scatter_indices`, `index_vector_dim`) : 1.
    C12a: Dimensions in `scatter_dims_to_operand_dims` are not unique.
    C13a: For any i in [0, size(`scatter_dims_to_operand_dims`)), `scatter_dims_to_operand_dims`[i] < 0.
    C13b: For any i in [0, size(`scatter_dims_to_operand_dims`)), `scatter_dims_to_operand_dims`[i] >= rank(`inputs`[0]).
    C14a: `index_vector_dim` < 0.
    C14b: `index_vector_dim` > rank(`scatter_indices`).
    C15a: `update_computation` does not have type
          `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
          where `Ek = element_type(inputs[k])` for any k $\in$ [0, N).
    C16a: type(`inputs[k]`) != type(`result[k]`) for any k $\in$ [0, N).
    ```
    
    Notes:
      * Some missing verifications were added.
      * Updates typo in spec wording (i.e. For any k -> For all k).
      * Updates notation `do` -> `di`, `ds` -> `dj`
    
    closes #987

[33mcommit aac89de63ba020d26b7afad7af6b127e7184890c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri May 26 01:37:53 2023

    Add interpreter for GatherOp (#1058)
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand tensor.
    (I2) start_indices tensor of integer type.
    (I3) offset_dims 1-dimensional tensor constant of type `si64`.
    (I4) collapsed_slice_dims 1-dimensional tensor constant of type `si64`.
    (I5) start_index_map 1-dimensional tensor constant of type `si64`.
    (I6) index_vector_dim constant of type `si64`.
    (I7) slice_sizes 1-dimensional tensor constant of type `si64`.
    (I8) indices_are_sorted constant of type `i1`.
    (C1) rank(`operand`) $=$ size(`offset_dims`) $+$
         size(`collapsed_slice_dims`).
    (C2) $0 \le$ `index_vector_dim` $\le$ rank(`start_indices`).
    (C3) size(`start_index_map`) $=$
         `index_vector_dim` $\lt$ rank(`start_indices`) ?
         dim(`start_indices`, `index_vector_dim`) : 1.
    (C4) All dimensions in `offset_dims` are unique and sorted in ascending
         order.
    (C5) $0 \le$ `offset_dims`[i] $\lt$ rank(`result`) $\forall i$
         such that $0 \le$ i $\lt$ size(`offset_dims`).
    (C6) All dimensions in `collapsed_slice_dims` are unique and sorted in
         ascending order.
    (C7) $0 \le$ `collapsed_slice_dims`[i] $\lt$ size(`slice_sizes`)
          $\forall i$ such that $0 \le$ i $\lt$ size(`collapsed_slice_dims`).
    (C8) `slice_sizes`[i] $\le$ 1 $\forall i \in$ `collapsed_slice_dims`.
    (C9) All dimensions in `start_index_map` are unique.
    (C10) $0 \le$ `start_index_map`[i] $\lt$ rank(`operand`) $\forall i$
         such that $0 \le$ i $\lt$ size(`start_index_map`).
    (C11) size(`slice_sizes`) $=$ rank(`operand`).
    (C12) $0 \le$ `slice_sizes`[i] $\le$ dim(`operand`, i) $\forall i$
          such that $0 \le$ i $\lt$ size(`slice_sizes`).
    (C13) `shape(result)` $=$ `combine(batch_dim_sizes, offset_dim_sizes)`
          where:
          * `batch_dim_sizes` = `shape(start_indices)` except that the dimension size
            of `start_indices` corresponding to `index_vector_dim` is not included.
          * `offset_dim_sizes` = `shape(slice_sizes)` except that the dimension sizes
            in `slice_sizes` corresponding to `collapsed_slice_dims` are not included.
          * `combine` puts `batch_dim_sizes` at axes corresponding to `batch_dims` and
            `offset_dim_sizes` at axes corresponding to `offset_dims`.
    (C14) `operand` and `result` have the same element type.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) start_indices is not a tensor of integer type. (Covered by ODS).
    I3: a) offset_dims is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(offset_dims) != `si64`. (Covered by ODS).
    I4: a) collapsed_slice_dims is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(collapsed_slice_dims) != `si64`. (Covered by ODS).
    I5: a) start_index_map is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(start_index_map) != `si64`. (Covered by ODS).
    I6: a) type(index_vector_dim) != `si64`. (Covered by ODS).
    I7: a) slice_sizes is not a 1-dimensional tensor.
        b) element_type(slice_sizes) != `si64`. (Covered by ODS).
    I8: a) element_type(indices_are_sorted) != `i1`. (Covered by ODS).
    (C1) a) rank(operand) != size(offset_dims) + size(collapsed_slice_dims).
    (C2) a) index_vector_dim < 0.
         b)  index_vector_dim > rank(start_indices).
    (C3) a) size(start_index_map) != dim(start_indices, index_vector_dim) if index_vector_dim < rank(start_indices).
         b) size(start_index_map) != 1 if index_vector_dim >= rank(start_indices).
    (C4) a) offset_dims values are not unique.
         b) offset_dims values are not sorted in ascending order.
    (C5) a) offset_dims[i] < 0 for any i.
         b) offset_dims[i] >= rank(result) for any i.
    (C6) a) collapsed_slice_dims values are not unique.
         b) collapsed_slice_dims are not sorted in ascending order.
    (C7) a) collapsed_slice_dims[i] < 0 for any i.
         b) collapsed_slice_dims[i] >= size(slice_sizes) for any i.
    (C8) a) slice_sizes[i] > 1 for any i in collapsed_slice_dims.
    (C9) a) start_index_map values are not unique.
    (C10) a) start_index_map[i] < 0 for any i.
          b) start_index_map[i] >= rank(operand) for any i.
    (C11) a) size(slice_sizes) != rank(operand).
    (C12) a) slice_sizes[i] < 0 for any i.
          b) slice_sizes[i] > dim(operand, i) for any i.
    (C13) no negative test needed since it's just inferring the shape.
    (C14) element_type(operand) !=  element_type(result). (Covered by ODS).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I7a: slice_sizes is not a 1-dimensional tensor.
    C1a: rank(operand) != size(offset_dims) + size(collapsed_slice_dims).
    C2a: index_vector_dim < 0.
    C2b: index_vector_dim > rank(start_indices).
    C3a: size(start_index_map) != dim(start_indices, index_vector_dim) if index_vector_dim < rank(start_indices).
    C3b: size(start_index_map) != 1 if index_vector_dim >= rank(start_indices).
    C4a: offset_dims values are not unique.
    C4b: offset_dims values are not sorted in ascending order.
    C5a: offset_dims[i] < 0 for any i.
    C5b: offset_dims[i] >= rank(result) for any i.
    C6a: collapsed_slice_dims values are not unique.
    C6b: collapsed_slice_dims are not sorted in ascending order.
    C7a: collapsed_slice_dims[i] < 0 for any i.
    C7b: collapsed_slice_dims[i] >= size(slice_sizes) for any i.
    C8a: slice_sizes[i] > 1 for any i in collapsed_slice_dims.
    C9a: start_index_map values are not unique.
    C10a: start_index_map[i] < 0 for any i.
    C10b: start_index_map[i] >= rank(operand) for any i.
    C11a: size(slice_sizes) != rank(operand).
    C12a: slice_sizes[i] < 0 for any i.
    C12b: slice_sizes[i] > dim(operand, i) for any i.
    ```
    
    Also fixed typo (C15) -> (C14)
    
    closes #976

[33mcommit bcf050a2e920702e4b63a481087b9f1980020226[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri May 26 01:21:34 2023

    Add interpreter for BatchNormGradOp (#1394)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor of floating-point type.
    (I2) `scale`: 1-dimensional tensor of floating-point type.
    (I3) `mean`: 1-dimensional tensor of floating-point type.
    (I4) `variance`: 1-dimensional tensor of floating-point type.
    (I5) `grad_output`: tensor of floating-point type.
    (I6) `epsilon`: constant of type `f32`.
    (I7) `feature_index`: constant of type `si64`.
    (C1) 0 <= `feature_index` < rank(`operand`).
    (C2) `operand`, `scale`, `mean`, `variance`, `grad_output`, `grad_operand`
         `grad_scale` and `grad_offset` have the same element type.
    (C3) `operand`, `grad_output` and `grad_operand` have the same shape.
    (C4) `scale`, `mean`, `variance`, `grad_scale` and `grad_offset` have the
         same shape.
    (C5) size(`scale`) = `dim(operand, feature_index)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor of floating-point type. (Covered by ODS).
    I2: a) `scale` is not a 1-dimensional tensor. (Covered by ODS).
        b) `scale` is not a tensor of floating-point type. (Covered by ODS).
    I3: a) `mean` is not a 1-dimensional tensor. (Covered by ODS).
        b) `mean` is not a tensor of floating-point type. (Covered by ODS).
    I4: a) `variance` is not a 1-dimensional tensor. (Covered by ODS).
        b) `variance` is not a tensor of floating-point type. (Covered by ODS).
    I5: a) `grad_output` is not a tensor of floating-point type. (Covered by ODS).
    I6: a) `epsilon` is not a constant of type `f32`. (Covered by ODS).
    I7: a) `feature_index` is not a constant of type `si64`. (Covered by ODS).
    C1: a) `feature_index` < 0.
        b) `feature_index` >= rank(`operand`).
    C2: a) element_type(`operand`) != element_type(`scale`). (Covered by ODS).
        b) element_type(`operand`) != element_type(`mean`). (Covered by ODS).
        c) element_type(`operand`) != element_type(`variance`). (Covered by ODS).
        d) element_type(`operand`) != element_type(`grad_output`). (Covered by ODS).
        e) element_type(`operand`) != element_type(`grad_operand`). (Covered by ODS).
        f) element_type(`operand`) != element_type(`grad_scale`). (Covered by ODS).
        g) element_type(`operand`) != element_type(`grad_offset`). (Covered by ODS).
    C3: a) shape(`operand`) != shape(`grad_output`).
        b) shape(`operand`) != shape(`grad_operand`).
    C4: a) shape(`scale`) != shape(`mean`).
        b) shape(`scale`) != shape(`variance`).
        c) shape(`scale`) != shape(`grad_scale`).
        d) shape(`scale`) != shape(`grad_offset`).
    C5: a) size(`scale`) != dim(operand, feature_index).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: `feature_index` < 0.
    C1b: `feature_index` >= rank(`operand`).
    C3a: shape(`operand`) != shape(`grad_output`).
    C3b: shape(`operand`) != shape(`grad_operand`).
    C4a: shape(`scale`) != shape(`mean`).
    C4b: shape(`scale`) != shape(`variance`).
    C4c: shape(`scale`) != shape(`grad_scale`).
    C4d: shape(`scale`) != shape(`grad_offset`).
    C5a: size(`scale`) != dim(operand, feature_index).
    ```
    
    Notes:
    * Added `i6` in the spec to better align with the spec comments
    referring to
    [batchnorm_expander.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/batchnorm_expander.cc#L521-L524)
    in XLA.
    * `size` -> `num_elements` to be consistent with what's written in
    `compute_mean` function.
    
    closes #1121

[33mcommit aedc509bfbb9f17a02afb068cb413d2981215541[m[33m ([m[1;33mtag: [m[1;33mv0.12.0[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu May 25 21:26:42 2023

    Move chlo::TopKOp type inference into TypeInference.h (#1536)
    
    This is needed to support the work on introducing mhlo::TopKOp. MLIR-HLO
    commit:
    https://github.com/tensorflow/mlir-hlo/commit/4651ac2e8375b706643fdab809e0bc30a7ecd666

[33mcommit 7d992e65c0cd67b79ac81c2eb39c4b6e7b146df6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu May 25 21:25:23 2023

    Improve documentation for getMinimumVersion (#1537)
    
    During review of https://github.com/google/jax/pull/16081, we received
    feedback that the name `get_minimum_version` is not very intuitive.
    
    While we haven't yet come up with a better name, improving documentation
    is the second best thing that we can do.

[33mcommit accf3fa9cff539e9c093c9ce0851741daf926fa8[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 25 20:47:43 2023

    Integrate LLVM at llvm/llvm-project@e837f4b7 (#1540)
    
    Need to clean up `llvm_disable_optional_support_deps` after:
    https://reviews.llvm.org/D151006

[33mcommit 468f7bd02bba1520530cad41c8d0124b01908bc9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 24 21:51:24 2023

    Add interpreter for BatchNormTrainingOp (#1393)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor of floating-point type.
    (I2) `scale`: 1-dimensional tensor of floating-point type.
    (I3) `offset`: 1-dimensional tensor of floating-point type.
    (I4) `epsilon`: constant of type `f32`.
    (I5) `feature_index`: constant of type `si64`.
    (C1) 0 <= `feature_index` < rank(`operand`).
    (C2) `operand`, `scale`, `offset`, `result`, `batch_mean` and `batch_var`
         have the same element type.
    (C3) size(`scale`) = `dim(operand, feature_index)`.
    (C4) size(`offset`) = `dim(operand, feature_index)`.
    (C5) size(`batch_mean`) = `dim(operand, feature_index)`.
    (C6) size(`batch_var`) = `dim(operand, feature_index)`.
    (C7) `operand` and `output` have the same type.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor of floating-point type. (Covered by ODS).
    I2: a) `scale` is not a 1-dimensional tensor. (Covered by ODS).
        b) `scale` is not a tensor of floating-point type. (Covered by ODS).
    I3: a) `offset` is not a 1-dimensional tensor. (Covered by ODS).
        b) `offset` is not a tensor of floating-point type. (Covered by ODS).
    I4: a) `epsilon` is not a constant of type `f32`. (Covered by ODS).
    I5: a) `feature_index` is not a constant of type `si64`. (Covered by ODS).
    C1: a) `feature_index` < 0.
        b) `feature_index` >= rank(`operand`).
    C2: a) element_type(`operand`) != element_type(`scale`). (Covered by ODS).
        b) element_type(`operand`) != element_type(`offset`). (Covered by ODS).
        c) element_type(`operand`) != element_type(`result`). (Covered by ODS).
        d) element_type(`operand`) != element_type(`batch_mean`). (Covered by ODS).
        e) element_type(`operand`) != element_type(`batch_var`). (Covered by ODS).
    C3: a) size(`scale`) != dim(operand, feature_index).
    C4: a) size(`offset`) != dim(operand, feature_index).
    C5: a) size(`batch_mean`) != dim(operand, feature_index).
    C6: a) size(`batch_var`) != dim(operand, feature_index).
    C7: a) type(`operand`) != type(`output`).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: `feature_index` < 0.
    C1b: `feature_index` >= rank(`operand`).
    C3a: size(`scale`) != dim(operand, feature_index).
    C4a: size(`offset`) != dim(operand, feature_index).
    C5a: size(`batch_mean`) != dim(operand, feature_index).
    C6a: size(`batch_var`) != dim(operand, feature_index).
    C7a: type(`operand`) != type(`output`).
    ```
    
    closes #1122

[33mcommit 6a5ee09907ff69be3f80e6d43561591566a577f4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 24 17:48:06 2023

    Simplify evalConvertOp (#1532)
    
    This PR introduces `Element convert(Type type, const Element &e)` which
    considerably simplifies `evalConvertOp` by hiding type-based dispatch in
    Element.h, consistently with many other implementations of evalFooOp
    functions.

[33mcommit 40e6532da4fe633edc4d0b3127f2d7e2a981c280[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed May 24 16:37:16 2023

    Bump MLIR Bytecode Format Version (#1534)
    
    Increment to use latest bytecode format per guidelines in: [vhlo.md >
    MLIR Bytecode Format
    Versions](https://github.com/openxla/stablehlo/blob/main/docs/vhlo.md#mlir-bytecode-format-versions).
    
    Includes version bump to v0.12.0 and test file generated using commands
    in: [vhlo.md > Add Versioned Serialization
    Test](https://github.com/GleasonK/stablehlo/pull/new/bytecode-format-version-bump).

[33mcommit 53a76fa57d8045373e608bb64f5e2dee3c4183c2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed May 24 15:17:12 2023

    Delete testdata bytecode tests (#1425)
    
    Recently we gathered some code coverage metrics for testdata tests, and
    compared that to our unit tests.
    
    It turns out that our testdata coverage was a strict subset of unit
    tests, including in generated files, which makes them not particularly
    useful as serialization and deserialization tests, and minimally useful
    as versioned semantic compatibility tests, but very useful as testdata
    for StableHLO users.
    
    For semantic compatibility, this approach does not scale well due to the
    high redundancy in test cases. The overhead for cloning all these files
    at each StableHLO version bump is too high for its limited value (25MB
    of bytecode files per version, difficult to review due to PR size,
    consistently breaks our tooling). We have created #1416 to rethink
    semantic compatibility testing, and are planning to only version
    `stablehlo_legalize_to_vhlo.mlir` for serialization testing, which alone
    has 95% coverage of compatibility machinery, covering all ops, types,
    and attributes.
    
    
    This PR was created using the following commands (meaning it should be
    uniform):
    
    ```
    cd stablehlo/testdata
    rm *.mlir.bc
    sed -i 's/stablehlo-translate.*--interpret/stablehlo-opt -inline %s | stablehlo-translate --interpret/' *.mlir
    sed -i "/0_9_0/d" *.mlir
    ```
    
    **EDIT 5/24: Alternatives considered and mitigations**
    
    A primary use case for testdata on top of reference interpreter testing,
    is to provide copy-pastable snippets for StableHLO users to test their
    backend implementations against. As such, maintaining some form of
    textual assembly format is a requirement. The primary alternative
    considered was to remove the MLIR files and only have bytecode files.
    This is more stable, and there is an MLIR vscode extension that permits
    viewing and editing bytecode files in place. This solution was not
    chosen for two main reasons - 1. Serialized portable artifacts are VHLO,
    meaning the decoded file would not be a copy-pastable snippet, as IR
    upgrades and conversion to StableHLO are still required. 2. This
    extension does not work on github, meaning files would need to be opened
    in VSCode or deserialized using an opt tool.
    
    The downside of only preserving text files is that StableHLO portable
    artifacts have stability, meaning they will not break between releases.
    Textual assembly format may break. In the case of assembly format
    changes, we may have a large amount of testdata that breaks and requires
    fixing. This overhead can be automated away with a script that checks
    out a known good release, generates bytecode files, and deserializes at
    HEAD to update the testdata. This script is tracked in #1533.

[33mcommit 509f3eb0d337cce5b63d7966f4dd6f26530eda32[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 24 04:59:14 2023

    Simplify verification of ReduceOp and ReduceWindowOp (#1467)
    
    While reviewing PRs that implement interpreters for ReduceOp and
    ReduceWindowOp, I noticed that the verifiers can be somewhat simplified.
    This PR does the necessary cleanup.

[33mcommit 2ef30c7ee3057ff3a3951a53bdc2442200dc3114[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 23 18:27:34 2023

    Update CustomCallOp status to `yes` and update ODS (#1521)
    
    We already have an implementation to handle custom_call ops by passing a
    fallback function to handle them. This PR updates the status to reflect
    that. Other minor change include updating the example in td file to use
    pretty-print format following the
    [guide](https://github.com/openxla/stablehlo/blob/main/docs/reference.md#testing-guidelines).

[33mcommit ad9d815007ed27fe14bbdf1a44c2d4827711790f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 23 17:50:04 2023

    Tag issue for failing tests for supported ops (#1438)
    
    One of the
    [checklist](https://github.com/openxla/stablehlo/blob/main/docs/reference_checklist.md)
    items is to tag tests failing due to floating-point differences with an
    issue #1278. So far, this has not been done, so this PR applies them in
    bulk.
    
    These tests are generated by:
    1. Get all .mlir tests from `testdata/`.
    2. Filter tests containing only supported ops.
    3. Remove `-DISABLED` from filtered tests.
    4. Run the test.
    5. Diff the list of failing tests with step 2.
    6. Replace `-DISABLED` with `-DISABLED(#1278)` if `(#1278)` not already
    present from step 5.

[33mcommit c576c2fb4211332a57bb395bfaa57bf414e15b4a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 23 17:20:30 2023

    Bump patch version after integrate 0.11.7 -> 0.11.8 (#1528)

[33mcommit a8cb1c747f5a54db0abf0f668ad9eb3ed47c27a6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 23 16:35:20 2023

    Fix issues identified during integrate (#1527)
    
    * BUILD.bazel: add missing dependencies to stablehlo-translate.
      * CMakeLists.txt: same.

[33mcommit fab5f24fe96b428dc746ce175dfef06c54285302[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 23 16:34:45 2023

    Bump patch version after integrate 0.11.7 -> 0.11.8 (#1528)

[33mcommit f556890d68c2da9ed93218959449e54e4858e460[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 23 16:34:33 2023

    Bump patch version after integrate 0.11.7 -> 0.11.8 (#1528)

[33mcommit 9fa406730f46c5062b82776acf1db8e8bf007417[m
Author: anakinxc <103552181+anakinxc@users.noreply.github.com>
Date:   Tue May 23 05:04:37 2023

    Fix build error on macOS (#1529)
    
    This is the same as #1293, but on newly added code.

[33mcommit a505ee5d96ee1860e975f69b26d2bcc1c51dd581[m[33m ([m[1;33mtag: [m[1;33mv0.11.7[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 23 01:46:42 2023

    Integrate LLVM at llvm/llvm-project@27eadeee6b45 (#1526)

[33mcommit b357106f7cdc51da144ffe033459944b1965c876[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 23 01:37:22 2023

    Add stop-gap static forward compatibility tests (#1525)
    
    Use https://github.com/openxla/stablehlo/pull/1524 as diffbase.
    
    This is a stop-gap measure to improve the detection of forward
    incompatibilities in the StableHLO repo, and repos where StableHLO is
    exported like openxla/xla, while the Forward Compatibility Testing RFC
    (https://github.com/openxla/stablehlo/pull/1498) is reviewed. These
    tests will be reworked based on the outcome of the RFC review.
    
    The forward compatibility test is a byte-wise comparison using:
    
    ```bash
    # %s = stablehlo/tests/stablehlo_legalize_to_vhlo.0_10_0.mlir
    diff %s.bc <(stablehlo-translate --serialize --target=0.10.0 --strip-debuginfo %s)
    ```

[33mcommit 5f89cf92e1ae3185ad064913814ce8e688435e1c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 23 01:34:07 2023

    Update bytecode docs to mention MLIR Bytecode Format and StableHLO releases (#1522)
    
    Placed in VHLO markdown file since this is more implementation detail
    than user-facing documentation.
    
    The plan is to increment the minor release of StableHLO whenever MLIR
    Bytecode Format updates, so we can keep StableHLO closely tied to the
    latest MLIR Bytecode Format. This also allows us to provide more strict
    forward compatibility requirements. I.e. we have no dependency on old
    bytecode versions a month after the newer bytecode version was adopted.

[33mcommit 9d2ac314f25ba886a20232ccb0415f25816e980c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue May 23 01:20:52 2023

    Update bytecode artifacts to remove debuginfo and include StableHLO producer string (#1524)
    
    Note in the below examples, the suffix `-09`, `-010`, `-011` refers to
    tooling at the StableHLO@v0.X.0 release:
    
    ```bash
    # for `-09`:
    git checkout v0.9.0
    # Checkout LLVM at `./build_tools/llvm_version.txt` version
    # Build LLVM and StableHLO
    ```
    
    Commands used to generate the files below.
    
    ```
    stablehlo-opt-v09 stablehlo/tests/stablehlo_legalize_to_vhlo.0_9_0.mlir --strip-debuginfo --stablehlo-legalize-to-vhlo --vhlo-to-version='target=0.9.0' --emit-bytecode | sed 's/MLIR17.0.0git/StableHLO_v0.9.0/' > /tmp/stablehlo_legalize_to_vhlo.0_9_0.mlir.bc
    stablehlo-opt-v010 stablehlo/tests/stablehlo_legalize_to_vhlo.0_10_0.mlir --strip-debuginfo --emit-bytecode | stablehlo-translate-v010 --serialize --target=0.10.0 | sed 's/MLIR17.0.0git/StableHLO_v0.10.0/' > /tmp/stablehlo_legalize_to_vhlo.0_10_0.mlir.bc
    stablehlo-opt-v011 stablehlo/tests/stablehlo_legalize_to_vhlo.0_11_0.mlir --strip-debuginfo --emit-bytecode | stablehlo-translate-v011 --serialize --target=0.11.0 | sed 's/MLIR17.0.0git/StableHLO_v0.11.0/' > /tmp/stablehlo_legalize_to_vhlo.0_11_0.mlir.bc
    ```
    
    This is the equivalent way at the time of each release to generate what
    the following command does today:
    
    ```
    stablehlo-translate --serialize --target=0.X.0 --strip-debuginfo
    ```
    
    Lastly, `--strip-debuginfo` is added to the diff check comparison since
    ReduceOp prettyprinting currently depends on debug info, and the
    serialized artifacts do not have debuginfo:
    https://github.com/openxla/stablehlo/blob/4cd6f24257a364857add0e3c1dc11b2364669d50/stablehlo/dialect/StablehloOps.cpp#L1489-L1490
    
    This is an issue that will be addressed separately:
    https://github.com/openxla/stablehlo/issues/1523

[33mcommit 4cd6f24257a364857add0e3c1dc11b2364669d50[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon May 22 23:43:21 2023

    Update the link to serialization APIs in compatibility.md (#1515)
    
    Now that we have these APIs documented right in this file, let's link
    directly to it.

[33mcommit f2472e6d2ab47c9e6594aff6508fa1c66e711ce6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon May 22 23:42:32 2023

    Bump patch version after integrate 0.11.6 -> 0.11.7 (#1513)

[33mcommit 428d88ddc5e94e246f3709c63399f32ffe30d5ab[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 22 23:40:45 2023

    Add API to determine bytecode format version of StableHLO release (#1520)
    
    Adds a mapping of StableHLO version to bytecode format version. Map only
    indicates the releases where the format version changes:
    
    ```c++
    <0.10.0, 1> // bytecode format incremented to v1 in 0.10.0
    <0.9.0, 0> // bytecode format started at v0 in 0.9.0
    ```
    
    Comparison algorithm validates supported version range, and walks the
    list until it finds a version less or equal to the requested version:
    
    
    ```c++
    // <- 0.12.0 is above curr version, failure()
    // <- 0.11.0 uses v1
    // <- 0.10.0 uses v1
    <0.10.0, 1>
    // <- 0.9.2 uses v0
    // <- 0.9.0 uses v0
    <0.9.0, 0>
    // <- 0.8.0 is below minimum, failure()
    ```

[33mcommit 7c120e2edc8557aa96670067c976b37a755519ab[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 22 21:26:35 2023

    Add option to stablehlo-translate to generate bytecode without debug info (#1519)
    
    Exposes `--strip-debuginfo` as a pass from `stablehlo-translate`. This
    is currently available in `stablehlo-opt`.
    
    Alternatively, we could use:
    
    ```
    stablehlo-opt file.mlir --strip-debuginfo --emit-bytecode | stablehlo-translate --serialize --target=X.Y.Z
    ```
    
    Must use bytecode across the bash pipe, otherwise debug info gets
    populated with <stdin> values:
    
    ```
    #loc972 = loc("<stdin>":853:5)
    #loc973 = loc("<stdin>":855:3)
    #loc975 = loc("<stdin>":856:10)
    #loc976 = loc("<stdin>":857:5)
    ```

[33mcommit 38743b2fe0bfeaeac8301acf464b30e6002a9e57[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 22 20:33:59 2023

    Consolidate VhloToVersion negative tests (#1517)
    
    Consolidate VhloToVersion negative tests. Use
    https://github.com/openxla/stablehlo/pull/1516 as diffbase.

[33mcommit 27e1e53fd5c4d37c140b273e0b5d1c02dd7bc2b5[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 22 20:17:31 2023

    Add producer to StableHLO portable artifacts. (#1516)
    
    Producer reads `StableHLO_v<target_version>`, for example:
    
    ```
    $ stablehlo-translate --serialize --target=0.9.0 file.mlir
    ML?RStableHLO_v0.9.0[...]
    ```
    
    Also moved some logic around and added `minimum` as supported target
    version now that we have a getMinimumVersion API.
    
    Currently there are no APIs that inspect the producer string, so this is
    purely debug info and does not impact forward or backward compatibility.

[33mcommit 05223052b7e2f387b3534496b0a0109094a6af2b[m[33m ([m[1;33mtag: [m[1;33mv0.11.6[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun May 21 18:43:18 2023

    Actually use the BytecodeWriterConfig in writeBytecodeToFile (#1512)
    
    #1511 introduced BytecodeWriterConfig to our serialization logic,
    but I forgot to actually use it. This PR fixes this oversight.
    
    This mistake on my part happened because we don't have forward
    compatibility tests yet. To make sure that I didn't mess up anything
    else this time, I have manually verified the following:
    * At HEAD, build/bin/stablehlo-translate --serialize produces different
    payloads when: 1) local clone of llvm-project is pristine, 2) local
    clone of llvm-project has kVersion manually changed to 2. The only
    difference is the bytecodeVersion field in the serialized payloads.
    * With this PR, build/bin/stablehlo-translate --serialize produces the
    same payloads when: 1) local clone of llvm-project is pristine, 2) local
    clone of llvm-project has kVersion manually changed to 2. As expected,
    using BytecodeWriterConfig overrides kVersion.

[33mcommit b1cff89dbef1e2db09beafd26ec7c5eabd338aac[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun May 21 17:54:22 2023

    Integrate LLVM at llvm/llvm-project@f7e2678bb706 (#1510)

[33mcommit e86b9c56110538f48960cc03126e67a9ad03d10b[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun May 21 17:40:03 2023

    Use bytecodeVersion = 1 when serializing MLIR bytecode (#1511)
    
    https://reviews.llvm.org/D149515 has just landed, so we must explicitly
    specify bytecodeVersion when using BytecodeWriter to avoid breaking
    forward compatibility guarantees.
    
    More specifically, we want to avoid a situation where:
      1) A StableHLO producer using a post-D149515 version of LLVM
         serializes a StableHLO program to bytecode. (This will use
         bytecodeVersion = 2 by default).
      2) A StableHLO consumer using a pre-D149515 version of LLVM within
         the 1 month StableHLO forward compatibility window cannot
         deserialize the StableHLO program.
    
    We don't have forward compatibility tests yet, so this PR doesn't have
    tests either. See #1498 for an RFC for forward compatibility testing.

[33mcommit 54bf2f32e790fa8105e90d8cdffbf09ce20da86e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun May 21 17:26:05 2023

    Fix issues identified during integrate (#1505)
    
    * BUILD.bazel: remove import of unused build_test.
      * BUILD.bazel: order dependencies alphabetically.

[33mcommit 26b63a9398d68a16aef27a035e4d8520596a7dc0[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat May 20 01:00:40 2023

    Bump patch version after integrate 0.11.5 -> 0.11.6 (#1507)
    
    This PR concludes this weeks integrates, which were a bit messed up
    because of my oversight. #1506 explains what happened and provides a
    fix, and this PR is the final step towards normalcy.

[33mcommit 4fe990afe3a79ffaf6ca502fb6eebde14abadeb6[m[33m ([m[1;33mtag: [m[1;33mv0.11.5[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat May 20 00:29:12 2023

    Belatedly bump patch version after integrate 0.11.4 -> 0.11.5 (#1506)
    
    Our integrate process involves bumping Version.h on GitHub after a
    successful landing of an downstream integrate. This is done to make sure
    that the subsequent downstream integrate is guaranteed to receive a
    different version, so that we can tell them apart down the line.
    
    Unfortunately, I forgot to do that after the previous downstream
    integrate:
    https://github.com/tensorflow/mlir-hlo/commit/e46e2b655d70a9099acd47be940ca3c0973583a2
    which had its Version.h say 0.11.4.
    
    Now we have another downstream integrate that has just landed:
    https://github.com/tensorflow/mlir-hlo/commit/a1cd423b7ae9cf9f48cd21494756d85d04e97411,
    and it has the same version as its predecessor (its Version.h also says
    0.11.4). This is exactly what the integrate process was trying to avoid.
    
    To untangle this, I propose that we:
      1) Bump Version.h to 0.11.5 on GitHub (this PR).
      2) Bump Version.h to 0.11.5 downstream (there'll be a separate CL).
      3) After 1) and 2) are merged, we tag the GitHub HEAD as v0.11.5.
      4) And then proceed as usual.

[33mcommit 6232753c7b74bb5d555293479b80902eed55ff2e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 19 20:03:36 2023

    Rename tosa/ passes to follow pass name conventions (#1504)
    
    For the legalization passes, we've been following the
    foo-legalize-to-bar convention. And for passes in general, we've been
    adhering to dialectname-something-something. These conventions are
    applied to the recently introduced tosa/ passes in this PR.
    
    Also, to further the consistency between existing passes and newly
    introduced passes, this PR removes manual definitions of createFooPass
    functions. At some point, TableGen got the ability to automatically
    generate these definitions, so we're leveraging it here.
    
    Thank you, @GleasonK for your feedback that led to the creation of this
    pull request.

[33mcommit 9e36ae5e5032ec28ea436a3e1153ddb54adfdfcf[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 19 19:19:38 2023

    Rename some files in tosa/ to follow filename conventions (#1503)
    
    In other parts of the repo, we're using the LLVM-style convention of
    FooBar.h and FooBar.cpp vs the Google-style convention of foo_bar.h and
    foo_bar.cc.

[33mcommit 608d220172fbad41e9055b2f559026f2c7e3085d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 19 18:54:53 2023

    Integrate tosa/ into the Bazel build (#1502)
    
    This PR merges the BUILD.bazel file from
    stablehlo/conversions/tosa/transforms into the root BUILD.bazel file, to
    follow the current convention.
    
    We cargo-culted this convention from the MLIR-HLO repository, so maybe
    it's time to get rid of it, but I'll leave that to future work.
    
    Furthermore, I added BUILD.bazel in stablehlo/conversions/tosa/tests, so
    that Bazel can actually run those tests.
    
    Finally, I noticed that a minor cleanup opportunity in the CMake build,
    and I figured it wouldn't hurt to pursue it in this PR.

[33mcommit 4e2ad864c806b8bf650eb55d88c6433155fa6472[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 19 18:05:46 2023

    Incorporate tosa/ tests into our test infrastructure (#1501)
    
    In #1466, we ran into some issues when adding the newly introduced tosa/
    suite to relevant test targets, so this PR revamps this system to
    properly integrate tosa/ as well as simplify similar work in the future.
    
    Instead of the state of the art with one `check-stablehlo` custom target
    and a bunch of suites becoming its dependencies, we now have three
    custom targets:
      1) check-stablehlo-ci whose name makes it clear that this is what
         runs in CI.
      2) check-stablehlo-slow for slow-running tests like the testdata/
         suite that we'd like to separate from the other suites, so that
         humans don't have to run them every time.
      3) check-stablehlo-quick for everything else.
    
    Each suite becomes a dependency of either -slow or -quick, making its
    nature explicit and clearly documented.
    
    Also, it looks like the tosa/ suite got broken by one of the LLVM bumps
    that happened between when its PR got created and when it got merged. I
    fixed those breakages too.

[33mcommit d9f723d046d62d0b2987ac01b359c8da0b8c57d3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 19 16:45:30 2023

    Integrate LLVM at llvm/llvm-project@e96123dfeabc (#1499)

[33mcommit cf2f2b84e564cbb5014275698720867322e443bb[m
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Fri May 19 16:44:36 2023

    [tosa] Add path to TOSA backed backends. (#1466)
    
    Connect these two industry standards by way of dialect legalization from
    StableHLO to TOSA. This adds basic support and testing: along with usage
    of some of the equivalent canonicalization patterns from MHLO (not
    included in this PR) this has been sufficient for some full models
    starting from ML framework to TOSA backed. Support is not complete and
    partly relies on some canonical StableHLO forms.
    
    The legalizations are also written primarily using PDLL, but we have not
    yet adopted some of the newer support there for variadics. This work
    started by targeting MHLO in TensorFlow repo as StableHLO was still
    young, but given StableHLO development it makes more sense to instead
    start there and provide a connection for community backends.
    
    No new repository dependency is introduced. The cmake config enables
    disabling building conversion,
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit 14691ce2e956f089d401b5bfee9fcb10e20d4755[m[33m ([m[1;33mtag: [m[1;33mv0.11.4[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 17 15:59:37 2023

    Integrate LLVM at llvm/llvm-project@33da608ecc0f (#1495)

[33mcommit dc3938e491c6b56caf0f0f8bfca0ac361727b6c2[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 17 15:12:50 2023

    Add limited support for multiple functions in shape refinement (#1484)
    
    --stablehlo-refine-shapes currently has a limitation of only supporting
    one function to avoid dealing with complexities of potential loops in
    the dataflow graphs.
    
    This PR slightly relaxes this limitation by not erroring out on multiple
    functions and instead refining shapes in just the `main` function among
    those (in case one doesn't exist, that would be an error).
    
    MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/36225413eab276eda72f936065d3ab361cf53889.

[33mcommit 14b7a892da76c2574e274be7ba7de640bcab1d3d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 17 15:08:04 2023

    Slightly reword compatibility.md (#1493)
    
    1\) With the addition of new content, the "Out of scope" section seemed
    a bit out of place, so I moved it to the bottom of the document.
    
    2\) Shortened "Creating portable artifacts" to "APIs" and applied
    further shortenings to subordinate sections.
    
    3\) Consistently referred to these APIs as "compatibility APIs" since we
    are in a document called compatibility.md.

[33mcommit f5a391162249925f820ef6839608023b7a0bd0be[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 16 05:49:15 2023

    Fix CholeskyOp breakage (#1487)
    
    The recent PR has merged with breakage in its tests, so this PR fixes
    it.

[33mcommit 8bbb0ee9f6136d6028084a81b76b6dd9756278ec[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 16 05:20:29 2023

    Add interpreter for CholeskyOp (#1444)
    
    Here are the constraints for CholeskyOp:
    ```
    (I1) `a` is a tensor of floating-point or complex type.
    (I2) `lower` is a tensor constant of `i1` type.
    (C1) `a` and `result` have the same type.
    (C2) rank(`a`) >= 2.
    (C3) dim(`a`, -2) = dim(`a`, -1).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `a` is not a tensor of floating-point type or complex type. (Covered by ODS).
    I2: a) `lower` is not a tensor constant of `i1` type. (Covered by ODS).
    C1: a) type(a) != type(result).
    C2: a) rank(a) < 2.
    C3: a) dim(`a`, -2) != dim(`a`, -1).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: type(a) != type(result).
    C2a: rank(a) < 2.
    C3a: dim(`a`, -2) != dim(`a`, -1).
    ```
    
    Notes:
    * Implementation inspired by the [Cholesky‚ÄìBanachiewicz
    algorithm](https://en.wikipedia.org/wiki/Cholesky_decomposition).
    
    closes #1123

[33mcommit 2358069918b19e49eabf9afc283d25b70d6dc4ac[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon May 15 20:10:59 2023

    Update serialization cookbook to include Python APIs (#1483)
    
    Move instructions on creating serialized artifacts to `compatibility.md`
    to have a centralized location to look for user compatibility
    documentation.
    
    Also add Python APIs and augment with links to example code.

[33mcommit c717cb992d6d3970a66fc00aba9569d2fc4c1605[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon May 15 18:55:43 2023

    Fix issues identified during integrate (#1485)
    
    * BUILD.bazel & CMakeLists.txt: add a missing dependency.
    * stablehlo/dialect/TypeInference.cpp,
    stablehlo/dialect/TypeInference.h, stablehlo/reference/Ops.cpp,
    stablehlo/reference/Tensor.h: fix clang-tidy warnings.
      * stablehlo/integrations/python/tests/stablehlo.py: fix formatting.

[33mcommit 4614c7fa8c451e44f085ce15466863edfbfc9c32[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon May 15 18:55:29 2023

    Bump patch version after integrate 0.11.3 -> 0.11.4 (#1486)

[33mcommit 19ca41caa46260b64581740b8f8f9a38bc86126b[m[33m ([m[1;33mtag: [m[1;33mv0.11.3[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri May 12 21:31:43 2023

    Integrate LLVM at llvm/llvm-project@8faffa3cd3e1 (#1482)

[33mcommit 89b9da3163855eb0721a8a3f487b651ecaae57fb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 12 21:26:17 2023

    Add API to get minimum supported StableHLO version (#1481)
    
    This is based on https://github.com/openxla/stablehlo/pull/1480

[33mcommit d6f684691fe3e27751aec24c51a236a3379093b7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri May 12 21:16:23 2023

    Split portable StableHLO APIs into separate file (#1480)
    
    Portable APIs are APIs with signatures that do not depend on MLIR. This
    provides a way to access some StableHLO APIs without needing an MLIR
    dependency/visibility at the call site.
    
    These APIs also can be safer in cases where shared objects are used, as
    passing MLIR Context across shared objects can cause problems.

[33mcommit fb050189fec06a9b50fe08cbe95a7a21e7db0375[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu May 11 00:09:04 2023

    Add interpreter for BatchNormInferenceOp (#1371)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `operand`: tensor of floating-point type.
    (I2) `scale`: 1-dimensional tensor of floating-point type.
    (I3) `offset`: 1-dimensional tensor of floating-point type.
    (I4) `mean`: 1-dimensional tensor of floating-point type.
    (I5) `variance`: 1-dimensional tensor of floating-point type.
    (I6) `epsilon`: constant of type `f32`.
    (I7) `feature_index`: constant of type `si64`.
    (C1) 0 <= `feature_index` < rank(`operand`).
    (C2) `operand`, `scale`, `offset`, `mean`, `variance` and `result` have the
    same element type.
    (C3) size(`scale`) = `dim(operand, feature_index)`.
    (C4) size(`offset`) = `dim(operand, feature_index)`.
    (C5) size(`mean`) = `dim(operand, feature_index)`.
    (C6) size(`variance`) = `dim(operand, feature_index)`.
    (C7) `operand` and `result` have the same type.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `operand` is not a tensor of floating-point type. (Covered by ODS).
    I2: a) `scale` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`scale`) != floating-point type. (Covered by ODS).
    I3: a) `offset` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`offset`) != floating-point type. (Covered by ODS).
    I4: a) `mean` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`mean`) != floating-point type. (Covered by ODS).
    I5: a) `variance` is not a 1-dimensional tensor. (Covered by ODS).
        b) element_type(`variance`) != floating-point type. (Covered by ODS).
    I6: a) `epsilon` is not a constant of type `f32`. (Covered by ODS).
    I7: a) `feature_index` is not a constant of type `si64`. (Covered by ODS).
    C1: a) `feature_index` < 0.
        b) `feature_index` >= rank(`operand`).
    C2: a) element_type(`operand`) != element_type(`scale`). (Covered by ODS).
        b) element_type(`operand`) != element_type(`offset`). (Covered by ODS).
        c) element_type(`operand`) != element_type(`mean`). (Covered by ODS).
        d) element_type(`operand`) != element_type(`variance`). (Covered by ODS).
        e) element_type(`operand`) != element_type(`result`). (Covered by ODS).
    C3: a) size(`scale`) != `dim(operand, feature_index)`.
    C4: a) size(`offset`) != `dim(operand, feature_index)`.
    C5: a) size(`mean`) != `dim(operand, feature_index)`.
    C6: a) size(`variance`) != `dim(operand, feature_index)`.
    C7: a) type(`operand`) != type(`result`).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: `feature_index` < 0.
    C1b: `feature_index` >= rank(`operand`).
    C3a: size(`scale`) != `dim(operand, feature_index)`.
    C4a: size(`offset`) != `dim(operand, feature_index)`.
    C5a: size(`mean`) != `dim(operand, feature_index)`.
    C6a: size(`variance`) != `dim(operand, feature_index)`.
    C7a: type(`operand`) != type(`result`).
    ```
    
    closes #963

[33mcommit 8e7ec970fad3e80a725145fde213032e28e2c38f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 10 21:20:57 2023

    Add interpreter for ConvertOp (#1349)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor. (Covered by ODS).
    (C1) `operand` and `result` have the same shape. (Covered by ODS).
    ```
    
    Notes:
    * Added one positive test apart from existing fp8 tests.
    * No additional constraint tests are needed as all constraints and shape
    inference is covered by ODS.
    * Left out handling special behaviors for floating-point/complex ->
    integer and vice versa to #180 (currently the behavior is implementation
    defined).
    * Updated spec to clarify semantics for complex to boolean case.
    
    closes #969
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit cc08d0990acc322f91695488eb810560c05e8c33[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 10 20:51:49 2023

    Add interpreter for ReduceWindowOp (#1336)
    
    We have the following constraints in the spec:
    
    ```
    (I1) `inputs`: variadic number of tensors.
    (I2) `init_values`: variadic number of 0-dimensional tensors.
    (I3) `window_dimensions`: 1-dimensional tensor constant of type `si64`.
    (I4) `window_strides`: 1-dimensional tensor constant of type `si64`.
    (I5) `base_dilations`: 1-dimensional tensor constant of type `si64`.
    (I6) `window_dilations`: 1-dimensional tensor constant of type `si64`.
    (I7) `padding`: 2-dimensional tensor constant of type `si64`.
    (I8) `body`: function.
    (C1) size(`inputs`) = size(`init_values`) = size(`results`) = N and
      N >= 1.
    (C2) All `inputs` have the same shape.
    (C3) `element_type(inputs[k]) = element_type(init_values[k])` for all k
      in [0, N).
    (C4) size(`window_dimensions`) = rank(`inputs[0]`).
    (C5) `window_dimensions[i]` > 0 for all i in [0, size(`window_dimensions`)).
    (C6) size(`window_strides`) = rank(`inputs[0]`).
    (C7) `window_strides[i]` > 0 for all i in [0, size(`window_strides`)).
    (C8) size(`base_dilations`) = rank(`inputs[0]`).
    (C9) `base_dilations[i]` > 0 for all i in [0, size(`base_dilations`)).
    (C10) size(`window_dilations`) = rank(`inputs[0]`).
    (C11) `window_dilations[i]` > 0 for all i in [0, size(`window_dilations`)).
    (C12) dim(`padding`, 0) = rank(`inputs[0]`) and dim(`padding`, 1) = 2.
    (C13) `body` has type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
      where `Ek = element_type(inputs[0])`.
    (C14) All `results` have the same shape.
    (C15) `shape(results[0]) = num_windows`
      * `dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1`.
      * `padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]`.
      * `dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1`.
      * `num_windows = (padded_input_shape == 0 || dilated_window_shape > padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1`.
    (C16) `element_type(results[k]) = element_type(init_values[k])` for all k
      in [0, N).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `inputs` is not variadic number of tensors. (Covered by ODS).
    I2: a) `init_values` is not variadic number of tensors. (Covered by ODS).
        b) `init_values` is not 0-dimensional tensors.
    I3: a) `window_dimensions` is not a 1-dimensional tensor constant.
        b) element_type(`window_dimensions`) != `si64`. (Covered by ODS).
    I4: a) `window_strides` is not a 1-dimensional tensor constant.
        b) element_type(`window_strides`) != `si64`. (Covered by ODS).
    I5: a) `base_dilations` is not a  1-dimensional tensor constant.
        b) element_type(`base_dilations`) != `si64`. (Covered by ODS).
    I6: a) `window_dilations` is not a 1-dimensional tensor constant.
        b) element_type(`window_dilations`) != `si64`. (Covered by ODS).
    I7: a) `padding` is not a 2-dimensional tensor constant.
        b) element_type(`padding`) != `si64`. (Covered by ODS).
    I8: a) `body` is not a function. (Covered by ODS).
    C1: a) size(`inputs`) != size(`init_values`)
        b) size(`inputs`) != size(`results`)
        c) size(`inputs`) < 1.
    C2: a) Any `inputs` does not have the same shape.
    C3: a) `element_type(inputs[k]) != element_type(init_values[k])` for any k
      in [0, N).
    C4: a) size(`window_dimensions`) != rank(`inputs[0]`).
    C5: a) `window_dimensions[i]` <= 0 for any i in [0, size(`window_dimensions`)).
    C6: a) size(`window_strides`) != rank(`inputs[0]`).
    C7: a) `window_strides[i]` <= 0 for any i in [0, size(`window_strides`)).
    C8: a) size(`base_dilations`) != rank(`inputs[0]`).
    C9: a) `base_dilations[i]` <= 0 for any i in [0, size(`base_dilations`)).
    C10: a) size(`window_dilations`) != rank(`inputs[0]`).
    C11: a) `window_dilations[i]` <= 0 for any i in [0, size(`window_dilations`)).
    C12: a) dim(`padding`, 0) != rank(`inputs[0]`)
         b) dim(`padding`, 1) != 2.
    C13: a) `body` does not have type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
      where `Ek = element_type(inputs[0])`.
    C14: a) Any `results` do not have the same shape.
    C15: a) `shape(results[0]) != num_windows`
      * `dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1`.
      * `padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]`.
      * `dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1`.
      * `num_windows = (padded_input_shape == 0 || dilated_window_shape > padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1`.
    C16: a) `element_type(results[k]) != element_type(init_values[k])` for any k
      in [0, N).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2b: `init_values` is not 0-dimensional tensors.
    I3a: `window_dimensions` is not a 1-dimensional tensor constant.
    I4a: `window_strides` is not a 1-dimensional tensor constant.
    I5a: `base_dilations` is not a  1-dimensional tensor constant.
    I6a: `window_dilations` is not a 1-dimensional tensor constant.
    I7a: `padding` is not a 2-dimensional tensor constant.
    C1a: size(`inputs`) != size(`init_values`)
    C1b: size(`inputs`) != size(`results`)
    C1c: size(`inputs`) < 1.
    C2a: Any `inputs` does not have the same shape.
    C3a: `element_type(inputs[k]) != element_type(init_values[k])` for any k in [0, N).
    C4a: size(`window_dimensions`) != rank(`inputs[0]`).
    C5a: `window_dimensions[i]` <= 0 for any i in [0, size(`window_dimensions`)).
    C6a: size(`window_strides`) != rank(`inputs[0]`).
    C7a: `window_strides[i]` <= 0 for any i in [0, size(`window_strides`)).
    C8a: size(`base_dilations`) != rank(`inputs[0]`).
    C9a: `base_dilations[i]` <= 0 for any i in [0, size(`base_dilations`)).
    C10a: size(`window_dilations`) != rank(`inputs[0]`).
    C11a: `window_dilations[i]` <= 0 for any i in [0, size(`window_dilations`)).
    C12a: dim(`padding`, 0) != rank(`inputs[0]`)
    C12b: dim(`padding`, 1) != 2.
    C13a: `body` does not have type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)`
      where `Ek = element_type(inputs[0])`.
    C14a: Any `results` do not have the same shape.
    C15a: `shape(results[0]) != num_windows`
      * `dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1`.
      * `padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]`.
      * `dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1`.
      * `num_windows = (padded_input_shape == 0 || dilated_window_shape > padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1`.
    C16a: `element_type(results[k]) != element_type(init_values[k])` for any k in [0, N).
    ```
    
    Notes:
    * Minor wording change in the spec.
    * We cannot verify C1a: size(`inputs`) != size(`init_values`) as noted
    in #1334.
    * Removed some duplicate verification tests.
    
    closes #983
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit f396777811792145c4915df2c7f842185cc6b017[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed May 10 16:34:26 2023

    Specification for quantized DotGeneralOp (#1413)
    
    ## Summary
    The PR proposes the spec for quantized dot-general op along with the
    specifications for a few other ops on which the dot-general depends on,
    for example, `slice`, `transpose`, and `reshape`.
    
    ## A few details
    Given `fp = tensor with floating-point type and q = tensor with
    uniformed quantized type`, the PR covers the semantics of
    (1) Static range quantized `dot_general` op `dot_general(q, q)`, and
    ~~(2) Hybrid quantized `dot_general` op `dot_general(fp, q)`: Currently,
    this version of the op only supports dynamic range quantization, where
    the on-the-fly quantization of `lhs` is fused in the op-semantics. IMO,
    once we support https://github.com/openxla/stablehlo/issues/1407, the
    quantization logic can be un-fused and made explicit in the MLIR graph
    (cc @sngyhan).~~
    
    **update**: As per the
    [discussion](https://github.com/openxla/stablehlo/pull/1413#discussion_r1183127043),
    it is decided to have only (1) in the spec. It might be too early to
    introduce (2), the "dynamic range quantizated" variant of the op, mainly
    because (a) only TFLite CPU implements it and (b) in the long, there are
    plans to implement dynamic range quantization expolicitly in the graph
    level.
    
    
    ## What comes next
    The plan forward is to propose a PR for convolution op in very near
    future. I realized that the spec for convolution depends on dot-general
    and a split might help the review process.
    
    Please let me know your review feedback.

[33mcommit 05d050d187ab3c614a4b134bf81ea8be00b03f4e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed May 10 01:51:13 2023

    Specification for quantized  AddOp (#1446)
    
    ## Summary
    
    The PR proposes the specification for quantized add op.
    
    ## A few details
    
    At some point we
    [decided](https://github.com/openxla/stablehlo/pull/1352#discussion_r1166196224)
    to drop the introduction of the specification of this op mainly because
    we were unsure about the fate of
    https://github.com/openxla/stablehlo/issues/1406.
    
    Please have a look at my revised proposal on
    https://github.com/openxla/stablehlo/issues/1406 and let me know if I am
    missing something. Otherwise, let us review this op and let me know your
    feedback.
    
    Side note: For those who are already aware of the context of prior
    introduction of this op, please note that the current proposal is almost
    same as before except that it does not have any additional constraint
    imposed by the op's semantics on `storage_min` or `storage_max`.

[33mcommit ea7153c5ccf8dd1435720d0c46397a63224ecc6e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 10 00:23:38 2023

    Clarify the checklist wording and provide examples (#1474)
    
    closes #1473

[33mcommit 7f265c6e40364c061a574b2a4cb6faea94c9af78[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 9 18:33:39 2023

    Bump patch version after integrate 0.11.2 -> 0.11.3 (#1471)

[33mcommit 17fda0c9f45798a0bd4ddd89c133ef8735f7bddd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 9 01:24:53 2023

    Add interpreter for ReduceOp (#1280)
    
    We have the following constraints in the spec:
    
    ```
    (I1) inputs: variadic number of tensors
    (I2) init_values: variadic number of 0-dimensional tensors
    (I3) dimensions: 1-dimensional tensor constant of type `si64`
    (I4) body: function
    (C1) All `inputs` have the same shape.
    (C2) element_type(`inputs[k]`) = element_type(`init_values[k]`) =
    element_type(`results[k]`) for all `k` $\in$ [0, N).
    (C3) size(`inputs`) = size(`init_values`) $=$ size(`results`) $=$ N where
    N >= 1.
    (C4) 0 <= `dimensions[d]` < rank(`inputs[0][d]`) for all dimension `d`.
    (C5) All dimensions in `dimensions` are unique.
    (C6) `body` has type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ...,`
    `tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)` where
    `Ek = element_type(inputs[k])`.
    (C7) shape(`results[k]`) = shape(`inputs[k]`) except that the dimension
    sizes of `inputs[k]` corresponding to `dimensions` are not included.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) inputs is not a variadic tensor. (Covered by ODS).
    I2: a) init_values is not a variadic 0-dimensional tensors.
    I3: a) dimensions is not a 1-dimensional tensor.
        b) element_type(dimensions) != si64. (Covered by ODS).
    I4: a) body is not a function. (Covered by ODS).
    C1: a) Not all `inputs` have the same shape.
    C2: a) element_type(`inputs[k]`) != element_type(`init_values[k]`) for any `k` $\in$ [0, N).
        b) element_type(`inputs[k]`) != element_type(`results[k]`) for any `k` $\in$ [0, N).
    C3: a) size(`inputs`) != size(`init_values`). (Covered by ODS).
        b) size(`inputs`) != size(`results`). (Covered by ODS).
        c) size(`inputs`) < 1. (Covered by ODS).
    C4: a) 0 > `dimensions[d]` for any dimension `d`.
        b) `dimensions[d]` >= rank(`inputs[0][d]`) for any dimension `d`.
    C5: a) Dimensions in `dimensions` are not unique.
    C6: a) `body` does not have type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ...,`
    `tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)` where
    `Ek = element_type(inputs[k])`.
    C7: shape(`results[k]`) != shape(`inputs[k]`) except that the dimension
    sizes of `inputs[k]` corresponding to `dimensions` are not included.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2a: init_values is not a variadic 0-dimensional tensors.
    I3a: dimensions is not a 1-dimensional tensor.
    C1a: Not all `inputs` have the same shape.
    C2a: element_type(`inputs[k]`) != element_type(`results[k]`) for any `k` $\in$ [0, N).
    C2b: element_type(`init_values[k]`) != element_type(`results[k]`) for any `k` $\in$ [0, N).
    C4a: 0 > `dimensions[d]` for any dimension `d`.
    C4b: `dimensions[d]` >= rank(`inputs[0][d]`) for any dimension `d`.
    C5a: Dimensions in `dimensions` are not unique.
    C6a: `body` does not have type `(tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ...,`
    `tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>)` where
    `Ek = element_type(inputs[k])`.
    C7a: shape(`results[k]`) != shape(`inputs[k]`) except that the dimension
    sizes of `inputs[k]` corresponding to `dimensions` are not included.
    ```
    
    Notes:
    * Verification for I2 is not added because of #704.
    
    closes #982

[33mcommit eaefd3143fb5d18636f5d7d3cdc5b5d3c9797326[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 9 00:35:33 2023

    Add missing checks/tests for supported f8 types (#1470)
    
    Migrating changes made in
    https://github.com/tensorflow/mlir-hlo/commit/ed8f354a68855753128e60e40c88b54af1fef6f5.
    closes #1454
    
    Reintroducing PR from #1459 as it was closed prematurely.

[33mcommit 09c82d2dbefeae322010548ee2542c97364f3d32[m[33m ([m[1;33mtag: [m[1;33mv0.11.2[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon May 8 22:23:04 2023

    Integrate LLVM at llvm/llvm-project@14f0776550b5 (#1468)

[33mcommit 72dc462bd459bc937b9c213f6afd0182c36022df[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat May 6 01:10:47 2023

    Bump patch version 0.11.1 -> 0.11.2 (#1465)

[33mcommit 579f865e350cbd3513df71ecec4f4a12f0acb5fb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 4 16:31:26 2023

    Add note on version bumping to compatibility.md (#1462)
    
    Closes #1319

[33mcommit 9e2b072b2e79d656fb6b7b782f18b7b53e5bbdf7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu May 4 15:46:12 2023

    Add Python Serialization APIs that operate on strings (#1461)
    
    More details on rationale in API comments of
    `stablehlo/dialect/Serialization.h`.
    
    After learning more about Python bindings, unless build is set up in a
    specific way where all dialect extensions built together so type IDs are
    accurate, passing/returning strings is safer. These APIs give the option
    to do either.
    
    Backport of
    https://github.com/tensorflow/mlir-hlo/commit/6d62e3157aec86d9a6c023595c1c7f89ecf928da

[33mcommit 7fcd2015c5c27931612b9c60ed3e1bee802e30d9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 3 22:10:14 2023

    Enable passing interpreter tests (#1418)
    
    Tests can slip through if a test contains more than one op from the PR
    queue and one PR is merged while the other PR does not check for
    enabling additional tests after rebase.

[33mcommit 1a97b32fdf6c23ccb33b2a0b87a8764c71874868[m
Author: Karthik Rangasai <39360170+karthikrangasai@users.noreply.github.com>
Date:   Wed May 3 21:49:00 2023

    Add interpreter for ComplexOp. (#1414)
    
    Closes #1101 .
    
    We have the following constraints in the spec:
    
    ```
    (I1) lhs is a tensor of 32 bit floating-point or 64 bit floating-point.
    (I2) rhs is a tensor of 32 bit floating-point or 64 bit floating-point.
    (C1) lhs and rhs have the same type.
    (C2) result and lhs have the same shape.
    (C3) the return type should be a complex type of the element type of the lhs i.e. element_type(`result`) = complex_type(element_type(`lhs`)).
    ```
    
    These constraints are covered by the following tests
    
    ```
    I1: lhs is not a tensor of 32 bit floating-point or 64 bit floating-point. (ODS)
    I2: rhs is not a tensor of 32 bit floating-point or 64 bit floating-point. (ODS)
    C1: type(lhs) != type(rhs).
    C2: shape(result) != shape(lhs).
    C3: element_type(`result`) != complex_type(element_type(`lhs`)). (ODS)
    ```

[33mcommit 4603fd199e8eb61a3ba28a787f46e640a9989b6a[m
Author: Karthik Rangasai <39360170+karthikrangasai@users.noreply.github.com>
Date:   Wed May 3 21:48:36 2023

    Add interpreter for Expm1Op. (#1411)
    
    closes #1102 .
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) operand and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    I1: a) operand is not a tensor of floating-point or complex type. (Covered by ODS).
    C1: a) type(operand) != type(result). (Covered by ODS).
    ```

[33mcommit bc5bcae44e267ccd6b3532afacd24704fcffdd8a[m
Author: Karthik Rangasai <39360170+karthikrangasai@users.noreply.github.com>
Date:   Wed May 3 21:40:16 2023

    Add interpreter for Log1pOp. (#1402)
    
    closes #1105
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) operand and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    I1: a) operand is not a tensor of floating-point or complex type. (Covered by ODS).
    C1: a) type(operand) != type(result). (Covered by ODS).
    ```

[33mcommit 5fa939785c0f87d7e112e6ac2798dc418e0eff90[m[33m ([m[1;33mtag: [m[1;33mv0.11.1[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 3 17:57:52 2023

    Integrate LLVM at llvm/llvm-project@8f966cedea59 (#1458)

[33mcommit d415650b81d1865aec0a72d0f0b9a3f27da21d9f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed May 3 17:37:12 2023

    Bump patch version 0.11.0 -> 0.11.1 (#1457)

[33mcommit c8a634d34826730df4481aeb169136e3a8433f7e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed May 3 16:14:01 2023

    Account for operand_layouts in CustomCallOp canonicalization (#1455)
    
    CustomCallOp canonicalization can delete operands called out in
    indices_of_shape_operands if certain conditions are met.
    
    What I missed when implementing this is that this requires fixing up the
    operand_layouts attribute.
    
    This has been originally implemented in
    https://github.com/tensorflow/mlir-hlo/commit/b35b237e77984ff8dd75f3a5a9f29b174d0e40c9
    earlier today, and this PR backports that work.

[33mcommit b90e52ef967e4a39a1844d91c8edd59349e463a3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 22:54:34 2023

    Drop support for index element types in Eval patterns (#1453)
    
    This PR is based on #1452.
    
    StableHLO ops don't actually support index element types, except in
    rare situations (see uses of HLO_DimensionTensor in the TableGen file)
    which don't apply to Eval patterns.

[33mcommit dd4deed0beec28cba3b91984b70ca50430cde8b3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 22:35:51 2023

    Refactor EvalSliceOpPattern to use APSInt-based matchInts (#1452)
    
    Erasing the bitness and the signedness of the underlying values to
    int64_t is reasonable when we're going to use these values in the
    int64_t context (e.g. take a shape of DynamicReshapeOp and then put it
    into ShapedTypeComponents).
    
    However, in the case of data movement ops like BroadcastInDimOp,
    ConcatenateOp and now SliceOp, this doesn't make much sense.
    
    This even caused a crash when matchInts was called when trying to
    evaluate slices of unsigned tensors. When we were trying to call
    DenseIntElementsAttr::get with an unsigned tensor type and values of
    type SmallVector<int64_t>, that led to a crash.
    
    Now that we have switched to SmallVector<APSInt>, this crash is fixed.
    I've also audited all occurrences of DenseIntElementsAttr::get to make
    sure that we no longer have any type mismatches that could lead to
    further crashes elsewhere.

[33mcommit 0f8c6e95626de6a215d2776787f58b146df0a1cf[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 21:49:11 2023

    Refactor matchInts to use APSInt instead of APInt (#1451)
    
    As I was cleaning up the uses of matchInts, I figured I'd do the
    long-standing refactoring to switch to using APSInt.
    
    This had the benefit of simplifying partial evaluation logic in
    --stablehlo-refine-shapes because with APSInts we no longer need to
    branch on whether the underlying APInts are signed or unsigned.

[33mcommit 3b60c2fd8c65d59ff0cd83a220d56e71192ff6b8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 2 19:10:00 2023

    Add interpreter for CbrtOp (#1417)
    
    Here are the constraints for CbrtOp:
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) `operand` and `result` have the same type.
    ```
    I1 and C1 are covered by the ODS, so no additional tests are added.
    
    Notes:
    * The implementation is inspired from De Moivre's formula to calculate
    nth root of a complex number. k is assumed 0 for principal root. See:
    https://en.wikipedia.org/wiki/De_Moivre%27s_formula
    
    closes #1099

[33mcommit 5572baa1071a98ec1389d4c17fb851bfa7b14508[m[33m ([m[1;33mtag: [m[1;33mv0.11.0[m[33m)[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 2 18:32:40 2023

    Integrate LLVM at llvm/llvm-project@a91cb9ce39dc (#1450)

[33mcommit da30def7b19529899c354f5527a0cbe61a5e45b6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 15:52:12 2023

    Assorted refactorings of the interpreter (#1440)
    
    Here are some of the things that I noticed over the last few weeks but
    didn't have the time to follow up on:
    
    1) Let's unify the boilerplate in the eval() function to: a)
    consistently use auto, b) drop the "runtime" part of variable names to
    make things easier to read, c) consistently declare temporary variables.
    As an alternative to c), we could skip creating temporaries except for
    result.
    
    2) Let's drop isSupportedFooType checks in Element.cpp when there's just
    one supported category of types. These checks are redundant because
    getFooValue will check that anyway.
    
    3) Same for the isSupportedComplexType + comparisonDirection check in
    evalCompareOp. It is redundant as well.
    
    4) There are few more minor cleanups in this pull request which I don't
    think need special description / justification.

[33mcommit 1d4198f2af9fad418afc4eaa20e126e65fef691a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 04:51:40 2023

    Improve logging in refineReturnTypes (#1388)
    
    While working on refineReturnTypes, I made some improvements to
    how the application of ShapedTypeComponents to Type happens.
    This is mostly an NFC that restructures the code to improve readability
    and logging.

[33mcommit cfc35b2d4cd427f3c54464f1c944fd593fb12377[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue May 2 03:45:50 2023

    Extend indices_of_shape_operands to work for tuple types (#1387)
    
    At the moment, indices_of_shape_operands are in 1:1 correspondence with
    result types, but that doesn't work when custom calls return tuples.
    
    This pull requests addresses this problem, and changes
    indices_of_shape_operands to be in 1:1 correspondence with flattened
    result types. Originally, I thought that I'd need to redesign
    indices_of_shape_operands, e.g. carry arrays of 1-dimensional tensors or
    arrays of strings, but this solution keeps the existing structure of
    the attribute, i.e. a 1-dimensional tensor of i64.
    
    The changes are surprisingly compact and cover the following three
    areas of the implementation:
      1) Verification logic for the attribute that lives in
         getShapeRefinements in Base.h.
      2) Helper logic which applies refinements to operation types
         that lives in refineReturnTypes in StablehloRefineShapes.cpp.
      3) inferMostSpecificType logic which is used to merged unrefined
         types and the corresponding refinements.
    
    These changes also led to an unexpected improvement. Now that
    --stablehlo-refine-shapes started operating on tuple types,
    tensor::CastOp stopped working (it only applies to tensors), so I was
    forced to look for a better solution and remembered about
    UnrealizedConversionCastOp. This was an easy migration, and as a nice
    bonus the pass no longer depends on the Tensor dialect.

[33mcommit 802bf1d1170575aa349d9821b924f29393b72721[m
Author: David Majnemer <david.majnemer@gmail.com>
Date:   Tue May 2 03:20:34 2023

    Add Float8E4M3B11FNUZ type support. (#1448)
    
    As proposed in [RFC: E4M3B11FNUZ in
    XLA](https://github.com/openxla/stablehlo/blob/main/rfcs/20230309-e4m3b11.md)
    (#1308), this change adds support for these types to StableHLO.
    
    This includes the type definitions, vhlo, and interpreter support. The
    testing approach mirrors the Float8E4M3FNUZ tests, since it is also a
    "non-standard" floating point type supported by StableHLO.

[33mcommit 43e9dda16b47bd0f33b977bb1c3a951f7708ee99[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue May 2 00:22:58 2023

    Integrate LLVM at llvm/llvm-project@dc275fd03254 (#1449)

[33mcommit 77e060469b02b75f8501d4d6e3475be2053aa7a0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon May 1 22:48:29 2023

    Bump patch version 0.10.1 -> 0.10.2 (#1447)
    
    After a StableHLO release is integrated into OpenXLA
    ([openxla/xla](https://github.com/openxla/xla/tree/main/third_party/stablehlo)),
    we bump the patch version so HEAD remains ahead of the latest release.

[33mcommit 43d81c6883ade82052920bd367c61f9e52f09954[m[33m ([m[1;33mtag: [m[1;33mv0.10.1[m[33m)[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Apr 27 20:52:03 2023

    Integrate LLVM at llvm/llvm-project@ba38640b9901 (#1441)

[33mcommit e5f8e944d2b50060502cf51e32bf8b7332d4a46e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 27 20:21:08 2023

    Bump patch to 0.10.1 after integrate (#1442)
    
    After a StableHLO release is integrated into OpenXLA
    ([openxla/xla](https://github.com/openxla/xla/tree/main/third_party/stablehlo)),
    bump the patch version so HEAD remains ahead of the latest release.

[33mcommit 1e5ef51f25f45a0a00ffb8881650b109cd5aeace[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Apr 21 23:08:00 2023

    Add interpreter for SortOp (#1283)
    
    We have the following constraints in the spec:
    
    ```
    (I1) inputs: variadic number of tensors.
    (I2) dimension: constant of type `si64`.
    (I3) is_stable: constant of type `i1`.
    (I4) comparator: function.
    (C1) `inputs` have at least 1 tensor.
    (C2) For all `i`, `type(inputs[i])` = `type(results[i])`.
    (C3) All tensors in `inputs` and `results` have the same shape.
    (C4) `-R` $\le$ `dimension` $\lt$ `R`, where `R` is rank of `inputs[0]`.
    (C5) `comparator` has type
    `(tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1>`,
    where `Ei` is element type of `inputs[i]`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) inputs is not variadic tensor. (Covered by ODS).
    I2: a) element_type(dimension) != `si64`. (Covered by ODS).
    I3: a) is_stable is not a constant of type `i1`. (Covered by ODS).
    I4: a) comparator is not a function. (Covered by ODS).
    C1: a) size(inputs) < 1. (Covered by ODS).
    C2: a) For any `i`, `type(inputs[i])` != `type(results[i])`.
    C3: a) Any tensors in `inputs` and `results` do not have the same shape. (Covered by ODS).
    C4: a) `dimension` < `-R` where `R` is rank of `inputs[0]`.
        b) `dimension` >= `R`, where `R` is rank of `inputs[0]`.
    C5: a) `comparator` does not have type
    `(tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1>`,
    where `Ei` is element type of `inputs[i]`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C2a: For any `i`, `type(inputs[i])` != `type(results[i])`.
    C4a: `dimension` < `-R` where `R` is rank of `inputs[0]`.
    C4b: `dimension` >= `R`, where `R` is rank of `inputs[0]`.
    C5a: `comparator` does not have type
    `(tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1>`,
    where `Ei` is element type of `inputs[i]`.
    ```
    
    Notes:
    * Simplified spec wording from "a variadic number of tensors in `foo`"
    to just "`foo`" for brevity.
    * Clarified the semantics to describe the op's behavior in more detail.
    
    closes #991
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit 76e76ce7ea7e865f8a2e124565d5a556d372490b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Apr 20 23:21:00 2023

    Add interpreter for ShiftRightArithmeticOp (#1431)
    
    Here are the constraints for the ShiftRightArithmeticOp:
    ```
    (I1) lhs is a tensor of integer type.
    (I2) rhs is a tensor of integer type.
    (C1) `lhs`, `rhs`, and `result` have the same type.
    ```
    I1, I2, and C1 are covered by the ODS, so no additional tests are added.
    
    Notes:
    * Corner cases (shift overflow) has not been accounted for: #1150
    
    closes #1113

[33mcommit 21bcf32ec64cc2dbdb75a2b98664ac23db4a3500[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Apr 20 22:33:35 2023

    enable a few interpreter tests (#1437)
    
    A few interpreter tests which might have missed enabling.

[33mcommit a67229858619361d55dcc6e54d2d1079d39c4f1a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Apr 20 22:28:46 2023

    Add interpreter for ShiftRightLogicalOp (#1429)
    
    Here are the constraints for the ShiftRightLogicalOp:
    ```
    (I1) lhs is a tensor of integer type.
    (I2) rhs is a tensor of integer type.
    (C1) `lhs`, `rhs`, and `result` have the same type.
    ```
    I1, I2, and C1 are covered by the ODS, so no additional tests are added.
    
    Notes:
    * Corner cases (shift overflow) has not been accounted for: #1150
    
    closes #1114

[33mcommit 86b2fa6a3817966821be143f307b3457ea10bb61[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Apr 20 22:15:47 2023

    Add interpreter for GetDimensionSizeOp (#1436)
    
    Here are the constraints for the GetDimensionSizeOp:
    ```
    (I1) operand is a tensor.
    (I2) dimension is a constant of type `si64`.
    (C1) 0 <= dimension < rank(operand).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) dimension is not a constant of type `si64`. (Covered by ODS).
    C1: a) 0 < dimension.
        b) dimension < rank(operand).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    ```
    C1a: 0 < dimension.
    C1b: dimension < rank(operand).
    ```
    
    closes #1432

[33mcommit 45a85ebd8afcc67429d7158c25af2381e80f74f9[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 20 18:24:46 2023

    Add missing dependency causing issues in google infra (#1435)
    
    Needed since `StablehloRefineShapes.cpp` includes `Base.h`:
    
    
    https://github.com/openxla/stablehlo/blob/main/stablehlo/transforms/StablehloRefineShapes.cpp#L45

[33mcommit 69a873702df5489911efef87f4674943730da630[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 20 18:14:49 2023

    Update `vhlo.md` with contribution guidelines. (#1433)
    
    Add guidelines for implementing compatibility for new StableHLO features
    in VHLO. Will be useful to link to this doc in future reviews.
    
    This does not impact anything about the compatibility process in
    `compatibility.md` or the Compatibility RFC, this is intended to aid
    developers.

[33mcommit 095fa9ef6df2058f68819d1b45afa211b4d1aebd[m[33m ([m[1;33mtag: [m[1;33mv0.10.0[m[33m)[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 20 15:54:53 2023

    Integrate LLVM at llvm/llvm-project@98f5a340975b (StableHLO 0.10.0) (#1434)
    
    Once merged and CI passes, this commit can be tagged for release 0.10.0.

[33mcommit 69b13c7fcdcfc61bd75d1315629289a7157e1f6c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Apr 20 02:37:46 2023

    Setup for StableHLO 0.10.0, serialization test and doc updates (#1430)
    
    This release sets everything up for StableHLO 0.10.0 which includes two
    new FP8 types.
    
    Edit: I will tag the LLVM Integrate PR which will follow this one.
    
    Closes #1409

[33mcommit a3164810bfcf72b5694034753bc3512e74b6b215[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Apr 20 00:28:46 2023

    Add interpreter for ShiftLeftOp (#1428)
    
    Here are the constraints for the ShiftLeftOp:
    ```
    (I1) lhs is a tensor of integer type.
    (I2) rhs is a tensor of integer type.
    (C1) `lhs`, `rhs`, and `result` have the same type.
    ```
    I1, I2, and C1 are covered by the ODS, so no additional tests are added.
    
    Notes:
    * Corner cases (shift overflow) has not been accounted for: #1150
    
    closes #1112

[33mcommit 2ca5f3d02ffb8a187d4b683060d24e90fddcfa38[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Apr 19 23:43:02 2023

    Fix llvm commit lint script to use new commit for SHA (#1426)
    
    Had to run the script twice to update SHA because it was using the old
    commit value to calculate the hash

[33mcommit 9d8000d38aa09acb733ea1e08377bf4adcea75f6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Apr 19 22:48:03 2023

    Add missing coverage tests for power (#1427)
    
    As identified from the lcov coverage report, added a test to cover the
    following lines
    
    ```
     // When the exponent is negative
     if (base.abs().isOne())
            exponent = exponent.abs();
    ```
    
    from the interpreter implementation of power op.

[33mcommit 65ce9e60ae6c6c8bf9dd04274d06c29f589b675f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 19 21:16:06 2023

    Add interpreter for RoundOp (#1424)
    
    Here are the constraints for the RoundOp:
    ```
    (I1) operand is a tensor of floating-point type.
    (C1) `operand` and `result` have the same type.
    ```
    I1 and C1 are covered by the ODS, so no additional tests are added.
    
    closes #1110

[33mcommit b1400753f1abd1578324cb2ca4dfdc1c20d2a6e1[m
Author: Jake Hall <60800749+jakeh-gc@users.noreply.github.com>
Date:   Wed Apr 19 20:35:31 2023

    Add Float8E4M3FNUZ and Float8E5M2FNUZ type support. (#1379)
    
    As proposed in [RFC: Float8E4M3FNUZ and
    Float8E5M2FNUZ](https://github.com/openxla/stablehlo/blob/main/rfcs/20230321-fp8_fnuz.md)
    (#1342), this change adds support for these types to StableHLO.
    
    This includes the type definitions, vhlo, and interpreter support. The
    testing approach mirrors the BFloat16 tests, since it is also a
    "non-standard" floating point type supported by StableHLO.

[33mcommit f2666c01852f75dd191c2bfcae7ddda0a1313933[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 19 19:39:06 2023

    Add interpreter for RoundNearestEvenOp (#1423)
    
    Here are the constraints for the RoundNearestEvenOp:
    ```
    (I1) operand is a tensor of floating-point type.
    (C1) `operand` and `result` have the same type.
    ```
    I1 and C1 are covered by the ODS, so no additional tests are added.
    
    closes #1111

[33mcommit 30fbc8c3d2c6c678b4c733ad5fa2d4a8070a26b2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 19 18:36:05 2023

    Add interpreter for PopulationCountOp (#1422)
    
    Here are the constraints for the PopulationCountOp:
    ```
    (I1) operand is a tensor of integer type.
    (C1) `operand` and `result` have the same type.
    ```
    I1 and C1 are covered by the ODS, so no additional tests are added.
    
    closes #1107

[33mcommit 808e6e75b13650647c24678c34d39a3d19864f30[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Apr 19 06:24:08 2023

    Streamline the pseudocode describing SignOp (#1395)
    
    Inspired by #1382, this PR: 1) reduces nesting, 2) drops the final ifs
    from the is_integer and is_float code blocks, 3) makes it clear that the
    NaN case for complex numbers returns NaN for both real and imaginary
    components.

[33mcommit e6427c3a8c42df42e9af39b9fce8a8b4b80867a4[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 18 21:05:33 2023

    Integrate LLVM at llvm/llvm-project@c2c49f464435 (#1415)

[33mcommit 7d3c274064a2c593f972944f931d5f57682c30b3[m
Author: Karthik Rangasai <39360170+karthikrangasai@users.noreply.github.com>
Date:   Tue Apr 18 19:56:29 2023

    Add interpreter for SignOp. (#1382)
    
    closes #1115
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of signed integer, floating-point, or complex type.
    (C1) operand and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    I1: a) operand is not a tensor of signed integer, floating-point, or complex type. (Covered by ODS).
    C1: a) type(operand) != type(result). (Covered by ODS).
    ```

[33mcommit 4a32703c93c92eab208b7025c6e9fa738666c811[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Apr 18 18:35:15 2023

    Document open questions in the quantization spec (#1408)
    
    Following up on #1352, this pull request documents the unresolved
    conversations from that PR review.

[33mcommit 98812667d4fe2b8f19c3f3761d97cdd65df9b270[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Apr 18 00:00:43 2023

    Add interpreter for Atan2Op (#1400)
    
    Here are the following constraints:
    ```
    (I1) `lhs` is a tensor of floating-point or complex type.
    (I2) `rhs` is a tensor of floating-point or complex type.
    (C1) `lhs`, `rhs`, and `result` have the same type.
    ```
    I1, I2, and C1 are covered by the ODS, so no additional tests are added.
    
    closes #1097

[33mcommit e83c5e05a0732a9457d246a6ea254113dd845e27[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Apr 14 02:03:26 2023

    Introduce QuantizedType (#1352)
    
    StableHLO dialect currently supports quantization via:
      1) Supporting `quant.uniform` element types.
      2) Having dedicated ops like `uniform_quantize` / `uniform_dequantize`.
      3) Allowing regular ops like `add` / `convolution` to take quantized
    tensors.
    
    This support was inherited from MHLO when StableHLO was bootstrapped,
    and MHLO support was motivated by mobile use cases and inherited from
    TFLite.
    
    As pointed out in #1149, StableHLO specification doesn't support
    quantization at the moment, and this is an important gap that we would
    like to fix before StableHLO v1.0 (see #588).
    
    To continue the discussion started in #1149 and to make progress towards
    v1.0, this pull request:
      A) Adds QuantizedType to the StableHLO specification, modelled after
    [TFLite quantization
    spec](https://www.tensorflow.org/lite/performance/quantization_spec).
      B) To start a conversation about the applications of QuantizedType and
    the semantics of quantized ops, proposes semantics for quantized `add`.
    
    TFLite quantization spec doesn't cover everything. It specs constraints
    on types (which we captured accordingly in this pull request), but it
    doesn't go into describing semantics of quantized ops.
    
    As a result, the proposed semantics for quantized `add` is intentionally
    naive, as compared with the much more involved implementations in the
    TensorFlow repository, e.g.:
      *
    [tfl.add](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/add.cc).
      *
    [tf.UniformQuantizedAdd](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/uniform_quant_ops/uniform_quantized_add_op.cc).
    
    upd: After community discussion, we removed the spec for quantized
    `add` leaving that for future work, since further alignment is required.
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit c0769c2712e144c08984eaa4c3c5a99a1d16ab83[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Apr 13 22:41:23 2023

    Add interpreter for MapOp (#1373)
    
    We have the following constraints in the spec:
    
    ```
    (I1) inputs: variadic number of tensors.
    (I2) dimensions: 1-dimensional tensor constant of type `si64`.
    (I3) computation: function.
    (C1) All `inputs` and `result` have the same shape.
    (C2) size(`inputs`) = N >= 1.
    (C3) `dimensions = [0, ..., R-1]`, where `R` = rank(`inputs[0]`).
    (C4) `computation` has type `(tensor<E0>, ..., tensor<EN-1>) -> tensor<E'>`
    where `Ek` = element_type(`inputs[k]`) and `E'` = element_type(`result`).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) inputs is not a variadic tensor. (Covered by ODS).
    I2: a) dimensions is not a 1-dimensional tensor.
        b) element_type(dimensions) != si64. (Covered by ODS).
    I3: a) computation is not a function. (Covered by ODS).
    C1: a) Not all `inputs` have the same shape. (Covered by ODS).
        b) `inputs` and `result` do not have the same shape. (Covered by ODS).
    C2: size(`inputs`) < 1. (Covered by ODS).
    C3: a) `dimensions != [0, ..., R-1]`, where `R` = rank(`inputs[0]`).
    C4: a) `computation` does not have type `(tensor<E0>, ..., tensor<EN-1>) -> tensor<E'>`
    where `Ek` = element_type(`inputs[k]`) and `E'` = element_type(`result`).
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2a: dimensions is not a 1-dimensional tensor.
    C3a: `dimensions != [0, ..., R-1]`, where `R` = rank(`inputs[0]`).
    C4a: `computation` does not have type `(tensor<E0>, ..., tensor<EN-1>) -> tensor<E'>`
    where `Ek` = element_type(`inputs[k]`) and `E'` = element_type(`result`).
    ```
    
    closes #1106

[33mcommit e0ec2bb3e375de5c9db9f1f817da4256689e365f[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Apr 12 22:58:54 2023

    Document RFC status (#1399)
    
    This adds status headers to all RFCs under rfcs/ to document their
    timeline and status.

[33mcommit 5313209af24f790e817a74c09b15ede0c686694e[m
Author: Aart Bik <39774503+aartbik@users.noreply.github.com>
Date:   Wed Apr 12 22:43:38 2023

    Sparsity RFC (#1143)
    
    A manifesto RFC that outlines a potential path forward for adding sparsity
    support to StableHLO. No changes to StableHLO are proposed at this point -
    this will be done in subsequent RFCs.

[33mcommit ad88bc5001b4c5a81a15b4b37473293ea3fc3746[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 12 22:11:03 2023

    Unify APIs in Element.h (#1398)
    
    closes #1397

[33mcommit 5e4b80e6143d7c3eb0f5dde65e2a03a884b601a0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Apr 11 23:10:17 2023

    Update IsFiniteOp test to use check op (#1391)

[33mcommit 81ef370e5c8d5e3289cdf87f83cd8b30c1a1c281[m
Author: Karthik Rangasai <39360170+karthikrangasai@users.noreply.github.com>
Date:   Tue Apr 11 20:34:41 2023

    Add interpreter for IsFiniteOp. (#1385)
    
    closes #1104
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of floating-point.
    (C1) operand and result have the same shape.
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) operand is not a tensor of floating-point type. (covered by ODS)
    (C1) shape(operand) = shape(result). (covered by ODS)
    ```

[33mcommit 88580c251db0c06559a05b049206325b30099a24[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Apr 11 20:28:55 2023

    Integrate LLVM at llvm/llvm-project@c8525e980bca (#1389)

[33mcommit 9163688e9c7bc43c6f3b999bbebefce3ee193c1e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Apr 11 18:02:39 2023

    Audit casts to ShapedType and TensorType (#1386)
    
    When working on adding support for tuple types to
    RefineCustomCallOpPattern, I noticed that some logic in
    StablehloRefineShapes.cpp assumes shaped types where we aren't
    guaranteed to have shaped types and casts to ShapedType at will.
    
    1) This led me to audit our uses of casts to ShapedType, and during that
    audit I realized that many of these casts are not needed because in
    many cases TableGen generates TypedValue accessors for stuff like
    `::mlir::TypedValue<::mlir::TensorType> getLhs()`. This simplified
    the code quite a bit.
    
    2) Furthermore, I realized that in some places in our code we use
    TensorType, so I audited casts to TensorType as well. To simplify
    further refactorings, I figured that we could standardize on just one
    kind of type between ShapedType and TensorType, and since ShapedType
    is more general, I replaced all usages of TensorType with ShapedType.
    
    3) Overall, after all the refactorings we still have some casts to
    ShapedType in ChloOps.cpp, StablehloOps.cpp and TypeInference.cpp.
    They are there for the following reasons:
      * Unlike ops, op adaptors don't produce TypedValue, and use
        regular Value instead. This means that there's a tradeoff between:
        a) TypeInference.cpp using Value and doing casts internally,
        b) TypeInference.cpp using TypedValue and forcing all users to
           do casts. I think a) is preferable.
      * Unlike singular fields, variadic fields don't produce TypedValue
        either, and instead use ValueRange, so stuff that comes from there
        also has to be cast.
    
    4) The refactorings exposed the following latent bugs in our code which
    could lead to crashes in the future. Even though these bugs cannot be
    exercised at the moment, I figured it would be good to safeguard against
    them:
      * inferReturnTypeComponentsFromOperands used to cast the result of
        inferReturnTypes to ShapedType. An oversight in user-level
        implementation of inferReturnTypes could lead to a crash.
      * ConstantOp::isCompatibleReturnTypes used to cast both the left-hand
        side and the right-hand side to TensorType. Same as the previous
        bullet.
      * refineReturnTypes used to cast types in op->getResultTypes()
        to ShapedType. Calling this functions on ops which can take
        tuples could lead to a crash.

[33mcommit d732f2a3b209823784c1fa13dccb3efb7c9221a3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Apr 11 17:49:50 2023

    Harden --stablehlo-canonicalize-dynamism (#1384)
    
    When working on improvements for indices_of_shape_operands, I noticed
    that --stablehlo-canonicalize-dynamism canonicalized some operations
    that it shouldn't have.
    
    I figured that I should add negative tests for all failure cases in
    StablehloCanonicalizeDynamismPass, and this uncovered several issues
    in the logic of the pass:
      * DynamicBroadcastInDimOp::output_dimensions,
        DynamicIotaOp::output_shape, DynamicReshapeOp::output_shape were
        discarded even if they weren't constants.
      * DynamicGatherOp/DynamicPadOp/RealDynamicSliceOp canonicalizations
        were using type inference to create static versions of the ops,
        which means that they could change the original result type,
        potentially producing invalid programs if someone downstream was
        relying on this exact result type.
    
    This pull request provides fixes to those issues and comprehensively
    covers all notifyMatchFailure cases in
    StablehloCanonicalizeDynamismPass to make sure that this doesn't happen
    again.

[33mcommit daf0e9665b13272bab8d30e94cb640a22cd20cae[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Apr 11 16:45:34 2023

    Drop indices_of_shape_operands once they are no longer needed (#1383)
    
    --stablehlo-canonicalize-dynamism can now drop indices_of_shape_operands
    once they are no longer needed, similarly to how it transforms
    dynamic_foo ops to foo ops when operands like output_shape are no longer
    needed.
    
    To make sure that we only do the transformation on valid IR, I also
    wrote verification logic for indices_of_shape_operands as well as tests
    for this logic.
    
    In the future, when we upgrade indices_of_shape_operands from an
    experiment to a full-fledged StableHLO feature, this logic will be moved
    to a proper verifier.

[33mcommit 7f6976e15595d3a37ad177a58b4a3c2024d9ae9c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Apr 6 20:00:41 2023

    Introduce --stablehlo-canonicalize-dynamism (#1378)
    
    This pull request achieves two goals:
    
    1) Makes specialization of dynamic StableHLO programs completely
    independent from MHLO. Before this PR, we were depending on StableHLO =>
    MHLO and then MHLO canonicalization to actually specialize away
    dynamic_foo ops, but not anymore.
    
    2) Provides a platform for addressing #1367. As a follow-up to this PR,
    we will add another pattern to this pass that transforms CustomCallOp to
    erase the indices_of_shape_operands attributes as well as the operands
    called out in indices_of_shape_operands.
    
    The patterns in this PR are quite straightforward, since almost all of
    the work is done in --stablehlo-refine-shapes.
    CanonicalizeRealDynamicSliceOpToDynamicSliceOpPattern is the only
    exception because it isn't just about "the result has static shape,
    let's go" or "the attributes are all static, let's go".
    
    Furthermore, this PR extends some existing verifiers and adds some new
    verifiers for dynamic ops. Since dynamic ops aren't specced yet, it is
    expected that they have implementation gaps, but in this case, these
    gaps would've allowed invalid dynamic ops to get transformed into valid
    static ops, so I fixed them right away.

[33mcommit 980db6032f0fef63c80f2eaa78a467da788a1f22[m
Author: Jake Hall <60800749+jakeh-gc@users.noreply.github.com>
Date:   Wed Apr 5 22:32:08 2023

    [RFC] Add Float8E4M3FNUZ and Float8E5M2FNUZ to StableHLO. (#1342)
    
    This is a proposal to add Float8E4M3FNUZ and Float8E5M2FNUZ floating
    point types to StableHLO.
    Feedback welcome, see `rfcs/20230321-fp8_fnuz.md` for more details.

[33mcommit 092659fbc540368bce9047ef5427a5ebb2106b81[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 5 21:19:17 2023

    Add interpreter for ClzOp (#1372)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor of integer type.
    (C1) `operand` and `result` have the same type.
    ```
    I1 and C1 are covered by the ODS, so no additional tests are added.
    
    closes #1100

[33mcommit 682cb76be469b79f1afc30553ce9fdab76c7fe0c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 5 21:09:58 2023

    Add interpreter for WhileOp (#1281)
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand: variadic number of tensors or tokens
    (I2) cond: function
    (I3) body: function
    (C1) `cond` has type `(T0, ..., TN-1) -> tensor<i1>`, where
         `Ti` = `type(operand[i])`.
    (C2) `body` has type `(T0, ..., TN-1) -> (T0, ..., TN-1)`, where
         `Ti` = `type(operand[i])`.
    (C3) For all `i`, `type(results[i])` = `type(operand[i])`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not variadic tensors or tokens. (Covered by ODS).
    I2: a) cond is not a function. (Covered by ODS).
    I3: a) body is not a function. (Covered by ODS).
    C1: a) `cond` does not have type `(T0, ..., TN-1) -> tensor<i1>`, where
         `Ti` = `type(operand[i])`.
    C2: a) `body` does not have type `(T0, ..., TN-1) -> (T0, ..., TN-1)`, where
         `Ti` = `type(operand[i])`.
    C3: a) For any `i`, `type(results[i])` != `type(operand[i])`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1a: `cond` does not have type `(T0, ..., TN-1) -> tensor<i1>`, where
         `Ti` = `type(operand[i])`.
    C2a: `body` does not have type `(T0, ..., TN-1) -> (T0, ..., TN-1)`, where
         `Ti` = `type(operand[i])`.
    C3a: For any `i`, `type(results[i])` != `type(operand[i])`.
    ```
    
    closes #992

[33mcommit 1d6a8587ea959db3574c1311905a8bf1ef509db0[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Wed Apr 5 14:31:34 2023

    Integrate LLVM at llvm/llvm-project@588da01621a (#1376)

[33mcommit 9b206122ff447eac3ff2bfc92c1566b327310efc[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Apr 5 00:54:25 2023

    Order functions alphabetically in Ops.cpp (#1374)

[33mcommit 015f66c959b94447f3e26382d0cbec8af0b2fd81[m
Author: David Huntsperger <5672572+pcoet@users.noreply.github.com>
Date:   Tue Apr 4 14:18:03 2023

    StableHLO doc review (#1363)
    
    I reviewed the StableHLO docs and did some light copy editing. I had to
    timebox this work, and I was only able to skim the spec. Overall,
    content is well written, thoughtful, has a friendly tone -- good stuff!

[33mcommit 0f8ddefda2102a47aa3be245587000bdb2cb89bb[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Apr 4 01:25:35 2023

    refactor to improve evalOp function type checking (#1347)
    
    This is to make sure that, during compilation, the
    interfatce-type-checking for the `evalOp*` function calls in eval
    function
    (https://github.com/openxla/stablehlo/blob/311f14ce78c7fe35d304ee91007b58c335cf821e/stablehlo/reference/Ops.cpp#L484)
    are relying on the declarations in Ops.h. Currently, the checking is
    happening locally within Ops.cpp as the calls to those `evalOps*`
    functions are preceded by the corresponding definitions. The problem
    with the current setup is that a change in declaration Ops.h will go
    unnoticed, which is what exactly happened in
    https://github.com/openxla/stablehlo/pull/1343. The proposed fix will
    make sure that such inconsistencies can be caught during compilation
    time.

[33mcommit a2c36eb790c5e70109cf3c2b55f43dcdc779727e[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Mon Apr 3 20:06:15 2023

    Integrate LLVM at llvm/llvm-project@ed372d194f93 (#1368)

[33mcommit 12a0ff94b57982edb0a745e57c37e1d53ff90c74[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Mar 31 14:54:23 2023

    Update function type for return with multiple operands, some repeating (#1362)
    
    The way the refine function type algorithm works currently has an
    assumption that ReturnOp does not have redundant operands. This is not
    always the case in practice.
    
    Several JAX tests were hitting this issue.
    
    
    Closes #1350

[33mcommit 469b545901301c0336ff0cb8941dd68ed942a1a4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Mar 30 18:16:58 2023

    Enabling a few passing interpreter tests (#1360)
    
    As part of https://github.com/openxla/stablehlo/issues/1291,
    enabling a few test which should have passed with current state of
    interpreter, without any change of precision.

[33mcommit 7a93924407a66570cb28ae379c6644759894737f[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Mar 29 21:59:54 2023

    Integrate LLVM at llvm/llvm-project@c6a132a8023d (#1359)

[33mcommit b10da40fcdf8997420e0aa7a7d9c01f441129c2c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 28 14:46:52 2023

    Add Python API version number (#1353)
    
    Adds an arbitrary version number which gets incremented on all changes
    to public APIs.
    
    StableHLO API's do not offer any stability. This change may be useful
    for filtering tests that require bug fixes, etc. The goal is to help
    downstream StableHLO integration run more smoothly as the API's/features
    evolve.

[33mcommit 458bdd95771e9861e6488868e315a1b0340058ba[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Mar 28 00:36:01 2023

    Integrate LLVM at llvm/llvm-project@42058eea7912 (#1356)

[33mcommit 94fa4eb8d7e4665e4010180bab20e593ac984657[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 24 23:33:24 2023

    [RFC] Increase backward compatibility guarantees to 6 months (#1306)
    
    We have recently had the first official release of StableHLO, which
    fully implemented the compatibility guarantees established by the
    compatibility RFC accepted in December.
    
    These guarantees ended up being more compelling than we anticipated in
    the RFC discussions, and there are stakeholders who are interested in
    using them in production right away. To enable that, we have received a
    request to bump up the backward compatibility guarantees to 6 months
    (from 1 month for the 0.x.x series, as established by the original RFC).
    
    I would like to proposes to fulfil this request to further strengthen
    the practical usefulness of the StableHLO opset, given that the
    additional maintenance cost looks acceptable (5 extra months of
    maintaining backward-compatible VHLO ops).
    
    This RFC does not affect the long-term compatibility guarantees
    established for the 1.x.x series and onwards, which remain at 5 years of
    forward and backward compatibility.

[33mcommit 466475448ae87988b9478e56af00c0fba231ccc1[m
Author: David Majnemer <david.majnemer@gmail.com>
Date:   Fri Mar 24 23:31:14 2023

    [RFC] Add a new type to StableHLO: E4M3B11FNUZ (#1308)
    
    This is a proposal to add a new floating point type to StableHLO, please
    see rfcs/20230309-e4m3b11.md for more details.

[33mcommit ef7a111784a2a5574de0b1165e3bdfc5397dce5a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Mar 22 19:07:40 2023

    Integrate LLVM at llvm/llvm-project@411b1d8f0795 (#1348)

[33mcommit d2e64e4d72c11caac054b4271f89164e78a90526[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Mar 22 16:06:30 2023

    Fix constraints for TransposeOp (#1345)
    
    Fixes the type constraint (C3) of [transpose
    op](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#transpose).
    
    fixes https://github.com/openxla/stablehlo/issues/1339. Please refer to
    the same ticket for more details.
    
    Note that the implementation of the (C3) is still correct
    https://github.com/openxla/stablehlo/blob/076c6abc47edb53ed6510fec638112a4dc78ec33/stablehlo/dialect/TypeInference.cpp#L2898

[33mcommit 7e586e09c31382492862cdcdbd131eb7c849cc0d[m
Author: anakinxc <103552181+anakinxc@users.noreply.github.com>
Date:   Wed Mar 22 15:54:43 2023

    Fix CompareOp interpreter interface inconsistency (#1343)
    
    It looks like the decl of evalCompareOp has an extra `compareType`
    parameter.

[33mcommit 076c6abc47edb53ed6510fec638112a4dc78ec33[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 22 15:27:35 2023

    Add Python Serialization APIs (#1330)
    
    Adds API to determine earliest forward compatible version that is able
    to be targeted by serialization. Currently we offer 1 month forward
    compatibility.
    
    Closes #1301

[33mcommit 311f14ce78c7fe35d304ee91007b58c335cf821e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Mar 22 01:08:49 2023

    Spec update for ConvolutionOp: Clarify permute (#1340)
    
    fixes https://github.com/openxla/stablehlo/issues/1338
    Please refer to the https://github.com/openxla/stablehlo/issues/1338 for
    more details.

[33mcommit cc287f69b7115725164f26a5a164f09f9dae091e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Mar 20 18:50:44 2023

    Integrate LLVM at llvm/llvm-project@a7cf2892e964 (#1337)

[33mcommit c227a2cf32d20d33aaf9863a33d1da0e7a415d16[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Mar 20 18:50:08 2023

    Add interpreter for CompareOp (#1199)
    
    We have the following constraints in the spec:
    
    ```
    (I1) lhs tensor.
    (I2) rhs tensor.
    (I3) comparison_direction enum of `EQ`, `NE`, `GE`, `GT`, `LE`, or `LT`.
    (I4) compare_type enum of `FLOAT`, `TOTALORDER`, `SIGNED`, or `UNSIGNED`.
    (C1) `lhs` and `rhs` have the same element type.
    (C2) `lhs`, `rhs`, and `result` have the same shape.
    (C3) Given `E` is the `lhs` element type, the following are legal values of
         `compare_type`:
         * If `E` is signed integer type, `compare_type` = `SIGNED`.
         * If `E` is unsigned integer or boolean type, `compare_type` = `UNSIGNED`.
         * If `E` is floating-point type,
           `compare_type` $\in$ {`FLOAT`, `TOTALORDER`}.
         * If `E` is complex type, `compare_type` = `FLOAT`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) lhs is not a tensor. (Covered by ODS).
    I2: a) rhs is not a tensor. (Covered by ODS).
    I3: a) comparison_direction is not enum of `EQ`, `NE`, `GE`, `GT`, `LE`, or `LT`. (Covered by ODS).
    I4: a) compare_type is not enum of `FLOAT`, `TOTALORDER`, `SIGNED`, or `UNSIGNED`. (Covered by ODS).
    C1: a) element_type(lhs) != element_type(rhs). (Covered by the ODS).
    C2: a) lhs, rhs, and result do not have the same shape. (Covered by the ODS).
    C3: a) `E` is signed integer type and `compare_type` != `SIGNED`.
        b) `E` is unsigned integer type and `compare_type` != `UNSIGNED`.
        c) `E` is boolean type and `compare_type` != `UNSIGNED`.
        d) `E` is floating-point type and `compare_type` $\notin$ {`FLOAT`, `TOTALORDER`}.
        e) `E` is complex type and `compare_type` != `FLOAT`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C3a: `E` is signed integer type and `compare_type` != `SIGNED`.
    C3b: `E` is unsigned integer type and `compare_type` != `UNSIGNED`.
    C3c: `E` is boolean type and `compare_type` != `UNSIGNED`.
    C3d: `E` is floating-point type and `compare_type` $\notin$ {`FLOAT`, `TOTALORDER`}.
    C3e: `E` is complex type and `compare_type` != `FLOAT`.
    ```
    
    Notes:
    * However, due to the issue noted in #1071 and given the higher priority
    work of reference implementation, this spec compliance work (i.e.
    verifications for (C3)) will be deferred.
    * Updated the spec wording to be clear about special behaviors.
    * Also reorders Element.cpp and Element.h `operatorFoo` functions to be
    in lexicographical order.
    * We plan to drop complex element type support for comparison directions
    {GE, GT, LE, LT} in #560, so these were not implemented.
    
    closes #967

[33mcommit a5d8529b047acac3773017dce04fd42020262a53[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 17 22:58:10 2023

    Fix tests by updating STABLEHLO_TOOLS_DIR (#1335)
    
    `set(STABLEHLO_TOOLS_DIR ${CMAKE_BINARY_DIR}/bin)` is the right way of
    obtaining a path to the directory that contains stablehlo-opt and
    stablehlo-translate.
    
    CMAKE_CURRENT_BINARY_DIR works as well if we're building standalone, but
    that breaks down when building as part of another project, which we
    discovered when integrating into MLIR-HLO. This pull request fixes this
    oversight.

[33mcommit 2849dc33ae1eb80663185b8955d54a86ff550fe6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Mar 17 18:35:19 2023

    fix a typo in reduceOp spec (#1333)
    
    ReduceOp can generate multiple results and that is not reflected in the
    summary.

[33mcommit d07a07676e507a47a01554c69e1e4a84198907b7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Mar 17 00:42:06 2023

    Integrate LLVM at llvm/llvm-project@5cb8381aea56 (#1332)

[33mcommit a374f05195a0cb1262ba399f3f3d90176b8d572b[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Mar 17 00:30:53 2023

    Load VHLO before deserialization (#1331)
    
    Since VHLO is an implementation detail of StableHLO, we shouldn't expect
    users of the API to pre-load the dialect.
    
    Serialization isn't an issue since we list dependent dialects in our
    conversion passes which get loaded.

[33mcommit 1f336dbb5176e91680738096ecf8dec75f5f885a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 14 19:06:26 2023

    Print commit and sha on update (#1325)
    
    Print the updated values to the terminal. It seems like sometimes the
    SHA update fails, and I'm not sure if it's the sed failing or something
    else. I haven't been able to repro with the echo yet, but keeping it
    there will help future instances of debugging.

[33mcommit 6e103f95ca8caf76f22f42f9baad5995f4bcbf20[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 14 19:06:13 2023

    Fix issues found during integration testing. (#1326)
    
    The TypeInference signatures mismatch - not sure why they work in our
    CI, but fail during integ.
    
    ```
    //.h
    LogicalResult verifyConvolutionAttributes(
        std::optional<Location> location, Value lhs, Value rhs,
    
    // .cpp
    LogicalResult verifyConvolutionAttributes(
        std::optional<Location> location, Type lhsType, Type rhsType,
    ```
    
    Refine shapes doesn't handle complex numbers properly, minor update to
    get their underlying type.

[33mcommit 63b6f5ab7ab9059a9134a0ee1a49e731ab376c5f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Mar 13 21:17:38 2023

    Update interpreter checklist (#1323)
    
    The checklist is updated to enable additional tests in the `testdata`
    directory now that we have them to use extensively.

[33mcommit 0c7b9b19a0a16e9c53c3b91622714ff21edb1fd0[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Mar 13 20:35:43 2023

    Integrate LLVM at llvm/llvm-project@83586a61b162 (#1324)

[33mcommit 551db9eb601632fabebaaa1ccf0ff0e1dd1d623c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Mar 13 04:43:08 2023

    Add support for running FileCheck tests in the Bazel build (#1321)
    
    Add support for running FileCheck tests in the Bazel build
    
    Before this PR, only CMake build could actually ran tests, and Bazel
    build could only build things.
    
    With this PR, Bazel build can now test everything except for a few
    tests that involve Python bindings. This is relatively acceptable for
    now, and we have #362 to keep track of specifically Python tests.
    
    While learning about FileCheck support in Bazel and applying this
    knowledge to our repositoru, I discovered that our CMake build for
    FileCheck tests was vastly overcomplicated, so I simplified it
    considerably.
    
    Finally, I noticed that ci_build_llvm.sh builds mlir-cpu-runner.
    This is something that we got rid of in README.md recently, and I don't
    think we need this in CI either.
    
    Fixes #1315

[33mcommit 03e820fae8762607137d4292d859e28a95416f9b[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Mar 13 03:47:24 2023

    Synchronize our BUILD.bazel with downstream build (#1320)
    
    Our BUILD.bazel file has diverged from the downstream build file a
    little bit, and this PR gets them back in sync again.

[33mcommit 3722fce004b08668bc364c88aef84b24e116ac87[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Mar 10 18:45:00 2023

    Integrate LLVM at llvm/llvm-project@fbcca1bc03d3 (#1312)

[33mcommit 3504260f7b9d75a6c0208843a8a4bf9b2d9e7f76[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 10 17:42:53 2023

    Streamline the on: part of all GitHub workflows (#1310)
    
    I was looking to fix the long-standing issue with checks being skipped
    by paths/paths-ignore, leading to: 1) spuriously blocked merges if the
    checks are required, 2) weird "in progress" dots next to PRs.
    
    I discovered that there is no easy solution to this, but there is a
    workaround:
    https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/defining-the-mergeability-of-pull-requests/troubleshooting-required-status-checks#handling-skipped-but-required-checks.
    
    This solution works, but it's quite fiddly. Defining a shadow workflow,
    making sure that its paths/paths-ignore are an exact negation of the
    real workflow - this got old pretty fast.
    
    Then I realized that we don't have to suffer through this. We can just
    enable all our workflows on all pull requests. Now that we have fast CI
    runners, waiting for unnecessary builds when we only update docs would
    take a few minutes at most, and I think it's a worthy tradeoff for
    getting rid of awkwardness.
    
    So I went and removed all paths/path-ignore from all our workflows. When
    doing that, I noticed that their on: parts are different between one
    another, so I went ahead and unified that too. If we're running builds
    on pushes, we might as well run lints on pushes too.

[33mcommit 354ca18e028c42067d97a9226297a01b2e561eb4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 10 02:15:57 2023

    Link recently added frontend contract tickets in the spec (#1299)
    
    Today I realized that all "Consider removing" tickets are actually
    Frontend Contract tickets, not New Features. If we don't clean them up
    now in preparation for StableHLO v1.0, we will have to support them for
    a long long time.
    
    The outcome of some of these tickets might actually be "we considered
    removing this feature because it has problems, but it's actually in use
    so we'd need to introduce a new feature to replace it", but we still
    need to get them through the Frontend Contract phase.
    
    This PR updates the spec to call out these tickets. Also, it cleans up
    mentions of Specification Compliance tickets because those belong in the
    StableHLO dialect (there can be multiple implementations of the spec, so
    we shouldn't be preferring one of them in the spec by linking to its
    tickets).

[33mcommit d36c04e5dda66697fb7a47e16f345aec6508b1e1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Mar 10 00:50:07 2023

    Enable tests with the latest precision value (#1305)
    
    As mentioned here
    https://github.com/openxla/stablehlo/issues/1291#issuecomment-1463021997,
    the PR enables some tests based on the latest precision value
    https://github.com/openxla/stablehlo/pull/1278.

[33mcommit 6675668b412fd44cbd6a26334aace7371da9c6c8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 10 00:47:18 2023

    Fix formatting in the rfcs/ directory (#1307)
    
    Looks like one of our RFCs has formatting which is not compliant with
    our linters. This PR fixes that.

[33mcommit 96d832eaf5de241c09299608a09416be7412e35d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 10 00:12:12 2023

    Retire the link to GitHub Discussions (#1304)
    
    Following
    https://github.com/openxla/community/commit/62833d737ed5fc6756e0cc1b10aa44d9979b5790,
    this PR replaces the link to GitHub Discussions with a link to
    openxla-discuss@.

[33mcommit 78b665367b6dcaca9ccc180d3ca865481f7c4c4c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Mar 9 21:08:13 2023

    Implement prettyprinting for dot_general (#1276)
    
    Before:
    
    ```
      %0 = "stablehlo.dot_general"(%arg0, %arg1) {
        dot_dimension_numbers = #stablehlo.dot<
          lhs_batching_dimensions = [0],
          lhs_contracting_dimensions = [2],
          rhs_batching_dimensions = [0],
          rhs_contracting_dimensions = [1]
        >
      } : (tensor<2x2x2xi8>, tensor<2x2x3xi8>) -> tensor<2x2x3xi32>
    ```
    
    After:
    
    ```
      %0 = stablehlo.dot_general %arg0, %arg1,
        batching_dims = [0] x [0], contracting_dims = [2] x [1]
        : (tensor<2x2x2xi8>, tensor<2x2x3xi8>) -> tensor<2x2x3xi32>
    ```
    
    The batching_dims part is optional and can be dropped if the
    corresponding dims are empty.

[33mcommit 1d83673befb7b564d5df34c82eae9670c5fe2010[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Mar 9 21:07:55 2023

    Refactor CustomCallOp evaluator (#1292)
    
    This PR is to refactor the custom-call eval function which has been
    recently added to improve interpreter testing (#1290).

[33mcommit 73b4ece806409b8fc66e386d2a10e379f2ef1fa2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Mar 9 18:26:03 2023

    Update reference checklist example (#1300)
    
    The example is updated to explicitly show standard format discussed in
    the first bullet point under "**In `StablehloOps.td`**" section.

[33mcommit 3e7c020d9c22e5d5f3bf989c70d7be1d125e5f65[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Mar 9 18:25:36 2023

    Add interpreter for LogisticOp (#1239)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor of floating-point or complex type. (Covered by ODS).
    (C1) `operand` and `result` have the same type. (Covered by ODS).
    ```
    
    No additional tests are needed as all constraints and shape inference is
    covered by ODS.
    
    closes #978

[33mcommit 09c5a2f59fe7744a1b4217cff0d22a3f55c7daef[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Mar 9 18:19:40 2023

    Significantly compress syntax for bounded dynamism (#1277)
    
    Before:
    
    ```
      tensor<?x4xf32, #stablehlo.type_extensions<bounds = [4, ?]>>
    ```
    
    After:
    
    ```
      tensor<?x4xf32, #stablehlo.bounds<4, ?>>
    ```
    
    Old syntax is still accepted for compatibility reasons.
    
    Additionally, this PR makes a few cleanups which I noticed when working
    on it:
      1) TypeInference.h now takes HloDialectInterface* instead of Dialect*
         to increase type safety, similarly to newly introduced functions
         in AssemblyFormat.h.
      2) parseAttribute/parseType error messages were changed to say
         "StableHLO" instead of "stablehlo", to use the canonical name
         of the StableHLO dialect.

[33mcommit 4d55905c22be18b5535ab9dfdef058a4bc804d51[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Mar 9 17:15:34 2023

    Improve documentation around serialization (#1302)
    
    This PR changes the docs to:
      1) Clearly indicate in README.md that StableHLO has a serialization
         format.
      2) Use bytecode.md as a hub for documenting the serialization format
         and how to write/read it.

[33mcommit 29d280e43e39241ed6c74385e15fef0328e837f5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Mar 9 04:53:08 2023

    Update signature of `inferReverseOp` function for re-usability (#1295)
    
    This is a follow up PR to #1019 that partially addresses the proposal in
    #1000 and #1028. This PR enables ConvolutionOp to share ReverseOp's
    shape inference code.

[33mcommit d45977491282231b7d26f691ad15927bea8f53ff[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Mar 9 02:42:11 2023

    Update how pad is used in spec (#1297)
    
    Fix a bug in how pad is used in specification.

[33mcommit 98f3c30d2b194bf9e954b900e7afbc70e7bafcfd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Mar 8 15:49:24 2023

    Update signature of `inferConvolutionOp` function for re-usability (#1296)
    
    This is a follow up PR to #1019 that partially addresses the proposal in
    #1000 and #1028. This PR enables ConvolutionOp to
    use its shape inference code in the interpreter as well. The signature
    of `verifyConvolutionOp` is also updated as well.

[33mcommit 3d8ef1488f66857f0cc48c3bce7b1750cca7239b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Mar 8 04:35:08 2023

    Update signature of `inferPadOp` function for re-usability (#1294)
    
    This is a follow up PR to #1019 that partially addresses the proposal in
    #1000 and #1028. This PR enables ConvolutionOp to share PadOp's shape
    inference code.

[33mcommit 701e00b79653f29ce18daa7a98530868db664ec6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Mar 7 23:53:30 2023

    Fix compilation error on macOS (#1293)
    
    Just `0l` is ambiguous between multiple Element::Element constructors,
    including the ones that take int64_t, double and bool.

[33mcommit 3432ac0eda227d81f7a10cf5f53f2f3a4efb6939[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Mar 7 22:33:26 2023

    Fix for float diff error on Mac platform (#1278)
    
    fixes https://github.com/openxla/stablehlo/issues/1275

[33mcommit 114bb5d3bbc3144826a9c722ccde0ae24b5b89eb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 21:30:52 2023

    Add reference interpreter tests to compatibility suite (#1290)
    
    Spoke with @sdasgup3 / @ghpvnist about submitting these tests. Decided
    integer interpreter tests are safe to submit now, will keep FP tests
    filtered until we figure out proper FP thresholds.
    
    - V0: Filter all tests using `RUN-DISABLED:`
    - V1(now): Unfilter integer and boolean tests for supported ops. Filter
    supported op FP tests using `RUN-DISABLED(#1278):`
    - V2(after #1278): Unfilter floating point tests for supported ops.
    - Vfuture: Spec workstream will decide on process for unfiltering TPs
    for newly supported ops.
    
    Logfiles with existing failures can be found at commit:
    https://github.com/openxla/stablehlo/commit/b8875e16dace1cfac8691f8a223974ce785da973
    (these have been removed before submit, but commit should live on)

[33mcommit 3b30ebb7709a4561fc5abf8d380582e753f30523[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 19:44:30 2023

    Remove unnecessary include (#1288)
    
    Found during integrate, we don't have a bazel dependency for this
    include file because it is unused.

[33mcommit 79b3e7ed5dec1ed72ca1ff1afff168d16ad3c843[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 19:38:57 2023

    Update serialization compatibility tests to use stablehlo-translate APIs (#1289)
    
    Update all tests to use the following:
    
    ```
    // RUN: diff <(stablehlo-translate --deserialize %s.0_9_0.bc | stablehlo-opt) <(stablehlo-opt %s)
    // RUN: diff <(stablehlo-translate --serialize --target=current %s | stablehlo-translate --deserialize | stablehlo-opt) <(stablehlo-opt %s)
    ```
    
    The extra pipe through `stablehlo-opt` is needed because the upstream
    implementation of opt includes two trailing whitespaces while upstream
    translate only uses one.

[33mcommit 48b32cb5126471481774244ddd8fee2f3efe66e1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 18:14:31 2023

    Rename stablehlo-interpreter --> stablehlo-translate, fix tests/ (#1286)
    
    Rename interpreter tool to translate tool since it now does translate,
    serialize, and deserialize.

[33mcommit 4c64eb94cbd0a5a1ee8ae76c279ffe749e2d09e1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 17:48:42 2023

    Add StableHLO portable artifact serialization APIs (#1272)
    
    Add serialization APIs:
    
    ```c++
    // Write a StableHLO program to a portable artifact
    LogicalResult serializePortableArtifact(ModuleOp module,
                                            StringRef targetVersion,
                                            raw_ostream& os);
    
    // Read StableHLO portable artifact
    OwningOpRef<ModuleOp> deserializePortableArtifact(StringRef sourceStr,
                                                      MLIRContext* context);
    ```
    
    This is exposed via our translator tool `stablehlo-interpret`. I'll
    rename this tool in a followup to `stablehlo-translate` since that is
    more in-model, wanted to separate that fix to keep this PR concise.
    
    ```
    stablehlo-translate --serialize --target=0.9.0 file.mlir > file.mlir.bc
    stablehlo-translate --deserialize file.mlir.bc
    ```
    
    This creates an MLIR Bytecode File with the header
    `VHLO_serializerVerion_targetVersion`. I.e. for a `0.11.0` libStablehlo
    targeting `0.9.0`, the bytecode header reads: `VHLO_0.11.0_0.9.0`.
    
    Still todo:
    - Rename translate tool
    - Update serialization tests to use `stablehlo-translate --deserialize`
    - Expose these API's via Python
    
    Closes #1262

[33mcommit e9aae476e2c717d2753836050182115ff6848025[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Mar 7 17:48:25 2023

    Integrate LLVM at llvm/llvm-project@31c39439a894 (#1285)

[33mcommit 59617387f4a5df2cd010b1324a15fc1a92a5e185[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Mar 7 01:02:54 2023

    Improve newcomer experience (#1274)
    
    I've tried to follow our build instructions on my personal macbook to
    see how smooth the experience is and how onerous it is to run the
    testdata/ suite on a reasonably mainstream machine.
    
    Turns out that our instructions don't really work on macOS, and
    predictably checking the entire testdata/ takes a considerable amount of
    time (20+ seconds on a 2021 M1 Pro macbook).
    
    This PR fixes the build instructions to work on macOS and updates the
    CMake target called out in the readme to only check the tests/ suite.

[33mcommit 9c3321d3351e6e15c553b9fa2ed2d8c61d6460d9[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Mon Mar 6 20:09:32 2023

    Add interpreter for PowOp (#1207)
    
    closes #981
    
    We have the following constraints in the spec:
    
    ```
    (I1) lhs is a tensor of integer, floating-point, or complex type.
    (I2) rhs is a tensor of integer, floating-point, or complex type.
    (C1) lhs, rhs, and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) lhs is not a tensor of integer, floating-point, or complex type. (covered by ODS)
    (I2) rhs is not a tensor of integer, floating-point, or complex type. (covered by ODS)
    (C1) lhs, rhs, and result do not have the same type. (covered by ODS)
    ```
    
    Add a binary version of mapWithUpcastToDouble function.

[33mcommit b9c1357349becaa5585bbd503e6f5eace96c0c44[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Mar 5 02:03:05 2023

    Sync check ops with IREE check ops (#1230)
    
    fixes https://github.com/openxla/stablehlo/issues/1219
    
    Aligning [our Check
    dialect](https://github.com/openxla/stablehlo/blob/main/stablehlo/tests/CheckOps.td)
    with [IREE's Check
    dialect](https://github.com/openxla/iree/blob/main/compiler/src/iree/compiler/Modules/Check/IR/CheckOps.td),
    so that the conformance suite could also be used verbatim as a test
    suite for IREE.
    
    
    - **Alignment w.r.t the op signature:** The PR aligns the
    `stablehlo::check` ops to have signature comparable with IREE's check
    dialect ops so as to meet the purpose mentioned above.
    - **Alignment w.r.t op semantics:** Ops from both the dialects are using
    a tolerance based approach to compare floating point values for near
    equality.
    The error bound for IREE ops
    ([here](https://github.com/openxla/iree/blob/0312dd3c67852c6fa97df072cc10bbc00aecb9c3/runtime/src/iree/modules/check/module.cc#L69))
    is a flat epsilon value used for all float comparisons. The StableHLO
    check ops uses a relative (w.r.t the magnitude of the values under
    comaparison) epsilon value
    ([here](https://github.com/openxla/stablehlo/blob/8496d542f13b2f81ef6648ba1ac414d0f9343ade/stablehlo/reference/Element.cpp#L124)).
    From the choice of epsilon values, we can roughly say that the error
    bound, for StableHLO check ops, will be tighter while comparing normal
    values whose absolute values is less than a threshold (~840 = `0.0001 /
    std::numeric_limits<float>::epsilon()` for float dtype), beyond which
    the bound will be more generous. The quality of the errors bounds can be
    further evaluated (and ,if needed, can be adjusted) based on the
    conformance test runs.

[33mcommit d593b1bacd6edca08fb699174e0e80c72a2c8f62[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Mar 4 06:39:00 2023

    Fix typo in ReverseOp's spec example (#1273)

[33mcommit 74f65fec3b821ee83aed7b64732087563651e4bd[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Fri Mar 3 22:04:58 2023

    Fix crash in typechecker (#1271)
    
    fixes #1264

[33mcommit c081eab9c86b0e8d2fb207890566310116581ccf[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 3 18:20:23 2023

    Standardize summaries (#1266)
    
    MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/d4f40974fe47f0ce7b5b03ac4ee1d6b1bc77d3ba.
    
    Closes #611

[33mcommit 011b2a88d543cbe527e85c55ca42b278a310d5c2[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Mar 3 18:19:51 2023

    Use std::optional instead of llvm::Optional (#1267)
    
    MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/15a5cd35cca34f39cd528eaa6beda0d828ea7331

[33mcommit 2ec241c985639d711657edb7040ea505a4e802c7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Mar 3 00:32:13 2023

    Two minor bugs fixes in lint whitespace check (#1265)
    
    1. Pass if file list to lint is empty.
    2. Enforce extension checking at end of file (was checking .mlir.bc
    files)
    
    Given the pipe into bash using $0:
    
    ```
    get_source_files | xargs -L1 bash -c 'test "$(tail -c 1 "$0")"
    ```
    
    This requires the output of get_source_files to be non-empty. Added a
    `-z` check before calling this command. This is odd, but I can't find a
    better way to go about this check.

[33mcommit 1d2f106f0966f74da37ad91341b9f3553dc7ab7d[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Thu Mar 2 21:39:29 2023

    Integrate LLVM at llvm/llvm-project@588da01621a1f8 (#1261)

[33mcommit c1a47c7f4fd4f07502ed9ac84886084960c7837b[m[33m ([m[1;33mtag: [m[1;33mv0.9.0[m[33m)[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Mar 2 20:25:33 2023

    Update compatibility.md with a link to the VHLO cookbook (#1263)
    
    Making final adjustments to the docs to make sure that they clearly
    state how to obtain portable artifacts.

[33mcommit ae32941bedb65b581e8e3836ad32f87a1a3720f5[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 19:12:32 2023

    Version and bytecode stablehlo_legalize_to_vhlo.mlir for 0.9.0 (#1260)
    
    This adds a FileCheck based test for VHLO compatibility. This fils
    should be versioned much like LLVM's
    [`compatibility.ll`](https://github.com/llvm/llvm-project/tree/main/llvm/test/Bitcode)
    versioning.
    
    This tests three things:
    
    1. A direct deserialization and FileCheck produces the proper VHLO
    2. A round trip to StableHLO and back to version 0.9.0 is valid
    3. Conversion to StableHLO produces the same StableHLO as passing the
    `.mlir` file through `stablehlo-opt`.

[33mcommit 273c255d5acba98b11394cb7922c06dba230c67e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Mar 2 16:30:43 2023

    Treat VHLO as an implementation detail in compatibility.md (#1259)
    
    This PR reformulates compatibility.md solely in terms of StableHLO to
    avoid exposing users to the implementation details of the serialization
    format.
    
    There are dedicated document about those details - bytecode.md and
    vhlo.md - but the entire target audience of compatibility.md doesn't
    have to know about them.

[33mcommit 672a715641311950d40898c9f02a78709214b74c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 06:28:27 2023

    Add VHLO documentation and usage instructions (#1250)
    
    Use this
    [vhlo.md](https://github.com/openxla/stablehlo/blob/4288e9820e3c2426df33461c97b860da1818a379/docs/vhlo.md)
    link to view with images.
    
    Closes #1248

[33mcommit 53b2fc4c9a4db82d2a71b291519a862b5c2f59dc[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 06:26:12 2023

    Add VHLO bytecode tests for v0.9.0 (#1257)
    
    Closes #1234
    
    Files created using the following commands:
    
    ```bash
    for f in *.mlir; do stablehlo-opt $f --stablehlo-legalize-to-vhlo --emit-bytecode > $f.0_9_0.bc; done;
    sed '1s/^/\/\/ RUN: diff <(stablehlo-opt %s.0_9_0.bc --vhlo-to-version=target=current --vhlo-legalize-to-stablehlo) <(stablehlo-opt %s)\n/' *.mlir -i
    ```

[33mcommit f51d31943740ae6a76ed15c9f2e7990ca74f02a4[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 06:14:25 2023

    Decompose CHLO ops when possible, update filter message when not. (#1256)
    
    The following files cannot be converted and serialized by StableHLO:
    
    ```
    Use TanOp (#954):
      tan_shape_bfloat16_20_20.mlir
      tan_shape_complex64_20_20.mlir
      tan_shape_float16_20_20.mlir
      tan_shape_float32_20_20.mlir
    
    TopK doesn't support dynamism (#1255):
      vmap_top_k_dtypes_inshape_bool_5_3__k_2_dynamic.mlir
    ```
    
    Edit: Use `shape.shape_of` was resolved using `--shape-legalize-to-hlo
    --canonicalize`.
    
    Full conversion command:
    
    ```
    mlir-hlo-opt --chlo-legalize-to-hlo --shape-legalize-to-hlo --canonicalize -hlo-legalize-to-stablehlo file_with_chlo.mlir
    ```

[33mcommit 64b9c008add95b3c9afb6db8266dcc2f5b0766d1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 04:50:45 2023

    Remove trailing underscores from test file names (#1254)

[33mcommit ab095d19d294a02c22336b1b56076ef6f6c129a5[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Thu Mar 2 01:06:47 2023

    [CHLO] Add erf_inv (#1253)

[33mcommit 38aae81bebd68566dfd2e2fe8ed22b3114acaefd[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Mar 2 00:50:03 2023

    Only lint whitespace for changed files (#1252)
    
    Now that we have a bunch of test files, it saves a lot of time to only
    lint changed files.
    
    Granted we will likely be shedding most of these test files soon
    (reorganizing, trimming, changing where they are stored, or something
    else), this is still a worthwhile change to make.
    
    Also updated the fix link so that it is copy-pastable. This tool is only
    runnable from the root directory as it is, so including full relative
    path is probably best.

[33mcommit 9e04a8cae6418ed867903a236d2e4fdc1182f994[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Wed Mar 1 23:36:13 2023

    Integrate LLVM at llvm/llvm-project@a28b252d852c31 (#1251)

[33mcommit 2d53c48d95bf2e74112f80a0759aec2ded9e994d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 1 20:05:10 2023

    Remove default value for `known_*_dimension`, fix legalize to VHLO test (#1246)

[33mcommit cbfea8e2f1ec5c7b39d9653e9c2211ef458d6ce1[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Mar 1 20:03:09 2023

    Add compatibility.md (#1244)
    
    In preparation for the imminent v0.9 release, we put together a document
    which describes the exact extent of compatibility guarantees at this
    moment, including testing strategy and future work.
    
    Closes #679.

[33mcommit 743e1b28c34fa26dcd5d2693267610c20dbba4e2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 1 19:30:42 2023

    Fix prettyprint bug with default optional attr in CholeskyOp (#1243)
    
    Currently printing CholeskyOp with default value will cause a crash
    since the attribute is not assigned.
    
    This disables the diff test in `legalize` since there are a few diffs
    that were hidden due to this crashing behavior. These will be addressed
    in a followup.

[33mcommit 8e5d5cfc00c4e9f88d91a602ec61a545a1bf6b92[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Mar 1 19:20:19 2023

    Update build to remove traces of conformance (#1242)
    
    PR #1221 renamed stablehlo/conformance to stablehlo/testdata because
    we're not ready to call it a conformance suite, and this PR cleans up
    the remaining traces of the original name in the build file.

[33mcommit 091b737c85c66fd772fea262762eaf219d544b4c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Mar 1 17:52:33 2023

    Add serialization tests for dynamic test cases (#1241)
    
    These tests do not have expected results or input arguments. For now,
    they are only intended to be used as serialization tests for programs
    with dynamism.
    
    These tests were generated from
    [jax2tf/tests/shape_poly_test.py](https://github.com/google/jax/tree/main/jax/experimental/jax2tf/tests/shape_poly_test.py).

[33mcommit 8496d542f13b2f81ef6648ba1ac414d0f9343ade[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 28 21:19:08 2023

    Compatibility testing initial setup (#1231)
    
    Adds the following round trip to VHLO test to all test points in
    `testdata`. Tests with CHLO ops are filtered. Also updates tests that
    use default values to not have default's specified. This permits us to
    do round trip filecheck testing.
    
    ```
    // RUN: diff <(stablehlo-opt %s --stablehlo-legalize-to-vhlo --vhlo-to-version=target=current -emit-bytecode | stablehlo-opt --vhlo-legalize-to-stablehlo) <(stablehlo-opt %s)
    ```
    
    Closes #1217.
    These files will be bytecoded for #1234.

[33mcommit 2d8b1079873341ee2ba9d01e4895803096d90b00[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 20:49:14 2023

    Change sym_visibility default from "public" to "" (#1237)
    
    When removing default values from VHLO, I used "public" as the default
    value for vhlo::FuncOp's sym_visibility argument. That was a mistake
    because func::FuncOp doesn't say anything about "public" being the
    default.

[33mcommit a9d251d34e78120a8b0efff65c2a2ac7a045ace6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 20:19:46 2023

    Use _v1 in VHLO mnemonics (#1235)
    
    We've been oscillating on vhlo.foo vs vhlo.foo_v1 in the last few
    months, but vhlo.foo_v1 has won because of its explicitness - we have
    encountered quite a few questions along the lines of "if I see vhlo.foo,
    what version does it have?".
    
    In the current design, VHLO isn't supposed to be manipulated by users
    directly, so we figured that the downside of increased verbosity is
    going to be relatively minor in practice. If this ends up not being the
    case, then this decision isn't going to be set in stone - we will always
    be able to drop _v1 before the 1.0 release.

[33mcommit 90a077d5aa9b0aad8441e11858a26d50cc0a97a1[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 17:41:55 2023

    Remove OptionalAttr from VHLO (#1214)
    
    Continuing to explore the design principle of aligning VHLO with the
    spec, I realized that having optional attributes goes against that.
    
    In the spec, we deliberately skipped the notion of optional attributes
    and default values, treating all attributes as mandatory. Given that,
    I think it's worth considering doing the same in VHLO.
    
    More concretely, for all OptionalAttr and Default...Attr in StableHLO,
    this PR changes the corresponding VHLO attributes from
    OptionalAttr<VHLO_AnyAttr> to VHLO_AnyAttr. (FWIW, the three attributes
    modeled via DefaultValuedStrAttr in StableHLO have already been modeled
    via VHLO_AnyAttr in VHLO).
    
    Furthermore, the StableHLO => VHLO conversion is changed to add these
    attributes to VHLO operations in case they are missing from StableHLO
    operations. And vice versa - the VHLO => StableHLO conversion is changed
    to remove these attributes from VHLO operations if they have default
    values.
    
    This means that the StableHLO => VHLO => StableHLO roundtrip is no
    longer an identity function as far as syntax is concerned (it can remove
    explicitly provided StableHLO attributes which have default values).
    Semantically, the roundtrip remains an identity function.
    
    Closes #1056.

[33mcommit 4ed62e84e68192d10272e61766a9726c9e261c92[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 05:50:05 2023

    Refactor StableHLO <=> VHLO (#1213)
    
    In the last few pull requests, I crammed a lot of adhoc logic into
    the corresponding conversion patterns.
    
    When working on removing optional attrs from VHLO, I needed to further
    update the conversion patterns, and I found it to be unnecessarily
    complicated with all the stuff in there.
    
    This PR refactors the conversion patterns to encapsulate the logic
    that bridges the gaps between StableHLO and VHLO. I think that modifying
    the conversion patterns will become considerably easier now.

[33mcommit 90f4491187043239aab41d1d1053364e26011f7d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 05:35:54 2023

    Mark ReturnOp as having an interpreter (#1224)
    
    We don't have a ticket for writing an interpreter for this op, and it's
    not something that we can test in isolation, but it is supported, and
    this PR reflects that.

[33mcommit 096a456c201e178c2f73d4547302645a5e22d5aa[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 28 01:27:58 2023

    Remove all references to 0.3.0 and 0.4.0 (#1211)
    
    Having two distinct unofficial versions of StableHLO was useful when
    developing the versioning machinery, but once we release 0.9.0, this
    will likely be confusing because we never announced any other versions
    (and rightfully so - they were not supposed to be public).
    
    In preparation for v0.9.0, let's remove all references to 0.3.0 and
    0.4.0, squash all versions of VHLO ops and flush the VHLO versioning
    patterns.
    
    Closes #1203.

[33mcommit 526d1a7065016584febd54c003d52effa92c37dd[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Tue Feb 28 00:06:08 2023

     Integrate LLVM at llvm/llvm-project@bc8de519ea6d4 (#1223)

[33mcommit ae59cd139c0ecfd7f3fe6562e72535be8f293e11[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Feb 27 22:49:54 2023

    Create Element.h constructors to get Element with primitive value (#1154)
    
    Currently, there is not an easy way to initialize Element according to
    the element types and each op needs to implement their own logic to
    initialize them. The proposed API allows easily creating Element object
    by specifying the type and its primitive value.

[33mcommit 847806ecd63806d7c153d0dbacbea752397c5b57[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 27 21:50:59 2023

    Move generated tests to testdata until conformance requirements are fully spec'ed (#1221)
    
    A formal spec will be needed for conformance suite, covering topics
    like:
    - Test structure, ensure that all clients are able to consume and use
    the test data
    - Test content, criteria for a test to be included in the conformance
    suite
    - Test cleanup, remove redundancy in the test files
    - Op spec, only fully spec'ed ops should be covered in a conformance
    suite
    - Floating point imprecision, what to do for these ops, and what
    threshold to use?
    
    In the meanwhile we'll move `conformance/` to `testdata/` to avoid
    giving an impression that the tests in there can already be used as a
    conformance suite. These tests are still valuable and can be used for
    validating a lot of aspects of an implementation, but they are not yet a
    full conformance suite.
    
    Once the requirements are ready and we have a roadmap for implementing
    them, one of the items on the roadmap will be moving the tests back to
    `conformance/`.
    
    Related issues: #1220, #1219, #1218, #1217, #1216

[33mcommit 2eb59dd48f9b43b6816b9f4a581929bd447aefc0[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 27 18:42:37 2023

    Bootstrap conformance tests from JAX primitive test generation (#1215)
    
    This set of tests was generated by dumping
    [jax2tf/tests/primitive_harness.py](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/tests/primitive_harness.py)
    primitive tests to StableHLO programs with inputs and expected results.
    
    Missing from this test suite still is coverage for for distribution ops
    and dynamism ops (for example: all_to_all, all_gather, all_reduce,
    collective_permute and reduce_scatter, + some others like replica_id).
    
    Tests are written as follows:
    
    ```
    module @jit_testcase {
      func.func public @main() -> tensor<i1> {
        %0 = call @inputs() : () -> tensor<20x20xbf16>
        %1 = call @expected() : () -> tensor<20x20xbf16>
        // ... Body of test case
        %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<20x20xbf16>, tensor<20x20xbf16>) -> tensor<i1>
        return %3 : tensor<i1>
      }
      func.func private @inputs() -> tensor<20x20xbf16> {
        %0 = stablehlo.constant dense<"..."> : tensor<20x20xbf16>
        return %0 : tensor<20x20xbf16>
      }
      func.func private @expected() -> tensor<20x20xbf16> {
        %0 = stablehlo.constant dense<"..."> : tensor<20x20xbf16>
        return %0 : tensor<20x20xbf16>
      }
    }
    ```
    
    Two possible ways to run these tests (to-do in future PR, this is not a
    final design):
    
    ```
    # Dynamic test case using reference interpreter
    stablehlo-opt --inline %s | stablehlo-interpreter --interpret
    
    # Static test case roundtrip through VHLO bytecode serialization
    diff <(stablehlo-opt --stablehlo-legalize-to-vhlo %s --emit-bytecode | stablehlo-opt --vhlo-legalize-to-stablehlo) <(stablehlo-opt %s)
    ```
    
    This custom call to check equality can be implemented by the reference
    interpreter or any backend.
    
    Closes #1204

[33mcommit 8e6b361de2ac9d9ba49253a1108904aa95fb34c1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Feb 26 17:38:38 2023

    Add index info in interpreter-mismatch error message (#1210)
    
    Even though we have a dedicated issue
    https://github.com/openxla/stablehlo/issues/1196 to help improving such
    cases, this change will have some immediate benefits during debugging.

[33mcommit 6886b59f6cd4369674e7e3beff61301c145176e2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Feb 25 02:59:41 2023

    Add interpreter for CaseOp (#1171)
    
    We have the following constraints in the spec:
    
    ```
    (I1) index 0-dimensional tensor constant of type `si32`.
    (I2) branches variadic number of functions.
    (C1) `branches` have at least one function.
    (C2) All functions in `branches` have 0 inputs.
    (C3) All functions in `branches` have the same output types.
    (C4) For all `i`, `type(results[i]) = type(branches[0].outputs[i])`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) index is not 0-dimensional tensor.
        b) element_type(index) != `si32`. (Covered by ODS).
    I2: a) branches is not variadic functions. (Covered by ODS).
    C1: a) size(branches) < 1.
    C2: a) not all functions in `branches` have 0 inputs.
    C3: a) not all functions in `branches` have the same output types.
    C4: a) type(results[i]) != type(branches[0].outputs[i]) for any i.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I1a: index is not 0-dimensional tensor.
    C1a: size(branches) < 1.
    C2a: not all functions in `branches` have 0 inputs.
    C3a: not all functions in `branches` have the same output types.
    C4a: type(results[i]) != type(branches[0].outputs[i]) for any i.
    ```
    
    * The status for verification remains `revisit` due to #365 and #581.
    
    closes #965

[33mcommit 6e4139b1f039f007015ddceec1b660455b0e1c14[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Fri Feb 24 21:59:42 2023

    Add interpreter for RemOp (#1092)
    
    (I1) lhs tensor of integer, floating-point or complex type (covered by
    ODS)
    (I2) rhs tensor of integer, floating-point or complex type (covered by
    ODS)
    (C1) lhs, rhs and result have the same type. (covered by ODS)
    
    Note that all constraints are covered by ODS, so no tests in
    ops_stablehlo.mlir.
    
    It is expected that `hlo_evaluator` and `mlir_interpreter` handle corner
    case differently
    ([hlo_evaluator](https://github.com/tensorflow/tensorflow/blob/c4c3c066d48983f57afd9e8fbe3734b03ee98018/tensorflow/compiler/xla/hlo/evaluator/hlo_evaluator_typed_visitor.h#L686)
    vs
    [mlir_interpreter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/mlir_hlo/tools/mlir_interpreter/dialects/cwise_math.h#L83)).
    Here is our way (same as comment in code):
    
    // These cases are implementation-defined, and the behavior of this
    interpreter:
    // For integers: if rhs is 0, the result is the same as `lhs`.
    // For signed integers, if lhs is INT_MAX and rhs is -1, the result is
    0.
    // For floats: use APFloat.mod(). If rhs is 0, the result is
    0x7FF8000000000000
    
    There are tests for corner cases and NOT included in test file (on
    purpose).
    ```
    // Results from hlo interpreter
    
    // CHECK-LABEL: Evaluated results of function: remainder_op_test_si64
    func.func @remainder_op_test_si64() -> tensor<2xi64> {
      %lhs = stablehlo.constant dense<[17, 0x7FFFFFFFFFFFFFFF]> : tensor<2xi64>
      %rhs = stablehlo.constant dense<[0, -1]> : tensor<2xi64>
      %result = stablehlo.remainder %lhs, %rhs : tensor<2xi64>
      func.return %result : tensor<2xi64>
    }
    
      17 : i64
      0 : i64
    
    // Results from mlir interpreter
    
    // CHECK-LABEL: Evaluated results of function: remainder_op_test_f64
    func.func @remainder_op_test_f64() -> tensor<1xf64> {
      %lhs = stablehlo.constant dense<[1.0]> : tensor<1xf64>
      %rhs = stablehlo.constant dense<[0.0]> : tensor<1xf64>
      %result = stablehlo.remainder %lhs, %rhs : tensor<1xf64>
      func.return %result : tensor<1xf64>
    }
    
    0x7FF8000000000000 : f64
    ```

[33mcommit 5c81e5c81b51f0c6b678b011f771eac5d06c3762[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Feb 24 19:21:00 2023

    Fix a typo in the spec of IfOp (#1208)
    
    PR #1164 updated the spec of IfOp to say that pred is a 0-dimensional
    tensor, fixing an oversight from the PR that introduced this spec.
    
    However, this PR also said that pred is a constant, which it is not. I
    missed that during review, and this PR fixes this.

[33mcommit bc8ad37f1c66f69cd3e8c68a634246bb76a99010[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Feb 24 18:48:28 2023

    Simplify evalConcatenateOp (#1209)
    
    Using a range-based for loop makes things easier to read.

[33mcommit 03addd938f56feca664d28bb2094b9966ad7808a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Feb 24 08:30:52 2023

    Add interpreter for ConcatenateOp (#1027)
    
    We have the following constraints in the spec:
    
    ```
    (I1) inputs variadic number of tensors.
    (I2) dimension constant of type `si64`.
    (C1) All tensors in `inputs` have the same element type.
    (C2) All tensors in `inputs` have the same shape except for the size of the
         `dimension`th dimension.
    (C3) `inputs` have N tensors where N >= 1.
    (C4) 0 $\le$ `dimension` $\lt$ `rank(inputs[0])`.
    (C5) `result` has the same element type as the tensors in `inputs`.
    (C6) `result` has the same shape as the tensors in `inputs` except for the
         size of the `dimension`th dimension, which is calculated as a sum of the size
         of `inputs[k][dimension]` for all `k` in `inputs`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) `inputs` is not a variadic tensor. (Covered by ODS).
    I2: a) dimension is not constant  of type `si64`. (Covered by ODS).
    C1: a) all `inputs` does not have the same element type. (Covered by ODS).
    C2: a) Not all tensors in `inputs` have the same shape except for the size of the
         `dimension`th dimension.
    C3: a) `inputs` have 0 tensors. (Covered by ODS).
    C4: a) dimension < 0.
        b) dimension >= rank(inputs[0]).
        c) rank(inputs[i]) > 0 for all i.
    C5: a) element_type(result) != element_type(inputs[0]). (Covered by ODS).
    C6: a) shape(inputs[i]) for all i does not have same shape except non-concat dimension.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C2a: Not all tensors in `inputs` have the same shape except for the size of the
         `dimension`th dimension.
    C4a: dimension < 0.
    C4b: dimension >= rank(inputs[0]).
    C4c: rank(inputs[i]) > 0 for all i.
    C6a: shape(inputs[i]) for all i does not have same shape except non-concat dimension.
    ```
    
    closes #968

[33mcommit 0afd3a112a4c656144e0da5e811697432d6cc7a9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Feb 24 02:32:45 2023

    Add interpreter for ImagOp (#1145)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) shape(`result`) = shape(`operand`).
    (C2) element_type(`result`) $=$
         * element_type(`operand`) if it's a floating-point type.
         * real_type(element_type(`operand`)) otherwise.
    ```
    I1 and C1 is covered by the ODS and C2 is covered in inferImagOp() as
    part of the type inference. Two redundant ODS tests are removed.
    
    closes #1103

[33mcommit a42ca1da413b320dc0e571f1959badb9ebcc4201[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Feb 24 02:31:49 2023

    Add interpreter for RealOp (#1144)
    
    Here are the following constraints:
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) shape(`result`) = shape(`operand`).
    (C2) element_type(`result`) $=$
         * element_type(`operand`) if it's a floating-point type.
         * real_type(element_type(`operand`)) otherwise.
    ```
    I1 and C1 is covered by the ODS and C2 is covered in inferRealOp() as
    part of the type inference. Two redundant ODS tests are removed.
    
    closes #1108

[33mcommit 45744d97ef9034e1e20ed46b6279072d133b9a90[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 23 22:46:36 2023

    Update checklist to review shape inference (#1201)
    
    * The checklist now covers tests in `infer_stablehlo.mlir` and moves
    related test from `ops_stablehlo.mlir` to this file.
    * Clarification made on the example to remove ambiguity in certain
    cases.

[33mcommit 7372ed7ebaca0a521a682599e58fac125834ea76[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Feb 23 22:04:32 2023

    Replace `FileCheck` based testing with Check dialect (#1026)
    
    Fixes #268
    
    The PR does the following:
    1. Introduces a dialect `CHECK` with opset (e.g.
    `stablehlo::check::expect_eq`, `stablehlo::check::expect_eq_const`,
    `stablehlo::check::expect_almost_eq`, and
    `stablehlo::check::expect_almost_eq_const`) to be used for implementing
    test harness for StableHLO interpreter. The opset is inspired from
    [iree::CheckOps.td](https://github.com/iree-org/iree/blob/main/compiler/src/iree/compiler/Modules/Check/IR/CheckOps.td).
    2. Implement the operations of the dialect. This will enable setting
    test assertions on the results of the interpreter.
    3. Modify all the `interpret_*.mlir` files to accommodate assertion
    based testing. Also, removes the dependency on
    [FileCheck](https://llvm.org/docs/CommandGuide/FileCheck.html).
    
    Note:
    1. The dialect has a dependency on
    [StablehloBase](https://github.com/openxla/stablehlo/blob/7be911a1a8468248efdc16e2a51ed5ff30c1cfbb/stablehlo/dialect/CMakeLists.txt#L22)
    because it is sharing the types like `HLO_Tensor`. Even though it is
    possible to avoid any dependency (by using generic types like
    `AnyTensor`), but the decision is deliberate in order to brand dialect
    as a test facilitator for stableHLO only.
    
    2. The core check for checking two floating points are equal modulo some
    tolerance is inspired from
    https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/
    and https://en.cppreference.com/w/cpp/types/numeric_limits/epsilon
    
    3. The dialect get registered only when running the interpreter: not
    registered to be used with `stablehlo-opt` tool.

[33mcommit c3e1cc0497e288e3362b61ee13e879803d7d717c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Feb 23 20:40:51 2023

    Reformat BUILD.bazel (#1205)
    
    A downstream linter noticed a change in BUILD.bazel and decide to opine
    on overall formatting in the file, even beyond the changed lines.
    
    I think it's a reasonable idea to humor it and apply the formatting
    changes here as well.

[33mcommit a415d6828171d92f133330aa9a2699c6dfe66354[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Feb 22 21:46:26 2023

    Add interpreter for DivideOp (#1008)
    
    closes #971
    
    We have the following constraints in the spec:
    
    ```
    (I1) lhs is a tensor of integer, floating-point, or complex type.
    (I2) rhs is a tensor of integer, floating-point, or complex type.
    (C1) lhs, rhs, and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) lhs is not a tensor of integer, floating-point, or complex type. (covered by ODS)
    (I2) rhs is not a tensor of integer, floating-point, or complex type. (covered by ODS)
    (C1) lhs, rhs, and result do not have the same type. (covered by ODS)
    ```

[33mcommit f5e19a58f2cbdc8bd732a7e9d76ab361e1d3318c[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Feb 22 21:01:28 2023

    Add interpreter for RsqrtOp. (#1085)
    
    closes #986
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) operand and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) operand is not a tensor of floating-point or complex type. (covered by ODS)
    (C1) operand and result do not have the same type. (covered by ODS)
    ```

[33mcommit 90daee03de92a1d47439bafde8ae4d8750ae6781[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Feb 22 16:51:15 2023

    Integrate LLVM at llvm/llvm-project@88bd2601c013 (#1200)

[33mcommit b29c1fa537a73fc63911070abe152830bbaa07a5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Feb 22 01:01:59 2023

    Stop using BooleanV1Type in IntegerV1Attr (#1198)
    
    This change was depending on two independent PR chains landing, so it
    couldn't be done in either of these chains.
    
    Now that both PR chains have landed, this has become a very
    straightforward change.

[33mcommit 6dcb83095f714892b9465d5ba7a133ee2a149c27[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 23:02:30 2023

    Add support for collective ops in --stablehlo-refine-shapes (#1180)
    
    At the moment, AllGatherOp and ReduceScatterOp don't have shape
    functions because they rely on `dim(process_groups, 1)` which in its
    full generality requires knowledge of `num_partitions`.
    
    (Per https://github.com/openxla/stablehlo/blob/main/docs/spec.md: For
    cross_replica and flattened_ids process grouping strategies,
    `dim(process_groups, 1)` is equal to `dim(replica_groups, 1)`. However,
    for cross_replica_and_partition `dim(process_groups, 1)` is equal to
    `dim(replica_groups, 1) * num_partitions`).
    
    Even though we cannot have shape functions for these two ops without
    changing the StableHLO spec, we can still make best effort at refining
    their shapes for cross_replica and flattened_ids cases. This is what
    this PR implements.

[33mcommit b4be1838cf83db0d5c49f493136cf03f75751cc8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 22:58:50 2023

    Add support for folding SignOp in --stablehlo-refine-shapes (#1179)
    
    This follows up on the recent DynamicConvOp pull request and introduces
    additional functionality to handle accompanying shape computations.

[33mcommit c3c588f01f28020da9ee6acbb9b3580f37c62ab6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 21:58:32 2023

    Audit VhloAttrs.td (#1195)
    
    This pull request completes the audit of VhloAttrs.td, following a
    significant refactoring aimed to align the file with the spec.
      1) Order alphabetically, merging native and forked attributes.
         (Within the vision that attributes must correspond 1:1 to the
         specification, there's no difference between where these attributes
         are originally coming from, so there's no need to separate the
         two categories).
      2) Add comments to each attribute, either identifying the
         corresponding program element from the spec or linking a ticket
         that tracks the alignment effort.
      3) Rename kFooAttr to kFooV1Attr to be consistent with the TableGen
         file that defines the attributes.
      4) Renumber kFooAttr in increasing order to match the alphabetical
         order of the names. (This is a breaking change for VHLO, but I
         think it's fine because we haven't officially released it yet).
      5) A few more minor renamings and cleanups.
    
    Fixes #1047.

[33mcommit dad6fa1a02295f5aba18726ecb9bec2c87d9df73[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 21:42:43 2023

    Inline VHLO_ConvDimensionNumbersAttr and friends (#1194)
    
    I think that grouping of the elementary attrs into bigger attrs is
    pretty incidental as far as serialization format goes, so I would
    propose to break all of them down, just like the spec does.
    
    This is a breaking change for VHLO, and I was hesitating whether to
    provide proper versioning for it or not. Ultimately, I decided that
    I won't be doing that, similarly to how the work on forking attributes
    and types (also a breaking change) didn't do that. We haven't yet
    officially released VHLO, so I think that it's fine. See #1047.

[33mcommit 8b471e7eb4f801072a698b76fdcbfca8fada5fc2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 21 21:04:36 2023

    Introduce Index class (#1173)
    
    closes https://github.com/openxla/stablehlo/issues/1163
    
    The PR:
    1. Introduces a class `Index` to represent the index vector of a
    `Tensor`. That is to replace the explicit use of `ArrayRef<int64>` for
    the purpose.
    2. `Tensor::begin_index` and `Tensor::end_index` to return `Index`
    objects instead.
    3. Promote the use of `Index` in various `evalFooOp`. The use of
    overload `Index::operator+` is handy for `DynamicSlice` and
    `DynamicUpdateSlice` op.
    
    ---------
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit aac93e3d7e98de2312d2985c3aead1d2d0ae76ad[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 20:51:33 2023

    Replace VHLO_UnitAttr with VHLO_BooleanAttr (#1193)
    
    This is one of the several steps towards aligning VhloAttrs.td with
    the StableHLO spec.
    
    As far as representational power goes, UnitAttr is equivalent
    to BooleanAttr, so in the spirit of VHLO being a shallow dialect, I
    think that we should remove the former and use the latter instead.
    
    When making this change, I noticed that BooleanAttr attributes from
    StableHLO get translated to IntegerAttr attributes from VHLO. This has
    been changed to use the newly introduced BooleanAttr from VHLO.
    
    This is a breaking change for VHLO, and I was hesitating whether to
    provide proper versioning for it or not. Ultimately, I decided that
    I won't be doing that, similarly to how the work on forking attributes
    and types (also a breaking change) didn't do that. We haven't yet
    officially released VHLO, so I think that it's fine. See #1047.

[33mcommit 8037d2bedfa192095901356e60ad8cc182481a22[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 20:36:09 2023

    Remove VHLO_FlatSymbolRefAttr (#1192)
    
    This is one of the several steps towards aligning VhloAttrs.td with
    the StableHLO spec.
    
    As far as representational power goes, FlatSymbolRefAttr is equivalent
    to SymbolAttr, so in the spirit of VHLO being a shallow dialect, I think
    that we should remove the former and use the latter instead.
    
    This is a breaking change for VHLO, and I was hesitating whether to
    provide proper versioning for it or not. Ultimately, I decided that
    I won't be doing that, similarly to how the work on forking attributes
    and types (also a breaking change) didn't do that. We haven't yet
    officially released VHLO, so I think that it's fine. See #1047.

[33mcommit 946acc6cdff9a482b2e57054e158d54bd0e39497[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 20:02:42 2023

    Minor refactoring for FuncOp-related attributes (#1191)
    
    Minor refactoring for FuncOp-related attributes
    
    VHLO_DictionaryAttrV1 didn't have "V1" appended to its name, unlike
    other nearby attributes.
    
    Also, since neither VHLO_DictionaryAttr nor VHLO_TypeAttr are called
    out in the spec yet, I've added corresponding todos that link to
    the appropriate ticket. See #1047.

[33mcommit 4283ec071baf74343adac726815c539c2cfc0e46[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 19:26:05 2023

    Audit VhloTypes.td (#1189)
    
    This pull request completes the audit of VhloTypes.td, including
    refactorings aimed to align the file with the spec.
      1) Order alphabetically, merging native and forked types.
         (Within the vision that types must correspond 1:1 to the
         specification, there's no difference between where these types
         are originally coming from, so there's no need to separate the
         two categories).
      2) Introduce BooleanType to match the spec.
      3) Rename FunctionType::results to FunctionType::outputs to match
         the spec.
      4) Rename IntegerI...Type to IntegerSI...Type to match the spec.
      5) Rename floating-point types to FloatBF...Type or FloatF...Type
         to match the naming style used for integer types.
      6) Add comments to each attribute, either identifying the
         corresponding program element from the spec or linking a ticket
         that tracks the alignment effort.
      7) Rename kFooType to kFooV1Type to be consistent with the TableGen
         file that defines the attributes.
      8) Renumber kFooAttr in increasing order to match the alphabetical
         order of the names. (This is a breaking change for VHLO, but I
         think it's fine because we haven't officially released it yet).
      9) A few more minor renamings and cleanups.

[33mcommit 7c49626419bcf26e052a9bcc8c4412f5f21a39fc[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 17:00:06 2023

    Rename VHLO_DenseIntOrFPElementsAttr to VHLO_TensorAttr (#1190)
    
    This is one of the several steps towards aligning VhloAttrs.td with the
    StableHLO spec.
    
    Keeping the forked name is understandable, but I think that alignment
    with the spec is more important. See #1047.

[33mcommit 73a238852b82849e2527c4e95fe08f3ec44983e0[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 16:35:31 2023

    Audit VhloEnums.td (#1188)
    
    As I was auditing the VHLO dialect for the correspondence with the spec,
    I figured I'd streamline the VhloEnums.td as follows:
      1) Order alphabetically.
      2) Factor out I32EnumAttr boilerplate into VHLO_I32EnumAttr and
         reformat to compress the code a little bit.
      3) Drop summaries from I32EnumAttrs (this is aligned with the notion
         of VHLO being a shallow dialect, e.g. we don't have them for ops).
      4) Removed a few unused definitions.
    
    I've also noticed that getMinVersion/getMaxVersion implementations for
    VHLO_EnumAttr didn't have explicit error handling - probably this
    resulted from partially addressed feedback on one of the VHLO PRs?
    Anyway, I made these implementations consistent with similar VHLO code,
    using llvm::report_fatal_error instead of llvm_unreachable to forward
    port the corresponding PR in flight.

[33mcommit f9532544313c28dfa5277a0d66b9b6891f1cef81[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 16:35:08 2023

    Audit VhloOps.td (#1184)
    
    Our design principle for VHLO ops has been radical minimalism, and this
    PR further streamlines operation definitions:
      1) Sorts all ops alphabetically.
      2) Removes the stray assemblyFormat from ReturnOp.
      3) Spells out VHLO op design principles in a comment in VhloOps.td.
      4) Updates the description of VhloDialect.td to align with the
         design principles.
    
    Using the results of streamlining (the part where all ops got sorted
    alphabetically), I completed an audit of VhloOps.td, aligning it as
    much as possible with the StableHLO spec and leaving todos where that
    is not yet possible.
    
    During that audit, I discovered minor discrepancies between the spec
    and StableHLO/VHLO ODS, involving names and order of program elements,
    and fixed them as well.

[33mcommit 6ab2eb2393782300f25a2ffe61af04bc8d5ee3e4[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Feb 21 15:45:26 2023

    Add interpreter for LogOp (#1086)
    
    (I1) operand: tensor of floating-point or complex type (coved by ODS)
    (C1) operand and result have the same type. (coved by ODS)
    
    Note that there is only one constrain of this op and coved by ODS, so no
    tests in `ops_stablehlo.mlir`.

[33mcommit 59784920ccc59aefa1cbcf58a7e9122bf818ac5f[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Feb 21 11:38:44 2023

    Add interpreter for SqrtOp. (#1068)
    
    closes #1116
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of floating-point or complex type.
    (C1) operand and result have the same type.
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) operand is not a tensor of floating-point or complex type. (covered by ODS)
    (C1) operand and result do not have the same type. (covered by ODS)
    ```

[33mcommit 7e9ce9af667a65e2755c1fffec2ddd201f92d98d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 21 00:40:44 2023

    Integrate LLVM at llvm/llvm-project@8b3091b4b485 (#1197)

[33mcommit fe60daafb1cdbc09e9386fbfc64824ef5273ef38[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Feb 20 23:53:01 2023

    Add interpreter for ExpOp (#1079)
    
    (I1) operand: tensor of floating-point or complex type (covered by ODS)
    (C1) operand and result have the same type. (covered by ODS)
    
    Note that there is only one constrain of this op and covered by parent
    `StableHLO_UnaryElementwiseOp`, so no tests in `ops_stablehlo.mlir`.

[33mcommit 97839eed933837c2a6a8cbaab9e5418b29fd8b25[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun Feb 19 05:49:35 2023

    Replace all uses of llvm_unreachable with llvm::report_fatal_error (#1185)
    
    See
    https://discourse.llvm.org/t/llvm-unreachable-is-widely-misused/60587
    for motivation.

[33mcommit cbc144844d4086c8941c5073117ec135ba5970a5[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Fri Feb 17 01:04:06 2023

    Add interpreter for ClampOp (#999)
    
    ClampOp has Inputs and constrains:
    (I1) min is tensor. (By ODS)
    (I2) operand is tensor. (By ODS)
    (I3) max is tensor. (By ODS)
    
    (C1) Either rank(min)=0 or shape(min)=shape(operand).
    (C2) Either rank(max)=0 or shape(max)=shape(operand).
    (C3) min, operand, and max have the same element type. (By ODS)
    (C4) operand and result have the same type.
    
    I1, I2, I3, and C3 is done by ODS.
    C1, C2, C4 are verified by `inferClampOp()` in `TypeInference.cpp`

[33mcommit 5f04516a6a1c379aa9f8d31b2af3501357657db3[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Fri Feb 17 00:14:02 2023

    Add interpreter for SelectOp (#1087)
    
    The constrains in Spec has
    * (C1) Either `rank(pred)` $=$ `0` or `shape(pred)` $=$
    `shape(on_true)`.
    * (C2) `on_true`, `on_false` and `result` have same type.
    
    None is coved in ODS: all are coved in `InferSelectOp()` from
    `TypeInference.cpp`
    
    One reductant test is removed. others are renamed with C1~2.
    No change to `Element.h/cpp`, thus no need to add more test other than
    int64 tensor and scalar.

[33mcommit e0ac014e895450c06f2d85f39229f5dd9c4a77cf[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Feb 16 20:07:46 2023

    Add interpreter for AbsOp (#1001)
    
    closes #962
    closes #1083
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand is a tensor of integer, floating-point, or complex type.
    (C1) operand and result have the same shape.
    (C2) a: operand and result have the same element type when operand element type is integer or floating-point
          b: When element type of the operand is complex type, the element type of the result is the element
          type of the complex type (e.g. the element type of the `result` is `f64` for operand type `complex<f64>`).
    ```
    
    These constraints are covered by the following tests
    
    ```
    (I1) operand is not a tensor of integer, floating-point, or complex type. (covered by ODS)
    (C1) shape(operand) = shape(result). (covered by ODS)
    (C2) a: element_type(operand) = element_type(result) ; operand element type is integer or
         floating-point. (covered by type inference, negative test in ops_stablehlo.mlir)
         b: element type of the result is the element type of the complex type, when element type
         of the operand is complex type,  (e.g. the element type of the `result` is `f64`
         for operand type `complex<f64>`). (covered by type inference, negative test in ops_stablehlo.mlir)
    ```

[33mcommit fcd81f2be4c5ea247fdac706e40297f8c89685ee[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Feb 16 07:12:42 2023

    Move and merge verifier of Set/GetDimensionSizeOp (#1029)
    
    SetDimensionSizeOp is very special, compared to other ops when involving
    bounded dynamism: it is possible that the output have bounds while the
    inputs has no bounds at all. Thus we can't use this util function to
    generate the encoding at the end of shape function:
    ```
    Attribute boundsToEncoding(Attribute prototype, ArrayRef<int64_t> bounds)
    ```
    Because the `prototype`, which comes from `input.getEncoding()` maybe
    null. So in this PR it create bounds attr from dialect directly.
    
    Change `inferReturnTypes` to `inferReturnTypeComponents` and remove the
    TODO in comment as well.

[33mcommit eca286c4f89e820ffd2bdc36800ff670a549a8e0[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Feb 16 03:11:31 2023

    Minor typo fix (match parameter names in function signature and function definition) (#1177)

[33mcommit 5b6261b50171613b9063519d17d8fbf62e2b9064[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Feb 15 23:31:27 2023

    Integrate LLVM at llvm/llvm-project@4f15267d3dd7 (#1175)

[33mcommit a7280f966a005b5edf57b00815d3dd78e77db544[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 15 20:10:10 2023

    Add interpreter for IfOp (#1164)
    
    We have the following constraints in the spec:
    
    ```
    (I1) pred 0-dimensional tensor constant of type `i1`.
    (I2) true_branch function.
    (I3) false_branch function.
    (C1) `true_branch` and `false_branch` have 0 inputs.
    (C2) `true_branch` and `false_branch` have the same output types.
    (C3) For all `i`, `type(results[i]) = type(true_branch.outputs[i])`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) pred is not 0-dimensional.
        b) element_type(pred) != `i1`. (Covered by ODS).
    I2: a) true_branch is not a function. (Covered by ODS).
    I3: a) false_branch is not a function. (Covered by ODS).
    C1: a) `true_branch` does not have 0 inputs.
        b) `false_branch` does not have 0 inputs.
    C2: a) `true_branch` and `false_branch` does not have the same output types.
    C3: For all `i`, `type(results[i]) = type(true_branch.outputs[i])`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I1a: pred is not 0-dimensional.
    C1a: `true_branch` does not have 0 inputs.
    C1b: `false_branch` does not have 0 inputs.
    C2a: `true_branch` and `false_branch` does not have the same output types.
    ```
    
    Notes:
    * Fixed typo:
    ```
    `pred` | 1-dimensional tensor constant of type `i1`
    ```
    to
    ```
    `pred` | 0-dimensional tensor constant of type `i1`
    ```
    * The status for verification remains `revisit` due to #365 and #581.
    
    closes #977

[33mcommit 6c500e0974bf003630320d38cffd1ce197b80c48[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 15 02:17:15 2023

    Add interpreter for DynamicSliceOp (#1080)
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand tensor.
    (I2) start_indices variadic number of 0-dimensional tensors of integer type.
    (I3) slice_sizes 1-dimensional tensor constant of type `si64`.
    (C1) `operand` and `result` have the same element type.
    (C2) size(`start_indices`) $=$ size(`slice_sizes`) $=$ rank(`operand`).
    (C3) All `start_indices` have the same type.
    (C4) `slice_sizes[k]` $\in$ [0, dim(`operand`, `k`)] for all `k` $\in$ [0,
    rank(`operand`)).
    (C5) shape(`result`) $=$ `slice_sizes`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) start_indices is not a variadic 0-dimensional tensors. (Covered by ODS).
        b) start_indices does not have integer type. (Covered by ODS).
    I3: a) slice_sizes is not a 1-dimensional tensor.
        b) slice_sizes does not have type `si64`. (Covered by ODS).
    C1: a) element_type(operand) != element_type(result). (Covered by ODS).
    C2: a) size(start_indices) != size(slice_sizes).
        b) size(start_indices) != rank(operand).
    C3: a) all start_indices do not have the same type.
    C4: a) slice_sizes[i] < 0 for any i.
         b) slice_sizes[i] > dim(operand, i) for any i.
    C5: no negative test needed since it's just inferring the shape from slice_sizes.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I3a: slice_sizes is not a 1-dimensional tensor.
    C2a: size(start_indices) != size(slice_sizes).
    C2b: size(start_indices) != rank(operand).
    C3a: all start_indices do not have the same type.
    C4a: slice_sizes[i] < 0 for any i.
    C4b: slice_sizes[i] > dim(operand, i) for any i.
    ```
    
    Fixed typo:
    ```
    (C4) `slice_sizes[k]` $\in$ [0, dim(`operand`, `k`)) for all `k` $\in$ [0,
      rank(`operand`)).
    ```
    to
    ```
    (C4) `slice_sizes[k]` $\in$ [0, dim(`operand`, `k`)] for all `k` $\in$ [0,
      rank(`operand`)).
    ```
    
    closes #973

[33mcommit 8a02c309dc370f433bc7e7d1e7d5585e8ab88c5e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Feb 15 01:10:42 2023

    Print escaped string to preserve binary data (#1172)
    
    Need to escape string with unprintable characters because it can cause
    printer/parser issues.
    
    Namely, I recently was parsing a string where the binary data included
    the ascii for `"`, which caused the string to be malformed as
    `vhlo.string<"ab"cd">`

[33mcommit 66e871bc1aeb0c95bc64696ab3098127a31a7dfe[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 14 19:41:55 2023

    Prototype a mechanism to customize shape refinement for CustomCallOp (#1169)
    
    This is a prototype of the CustomCallOp shape refinement mechanism to
    enable specializing dynamically-shaped programs with custom calls to
    static shapes (#851).
    
    Based on the results of prototyping, I'm planning to propose an RFC to
    extend CustomCallOp, so that the logic can migrate from using an
    unregistered attribute to something more solid.

[33mcommit b0692e9c325a87527a737db685f330d6f806973e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 14 18:29:55 2023

    Organize ongoing work items in workstreams and milestones (#1168)
    
    As we've been exploring the StableHLO opset, we've been making notes
    about all kinds of todo items - starting from minor cleanups in the
    StableHLO dialect all the way to missing functionality in the opset.
    
    This is great because thanks to that we have documented at great length
    what it would take for StableHLO to reach its full potential - with
    perfect frontend contract, 100% specification compliance, etc etc.
    
    This level of detail (at the moment of writing, we have 275 tickets in
    our issue tracker) would benefit from some organization, and this is
    something that this PR proposes.
    
    Recently, we have categorized the tickets in the issue tracker into
    multiple workstreams and tied these workstreams to the milestones. The
    roadmap.md introduced in this PR brings this all together.

[33mcommit cb9275b21095f56521ea18c12a40ff5ad2b95258[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 14 18:29:01 2023

    Add support for DynamicConvOp to --stablehlo-refine-shapes (#1167)
    
    This PR adds a RefineConvolutionOpPattern pattern that reuses
    inferConvolutionOp to do shape refinement for DynamicConvOp.
    
    Additionally, it makes some changes to constant folding patterns to
    support related shape computations.

[33mcommit d26d7163ed4a5f184a98e7d2e8f1cb9e8dd49a43[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 14 18:03:17 2023

    Remove default values for VHLO versions in Enums / Attrs / Types / Ops (#1166)
    
    Closes #779

[33mcommit 1ac0e9e68ca0921baaee1744dc6a29dcf28e6be9[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Feb 14 04:33:43 2023

    Integrate LLVM at llvm/llvm-project@d3b0fba6084d (#1162)

[33mcommit 0753ff992da83dd891760dcd79167350f6ec45e7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 14 03:16:54 2023

    Add interpreter for DynamicUpdateSliceOp (#998)
    
    Unlike the spec, the interpreter uses its own implementation of clamp to
    calculate `adjustedStartIndices` since the start indices are given as
    variadic tensors, and formulating as one tensor to use the ClampOp is
    more loc.
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand tensor.
    (I2) update tensor.
    (I3) start_indices variadic number of 0-dimensional tensors of integer type.
    (C1) `operand` and `result` have the same type.
    (C2) element_type(`update`) $=$ element_type(`operand`).
    (C3) rank(`update`) $=$ rank(`operand`).
    (C4) size(`start_indices`) $=$ rank(`operand`).
    (C5) All `start_indices` have the same type.
    (C6) dim(`update`, `k`) $\in$ [0, dim(`operand`, `k`)] for all `k` $\in$ [0, rank(`operand`)).
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) update is not a tensor. (Covered by ODS).
    I3: a) start_indices does not have 0-dimensional tensors. (Covered by ODS).
        b) start_indices does not have integer type. (Covered by ODS).
    C1: a) element_type(operand) != element_type(result). (Covered by ODS).
        b) shape(operand) != shape(result).
    C2: a) element_type(update) != element_type(operand). (Covered by ODS).
    C3: a) rank(update) != rank(operand).
    C4: a) size(start_indices) != rank(operand).
    C5: a) all start_indices does not have the same type.
    C6: a) dim(update, k) < 0 for any k.
        b) dim(update, k) > dim(operand, k) for any k.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    C1b: no negative test needed since result shape is inferred from operand shape
    C3a: rank(update) != rank(operand).
    C4a: size(start_indices) != rank(operand).
    C5a: all start_indices does not have the same type.
    C6a: dim(update, k) < 0 for any k.
    C6b: dim(update, k) > dim(operand, k) for any k.
    ```
    
    closes #974

[33mcommit 8f69e0ba0c3a6041ac9ac7a76d1738ba81fc8ad9[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 14 02:42:57 2023

    Add interpreter for PadOp (#1005)
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand tensor
    (I2) padding_value 0-dimensional tensor
    (I3) edge_padding_low 1-dimensional tensor constant of type si64
    (I4) edge_padding_high 1-dimensional tensor constant of type si64
    (I5) interior_padding 1-dimensional tensor constant of type si64
    (C1) `operand`, `padding_value`, `result` have the same element type.
    (C2) `edge_padding_low`, `edge_padding_high`, `interior_padding` have the
    size equal to `operand`'s rank.
    (C3) 0 $\le$ `interior_padding[i]` for all `i` values in `interior_padding`.
    (C4) 0 $\le$ `dim(result, i)` for all `i`th dimension of `operand`, where
    `dim(result, i) = di + max(di - 1, 0) * interior_padding[i] + edge_padding_low[i] + edge_padding_high[i]`
    and `di = dim(operand, i)`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) padding_value is not a 0-dimensional tensor.
    I3: a) edge_padding_low is not a 1-dimensional tensor.
        b) element_type(edge_padding_low) != si64. (Covered by ODS).
    I4: a) edge_padding_high is not a 1-dimensional tensor.
        b) element_type(edge_padding_high) != si64. (Covered by ODS).
    I5: a) interior_padding is not a 1-dimensional tensor.
        b) element_type(interior_padding) != si64. (Covered by ODS).
    C1: a) element_type(operand) != element_type(padding_value) != element_type(result). (Covered by ODS).
    C2: a) size(edge_padding_low) != rank(operand).
        b) size(edge_padding_high) != rank(operand).
        c) size(interior_padding) != rank(operand).
    C3: a) interior_padding < 0.
    C4: no negative test added since it's just inferring the shape.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2a: padding_value is not a 0-dimensional tensor.
    I3a: edge_padding_low is not a 1-dimensional tensor.
    I4a: edge_padding_high is not a 1-dimensional tensor.
    I5a: interior_padding is not a 1-dimensional tensor.
    C2a: size(edge_padding_low) != rank(operand).
    C2b: size(edge_padding_high) != rank(operand).
    C2c: size(interior_padding) != rank(operand).
    C3a: interior_padding < 0.
    ```
    
    Note that for `I3a`, `I4a`, and `I5a`, they are partially covered by the
    `AllTypesMatch` trait, so only one test (`I2a`) is needed to test all
    three. Similarly with `C2a`, `C2b`, and `C2c` (`C2a` test is added to
    cover all three).
    
    closes #980

[33mcommit 2e04b75b844afe8590155c8cd9179c23d1bb14b3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 14 02:42:11 2023

    Add interpreter for BroadcastInDimOp (#1051)
    
    We have the following constraints in the spec:
    
    ```
    (I1) operand tensor.
    (I2) broadcast_dimensions 1-dimensional tensor constant of type `si64`.
    (C1) `operand` and `result` have the same element type.
    (C2) size(`broadcast_dimensions`) $=$ rank(`operand`).
    (C3) $0 \le$ `broadcast_dimensions[i]` $\lt$ rank(`result`) for all
         dimensions i in `operand`.
    (C4) All dimensions in `broadcast_dimensions` are unique.
    (C5) For all dimensions `j` in `operand`:
         * `dim(operand, j) = 1` or
         * `dim(operand, j) = dim(result, i)` where `i = broadcast_dimensions[j]`.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) broadcast_dimensions is not a 1-dimensional tensor.
        b) broadcast_dimensions does not have `si64` type. (Covered by ODS).
    C1: a) element_type(operand) != element_type(result). (Covered by ODS).
    C2: a) size(broadcast_dimensions) != rank(operand).
    C3: a) broadcast_dimensions[i] < 0 for any i.
        b) broadcast_dimensions[i] >= rank(result) for any i.
    C4: a) `broadcast_dimensions` values are not unique.
    C5: a) dim(operand, j) != 1 and dim(operand, j) != dim(result, i) where `i = broadcast_dimensions[j]` for all dimensions `j` in `operand`.
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2a: broadcast_dimensions is not a 1-dimensional tensor.
    C2a: size(broadcast_dimensions) != rank(operand).
    C3a: broadcast_dimensions[i] < 0 for any i.
    C3b: broadcast_dimensions[i] >= rank(result) for any i.
    C4a: `broadcast_dimensions` values are not unique.
    C5a: dim(operand, j) != 1 and dim(operand, j) != dim(result, i) where `i = broadcast_dimensions[j]` for all dimensions `j` in `operand`.
    ```
    
    closes #964

[33mcommit 914289270423b2b852ba78bd524676979b737a20[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Feb 13 20:21:45 2023

    Update ODS documentation for yet unspecced ops (#1159)
    
    For "Not in HLO" ops, the ODS now indicates that they are on their way
    out of StableHLO and informally explains their semantics.
    
    For "Dynamism" ops, the ODS now says that they are being worked on and
    also informally explains their semantics.
    
    Closes #739.

[33mcommit 6e31b5ff87c39e7f6710593138c0aa856cd91b8f[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Feb 13 20:02:32 2023

    Remove the undefined AllReduceOp build() (#1096)
    
    Migrate back from MHLO change:
    https://github.com/tensorflow/mlir-hlo/commit/69acab5abd3b6468703d31e83ff4f4871f887fa4

[33mcommit 52a9462f42c53795dbbf2c16c10e80db02221b94[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Feb 13 18:04:54 2023

    Add VHLO Python/C Bindings (#1142)
    
    These are pretty minimal VHLO bindings. We don't intend for Python/C
    users to directly interact with VHLO attributes / types, so we only need
    the ability for Python to register the dialect to parse and serialize.
    
    Currently this is based on CHLO bindings, except only Dialect-related
    bindings are provided currently, no attributes or types.

[33mcommit 7b4a76931aff91fd0b2f6950b489dcabc2987953[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Feb 13 17:45:25 2023

    Implement Region evaluation (#1093)
    
    Closes #995
    
    The implementation follows the discussion in #995 in the following ways:
    
    ## Class `InterpreterScope`
    
    To represent runtime stack corresponding to a region of a program under
    evaluation. The idea is to use a "parent-child" hierarchy of the class
    objects to implement the scoping rules.
    1. Name : `InterpreterScope`. Refrained fro using just `Scope` because
    that seems more like a static program concept.
     2. Members variables:
    - `stackFrame`: mapping from SSA values, defined in the current region,
    to their evaluated runtime `Tensor` values.
    - `parentScope`: a handle to `InterpreterScope` object corresponding to
    the (syntactically) enclosing region.
     3. Member functions:
    - `void add(Value ssaValue, Tensor runtimeValue)`: Add the mapping SSA
    value (`ssaValue`), defined in region, to its runtime value
    (`runtimeValue`) in the current scope.
    - `Tensor find(Value ssaValue) const;`: Find the runtime value mapped to
    SSA value `ssaValue`. The search starts
    with the current scope and then recursively continues over to the scope
    defined by `parentScope`.
    
    A few points to note:
    1. A `InterpreterScope` object is instantiated every time a region is
    evaluated. This is just a simplification over the following alternative
    - Store a map from a ` region` to corresponding `InterpreterScope`
    object.
    - Whenever a region is evaluated used the mapped `InterpreterScope`
    object for scope resolution.
    
    2. A `InterpreterScope` object treats the `parentScope` as immutable to
    align with the fact that a StableHLO program, in pure SSA form (without
    memory allocation/load/store ops), disallows mutating the `parentScope`
    object from within a region.
    
    3. Copy prohibited for `InterpreterScope` objects.
    
    ## Region evaluation
    
    The evaluation of a region is realized using
    
    ```
    /// Evaluates an mlir::Region `region` using the runtime values `runtimeArgs`
    /// corresponding to the arguments of the containing block, assuming that the
    /// region has only one block. Interprets the operations within the block and
    /// returns the runtime values for the terminator's arguments.
    llvm::Expected<llvm::SmallVector<Tensor>> eval(Region &region, llvm::ArrayRef<Tensor> runtimeArgs,
      const InterpreterScope *const parentScope);
    ```
    
    The above function is used to evaluated the region for any op including
    the `func::FuncOp`. Its definition provides a sole place when a
    `InterpreterScope` is stack-allocated and its `parentScope` is
    initialed.
    
    ### How the `eval(Region..` is invoked
    
    The evaluator of  `func::FuncOp` invokes the function as
    ```
    eval(func.getBody(), args, nullptr); // This is because IsIsolatedFromAbove trait for func::FucnOp is true
    ```
    
    The evaluator for other StableHLO ops, say `stableHLO::whileOp`, invokes
    the function as
    ```
    bool isIsolatedFromAbove = cond.getParentOp()->hasTrait<OpTrait::IsIsolatedFromAbove>();
    eval(cond, runtimeInputs, isIsolatedFromAbove ? nullptr : &scope);
    ```
    Note:
    - `scope` is the `InterpreterScope` object corresponding to the region
    containing the `whileOp`
    - As the evaluator for an op expected to know the semantics, one might
    get tempted to skip the check for
    [OpTrait::IsolatedFromAbove](https://mlir.llvm.org/docs/Traits/#isolatedfromabove)
    and directly pass the appropriate scope. Please note that these checks
    make the code adjust to any future changes in the behavior of the op
    w.r.t its visibility to outer scope.
    - The way the `parentScope` is initialized makes sure the following
    relationship to hold between a `InterpreterScope` object and its stored
    `parentScope`
    - `InterpreterScope Scope`: Defined whenever a `region` is evaluated
    - `Scope.parentScope`: `InterpreterScope` object corresponding to the
    syntactically enclosing region
    
    ## [Scoping rules](https://mlir.llvm.org/docs/LangRef/#value-scoping) to
    cover
    
    1. **A value defined by an argument to a region can always be used by
    any operation deeply contained in the region:** Covered by the design of
    `InterpreterScope`.
    2. **A value defined in a region can never be used outside of the
    region**: For a value defined in a region `RI` to be used outside of the
    region `RO`, then `RO`, during its evaluation, should receive the
    `parentScope` as the `InterpreterScope` object corresponding to `RI` ,
    which will never happen per how `parentScope` is always set to
    `InterpreterScope` object corresponding to the syntactically enclosing
    region. For example,
    
    For the evaluation of `R1` and `R2`, `parentScope` is set to
    `InterpreterScope` object corresponding to the `R`.
    ```
      { // enclosing Region, R
        { // region R1
          %value_defined_in_a_region = ...
        }, { // region R0
          %operation_wants_to_use_value_defined_in_a_region = xyz(%value_defined_in_a_region
        }
       //
    ```
    
    3. **A value defined in a region can be used by an operation which has a
    parent in the same region, if and only if the parent could use the
    value**:
    
    Here is the example to demonstrate the above rule
    ```
       { // Region R
           %value_defined_in_region = ...
    
           // CASE: I
           %parent_defined_in_same_region_as_value_and_can_use_value = ...({ '' Region R1
                %operation_uses_value = ...(%value_defined_in_region, ...
           })
    
          // CASE : II
          %parent_defined_in_same_region_as_value_and_cannot_use_value_as_isolated_from_above = ...({ // Region R2
                %operation_uses_value = ...(%value_defined_in_region, ...
           })
       }
    
    ```
    
    Case I: For evaluation `R1`, the `InterpreterScope` object corresponding
    to the `R` is provided as `parentScope`. Hence the value
    `value_defined_in_region` will be visible.
    
    Case II: For evaluation `R2`, the `nullptr` provided as `parentScope`
    depending on the op trait
    [OpTrait::IsolatedFromAbove](https://mlir.llvm.org/docs/Traits/#isolatedfromabove).
    Hence the value `value_defined_in_region` will not be visible.
    
    ## Testing
    
    As there is no region op currently implemented to test the PR, we have
    implemented a simplified version of `whileOp`. The simplification is
    that if the runtime value corresponding to `cond` block is `true`, then
    we iterate the loop for fixed (2) times.The simplification has to be
    made because there is no `compreOp` implemented at time time.
    
    We used the `whileOp` to implement a program, `interpret_while.mlir`,
    which captures values from various allowable scope. The illegal
    reference to scopes will be flagged by the versifier only. So the
    interpreter can assume the input program to always obey the scoping
    rules and thereby avoid checking for illegal cases.
    
    
    update: Changed name `InterpretScope` to `Scope`
    [Lexical_scope_vs._dynamic_scope](https://en.wikipedia.org/wiki/Scope_(computer_science)#Lexical_scope_vs._dynamic_scope)

[33mcommit dcf3a6a0c099094de3e8cf474a3cada6abeb1e2a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Feb 13 03:13:02 2023

    Link recently added frontend contract tickets in the spec (#1158)
    
    See #1156 (Decide on speccing numerical accuracy) and #1157 (Audit
    numerical corner cases) for more details.

[33mcommit d4d4591e0135d803214ef5cf87050e9a4db7c55d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 10 21:49:00 2023

    Sort BUILD.bazel file (#1148)

[33mcommit d05787858c278fb82c6efb877c6a40c06ef60b50[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 10 21:02:06 2023

    Allow -f fix for lint_markdown.sh (#1147)
    
    This change allows for 2 arguments to be specified, and forward them to
    the `markdownlint-cli`. This allows us to use the `-f` flag to autofix
    markdown issues.
    
    Modifying contribution from @scottamain, if there is any reason to avoid
    this flag let me know!

[33mcommit 636c72be6351642186e3ab54dab9f1f17c58cdb6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 10 20:50:13 2023

    Update the ODS with input constraints from the spec (#1146)
    
    Allow consistency cover all the spec constraints.

[33mcommit 95eb9ae6a4771ee8aa340b7227b1bd56fe464f82[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 9 23:18:16 2023

    Use const reference for loop variable to avoid copy (#1095)
    
    Manually fixing as part of clang-tidy check which is currently not
    supported in StableHLO #61

[33mcommit 053300e2c5c0e47a3836692711d4efb0589ba472[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 9 22:42:25 2023

    Integrate LLVM at llvm/llvm-project@f100ec2517e9 (#1094)

[33mcommit 14ba2bae5549d662fd761c031d6db08930b21392[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 9 02:16:16 2023

    Standardize the usage of braces in control flow statements (#1042)
    
    Bootstrapping from #1039, this PR removes braces around functions which
    only have one line for conciseness across all cpp files.

[33mcommit a5e160527c7c1eb2df3b10d50af6d356e798543a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 9 00:27:24 2023

    Integrate LLVM at llvm/llvm-project@785009e19fbc (#1091)

[33mcommit 0be55936d02c7d579bee7afe827969dcf42aec3c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Feb 8 23:42:20 2023

    Modify build file so StableHLO can be included in other projects (#1090)
    
    This should allow us to use one build file for mlir-hlo, tensorflow, and
    stablehlo repositories.
    
    Boils down to: tablegen targets need `include = ["."]` and most other
    targets need `skip_include_prefix = "."`, inspired by
    https://github.com/tensorflow/mlir-hlo/blob/master/BUILD

[33mcommit 5b31b015cf622f5442addeb61fe4ec7e971402e6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Feb 8 22:24:03 2023

    Don't load Tensor dialect in StableHLO dialect constructor (#1088)
    
    I recently noticed this code when reviewing #849, and I'm not sure why
    we need it there.
    
    This seems like a pretty strong statement about a fundamental role of
    the Tensor dialect in the workings of the StableHLO dialect, and I don't
    think we have established that yet.
    
    It would seem that we've inherited this from MHLO when bootstrapping
    StableHLO (#1), but I don't think I understand the reasoning on the MHLO
    side either. This change was introduced as part of an LLVM integrate
    (https://github.com/tensorflow/mlir-hlo/commit/ba0346b071f4212008943c50f47766e677992b56),
    and the commit description doesn't go into detail about motivation.
    
    Given that, I propose to revert this in the StableHLO dialect and see
    what happens. All tests in this repository are passing, but maybe we'll
    learn more after downstream integrations.

[33mcommit 9b1bc5a213431f782ed73ddb674ed4adef7b52e6[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Feb 8 19:32:12 2023

    Fork FuncOp and CallOp to VHLO (#899)
    
    Leaving out `ModuleOp` for now. It is awkward to convert ModuleOp to
    something else, also would also require adding `--no-implicit-module` to
    all calls which is less ergonomic and easy to miss. May decide to avoid
    forking `ModuleOp`, and instead just enforce that the op has valid
    types/attrs. If that op disappears we can get clever in the
    BytecodeReader. We can discuss this further next week.
    
    This PR (built on top of ):
    - Fork Ops: `FuncOp, CallOp`
    - Fork Types: `FunctionType`
    - Fork Attributes: `DictionaryAttr, TypeAttr` (`TypeAttr` used for
    `FunctionType` attribute)
    - Add conversions:
      + `func.func` <--> `vhlo.func`
      + `func.call` <--> `vhlo.call`
      + `func.return` --> `vhlo.return`
    + `vhlo.return` --> `func.return` if parens is `vhlo.func`, else
    `stablehlo.return`
    - Added pretty print for func ops, since these are so common and generic
    format of functions is difficult to write tests for.
    
    Closes #850.

[33mcommit e5db23da6e866560a420123d6bc002e459149d1f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Feb 8 01:15:20 2023

    Integrate LLVM at llvm/llvm-project@27aeb58ce4d1 (#1084)

[33mcommit d2635a14bc83b818dfa4d40f36eec93448ff67a3[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Feb 8 00:19:35 2023

    Add Input constraint labels (#1081)
    
    Added Input constraint labels to help refer to input constraint while
    1. writing PR description like [this](
    https://github.com/openxla/stablehlo/pull/996#discussion_r1096336612)
    which makes sure all the constraints have corresponding verification
    logic and tests.
    2. Adding comments in tests and implementation of verifiers.

[33mcommit c1d210b66aa9733331969110c1dde5a394e8c89c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 7 23:42:39 2023

    Update reference checklist w.r.t adding testing strategy (#1082)
    
    Please refer to
    https://github.com/openxla/stablehlo/pull/996#discussion_r1096336612

[33mcommit f9e9d0f2c0dc367c646f90e36d09a040bea9f046[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Feb 7 23:10:48 2023

    Add interpreter for SliceOp (#996)
    
    We have the following constraints in the spec:
    
    ```
    // Ix constraints come from the Inputs table
    (I1) operand tensor
    (I2) start_indices 1-dimensional tensor constant of type si64
    (I3) limit_indices 1-dimensional tensor constant of type si64
    (I4) strides 1-dimensional tensor constant of type si64
    (C1) operand and result have the same element type.
    (C2) size(start_indices) = size(limit_indices) = size(strides) = rank(operand).
    (C3) 0 <= start_indices[d] <= limit_indices[d] <= dim(operand, d) for all dimension d.
    (C4) 0 < strides[d] for all dimension d.
    (C5) dim(result, d) = ... for all dimension d in operand.
    ```
    
    These constraints will be comprehensively covered by the following
    tests:
    
    ```
    I1: a) operand is not a tensor. (Covered by ODS).
    I2: a) start_indices is not a tensor. (Covered by ODS).
        b) rank(start_indices) != 1.
        c) element_type(start_indices) != si64. (Covered by ODS).
    I3: a) limit_indices is not a tensor. (Covered by ODS).
        b) rank(limit_indices) != 1. (Covered by ODS).
        c) element_type(limit_indices) != si64. (Covered by ODS).
    I4: a) strides is not a tensor. (Covered by ODS).
        b) rank(strides) != 1. (Covered by ODS).
        c) element_type(strides) != si64. (Covered by ODS).
    C1: element_type(operand) != element_type(result). (Covered by ODS).
    C2: a) size(start_indices) != rank(operand).
        b) size(limit_indices) != rank(operand). (Covered by ODS).
        c) size(strides) != rank(operand). (Covered by ODS).
    C3: a) start_indices < 0.
        b) start_indices > limit_indices.
        c) limit_indices > shape(operand).
    C4: a) strides <= 0.
    C5: no negative test needed since it's just inferring the shape
    ```
    
    If we drop the "Covered by ODS" pieces, this will leave us with the
    following test cases:
    
    ```
    I2b: rank(start_indices) != 1.
    C2a: size(start_indices) != rank(operand).
    C3a: start_indices < 0.
    C3b: start_indices > limit_indices.
    C3c: limit_indices > shape(operand).
    C4a: strides <= 0.
    ```
    closes #990

[33mcommit 8cc948f6bb240f8885a92ae65a3a59fba506e1b2[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 7 19:31:44 2023

    Link frontend contract tickets in the spec (#1076)
    
    Now that we have clearly identified the deltas between how the StableHLO
    dialect is used by frontends and what is specced (see
    https://github.com/orgs/openxla/projects/6), I figured it would be good
    to link these deltas in the spec, so that it's clear what areas are
    expected to change in the near future.
    
    As I was working on this pull request, I also called out several tickets
    which are not part of the Frontend Contract project, because they
    represented highly likely todos from the New Features project. I think
    this is also useful to include in the spec for the readers to understand
    the anticipated direction of StableHLO evolution.

[33mcommit 079ef0e3ae34dfb2d676646b293eb94a645e8704[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Feb 7 07:38:28 2023

    Delete .bazelversion (#1075)
    
    Due to a limitation of downstream integration tools, the .bazelversion
    file is required to have a license header, and that's not recognized
    correctly by the Bazel build.
    
    Let's delete the file for now and look for ways to improve the
    integration tooling to be able to restore it.

[33mcommit 430e4622f3880a1379710a17cdc2be3d0de04b6b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 7 07:16:08 2023

    Add copyright tag to bazel configuration files (#1074)

[33mcommit 54cc761e5af64cf4c1c803f954296ec8f80a18bc[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Feb 7 03:46:06 2023

    Add VHLO support for FP8 (#925)
    
    This provides compatibility guarantees for FP8 types `f8E4M3FN` and
    `f8E5M2`. This change is relatively simple and shows all that goes into
    supporting a new datatype in VHLO.
    
    Closes #906.

[33mcommit e782fe61ab339e715f535ce79bc13e144c3103ab[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Feb 7 02:12:04 2023

    Fix signature of `extraClassDefinition` for `UniformQuantizedV1Type` (#1073)

[33mcommit 34914e2588b2b19c61d508987611853953a67cd7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Feb 6 20:38:44 2023

    Revert #958: Verify compare_type for compare_op (#1071)
    
    Reverting https://github.com/openxla/stablehlo/pull/958 to unblock
    downstream integration.
    
    ## Details
    The tests which are failing downstream falls into two categories:
    (1) Verifier failing on compareOps with wrong combination of
    element-type and compare_type.
    (2) Verifier failing on creating `compareOp` with `compare_type ==
    NOTYPE`. Note that creating a compareOp with `NOTYPE` is still allowed.
    (3) Type inference failures: As the implementation of verification and
    type inference is unified for this op, the above mentioned categories
    also contributed to type inference failures.

[33mcommit b2baafa784f13d0bc72610004730a184bb1b53e7[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Feb 6 18:19:18 2023

    Move and merge inferRngOp (#1064)
    
    Move inferRngOp() and merge with verifyRngOp().

[33mcommit 39dfa1cb18c00af2eecb3803fddb1b52c6e56394[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Feb 6 04:26:52 2023

    Move VhloTypeDefs.cpp.inc into VhloTypes.cpp (#1053)
    
    As discussed in #899, it is desirable to move the following includes
    from VhloOps.cpp to VhloTypes.cpp:
    
      #include "stablehlo/dialect/VhloTypeInterfaces.cpp.inc"
      #define GET_TYPEDEF_CLASSES
      #include "stablehlo/dialect/VhloTypeDefs.cpp.inc"
    
    This way, vhlo_types becomes self-contained, i.e. we will no longer have
    to link its users with vhlo_ops. Given the fairly involved dependency
    structure around the VHLO dialect, this is a desirable design property.

[33mcommit d3015e26096e1943b8785bcd6951efd3089b87e5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Feb 5 19:53:35 2023

    Addressed a few clang-tidy suggestions (#1059)
    
    Come up during internal integration.

[33mcommit 5ee08ec2ed8e1f9e51b3b38b862b3a69c4b4e3a2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sun Feb 5 19:22:50 2023

    Use public runners on forked repos (#1052)
    
    Resolves actions stalling endlessly on forks with:
    <img width="662" alt="image"
    src="https://user-images.githubusercontent.com/3036970/216694700-4b071f21-2408-4d37-b594-889b0ced2b24.png">
    
    
    See: https://github.com/GleasonK/stablehlo/actions/runs/4087406604
    - clang-format has the change and uses the public runner
    - lint whitespace does not and waits for runner
    
    Forked repo running CI:
    https://github.com/GleasonK/stablehlo/actions?query=ci-runner
    OpenXLA repo running CI:
    https://github.com/openxla/stablehlo/actions?query=ci-runner

[33mcommit 90f16b25cee07700a36eb9b6b560fa568901f588[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Feb 5 19:17:34 2023

    Fix the notations for dynamic_update_slice and dynamic_slice (#1040)
    
    `adjusted_start_indices` seems like a 1-d tensor from the type of
    `shape(operand)` in `clamp(0, start_indices, shape(operand)
    -slice_sizes)`.

[33mcommit ffd745f5607968d73aa4c16585b61b6c0bdf9318[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Feb 5 19:07:53 2023

    Update interpreter testing guideline with coverage criterion (#1050)
    
    The PR updates the interpreter testing guidelines based on
    https://github.com/openxla/stablehlo/pull/996#discussion_r1093590287.

[33mcommit 525463cc9b90b4acfdb52eafd03ab1caf4a34c0d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Feb 5 18:54:10 2023

    Add licence notice to BUILD.bazel (#1060)

[33mcommit 2ec1f191863612b98db49a5593ca421e5885ca86[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Sat Feb 4 19:09:52 2023

    Change return type for shape function of bounds (#1054)
    
    This is a quick fix revert back the return type of inferMost/LeastType
    methods from ShapeType to Type, to accommodate the tuple type in MHLO
    tests.

[33mcommit 0b37aa361f44d3399b6b2a4c4ebf90fbbd7e9a57[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Feb 4 18:40:58 2023

    Add .bazelignore (#1057)
    
    This enables Bazel to skip the Bazel build of the llvm-project/ folder
    that shows up when building the repo using the standard CMake workflow
    documented in README.md.
    
    Without this, Bazel gets pretty confused and produces rather cryptic
    error messages.

[33mcommit aab5c722d7590e5801438bb689049d5fd124696a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Feb 4 07:56:15 2023

    Add interpreter for ReverseOp (#1006)
    
    closes #985

[33mcommit c6bf20f8a9f41e69489bb30ce65ed92229a60d71[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 3 23:09:36 2023

    Fork Attributes and Types for VHLO (#849)
    
    - Change `CustomCallApiVersion` to an enum instead of an integer value.
    + To avoid transforming _all_ integers, the transformation from
    StableHLO --> VHLO special cases this conversion
    - Attributes
    + Forked Attributes: `IntegerAttr, StringAttr, UnitAttr, ArrayAttr`,
    `DenseIntOrFPElementsV1Attr, FlatSymbolRefV1Attr, FloatV1Attr`
    - Types
    + Forked Types: `RankedTensorType, UnrankedTensorType, TupleType,
    WitnessType`, `BFloat16V1Type, Float16V1Type, Float32V1Type,
    Float64V1Type, IndexV1Type, ComplexV1Type, IntegerV1Type,
    UniformQuantizedV1Type`
    - Bytecode implementations forked from
    [BuiltinDialectBytecode.cpp](https://github.com/llvm/llvm-project/blob/c48e0cf03a50bb8a2043ac4bb5e9a83ff135247a/mlir/lib/IR/BuiltinDialectBytecode.cpp)

[33mcommit e99cf277874213128eac75c6eaa0a60f8bf962c8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Feb 3 18:34:09 2023

    Handle unranked input types in `inferMostSpecificTypeComponents` (#1046)
    
    This is to fix the case when all the `inputTypes` in
    `inferMostSpecificTypeComponents` is unranked.

[33mcommit 81ad66901bfcd901592d46e86d3902bd3f58180e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Feb 3 06:19:24 2023

    Enable specializing dynamically-shaped StableHLO programs to static shapes (#1041)
    
    Dynamic shapes in StableHLO are an appealing feature for ML frameworks,
    but not all ML compilers implement them. As a result, there is a
    practical need for infrastructure that would bridge this gap by
    specializing dynamically-shaped StableHLO programs to static shapes.
    
    For example, we may want to take the following StableHLO program that
    performs a broadcasting addition of two dynamically-shaped tensors and
    then specialize it to static shapes based on ahead-of-time or
    just-in-time knowledge of the static shapes of its arguments.
    
    ```
    func.func @main(
        %arg0: tensor<i32>,
        %arg1: tensor<?x4xf32>,
        %arg2: tensor<2x?x4xf32>) -> tensor<2x?x4xf32> {
      %0 = stablehlo.constant dense<2> : tensor<1xi32>
      %1 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32>
      %2 = stablehlo.constant dense<4> : tensor<1xi32>
      %3 = stablehlo.concatenate %0, %1, %2, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
      %4 = stablehlo.dynamic_broadcast_in_dim %arg1, %3, dims = [1, 2] : (tensor<?x4xf32>, tensor<3xi32>) -> tensor<2x?x4xf32>
      %5 = stablehlo.add %4, %arg2 : tensor<2x?x4xf32>
      return %5 : tensor<2x?x4xf32>
    }
    ```
    
    Let's say that we know that %arg0 (which carries the size of the
    unbounded dimensions of %arg1 and %arg2) is equal to 3, then the
    corresponding statically-shaped program might look as follows.
    
    ```
    func.func @main(
        %arg1: tensor<3x4xf32>,
        %arg2: tensor<2x3x4xf32>) -> tensor<2x3x4xf32> {
      %0 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x4xf32>) -> tensor<2x3x4xf32>
      %1 = stablehlo.add %0, %arg2 : tensor<2x3x4xf32>
      return %1 : tensor<2x3x4xf32>
    }
    ```
    
    This PR implements an initial version of --stablehlo-refine-shapes, a
    pass that propagates shape information across StableHLO programs. In
    this initial version, we're making the following simplifying
    assumptions:
    
    (A1) Current design for StableHLO dynamism. We are using the current
    design for StableHLO dynamism, but we're also planning to update this
    pass to account for the changes in the upcoming dynamism RFC when it's
    ready. We found it useful to implement the first version of shape
    refinement prior to implementing the new design for dynamism and not the
    other way around in order to speed up the delivery.
    
    (A2) Shape polymorphism. We are limiting ourselves to StableHLO programs
    that support shape polymorphism. In such programs, only data can depend
    on shapes (e.g. a broadcast can produce data of a dynamically-computed
    shape), but shapes cannot depend on data. Therefore, the shapes of all
    intermediate values can be computed from the shapes of the inputs.
    
    (A3) Full specialization. We are only interested in full specialization
    to statically-shaped StableHLO programs. In alignment with A2, this
    means that we want the resulting programs to have statically-shaped
    types and constant shape values. Specialization to bounded dynamism
    might be an interesting avenue for future work.
    
    (A4) Inlined programs. We assume that these programs consist of a single
    StableHLO function, i.e. that it's fine to ask users to inline their
    programs into one main function before running shape refinement.

[33mcommit c3d77899e6c119c3ba328133d665782760996101[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Feb 3 02:19:36 2023

    Update llvm lint tool with flag to disable SHA256 check (#1044)
    
    The SHA256 check takes awhile, and really only impacts llvm revision
    bump changelists. Given that I run all lint scripts on pre-commit, I'd
    prefer a way to only verify commit, if commit changes, then I can worry
    about sha256 validation. In CI we will always check both.
    
    Also I refactored the script a bit, there was so much global state that
    it was difficult to make modifications, modified to have minimal global
    state and functions that echo their results. We lose some logging with
    this, but the error messages we see are still pretty good.
    
    Mismatch error messages:
    
    ```bash
    # ... commit mismatch
    
    $ ./build_tools/github_actions/lint_llvm_commit.sh -s .
    Validating LLVM commit hash...
    Commit mismatch:
    1c1
    < f58de2125caf75ec0d40bc3e094a93c0b314a66a
    ---
    > 858de2125caf75ec0d40bc3e094a93c0b314a66a
    
    Auto-fix using:
      $ lint_llvm_commit.sh -f <path/to/stablehlo/root>
    
    # ... SHA256 mismatch
    
    $ ./build_tools/github_actions/lint_llvm_commit.sh .
    Validating LLVM commit hash...
    Commit hashes match.
    Validating LLVM SHA256 hash...
    ...SHA256 mismatch:
    1c1
    < 896047c57addc2a349defab0c7fb84979aeac039ed9ada2901cd2e0ca88fffb6
    ---
    > a96047c57addc2a349defab0c7fb84979aeac039ed9ada2901cd2e0ca88fffb6
    
    Auto-fix using:
      $ lint_llvm_commit.sh -f <path/to/stablehlo/root>
    
    # ...  404
    
    $ ./build_tools/github_actions/lint_llvm_commit.sh .
    Validating LLVM commit hash...
    Commit hashes match.
    Validating LLVM SHA256 hash...
    ...SHA256 mismatch:
    1c1
    < a96047c57addc2a349defab0c7fb84979aeac039ed9ada2901cd2e0ca88fffb6
    ---
    > Error 404 downloading LLVM at commit '858de2125caf75ec0d40bc3e094a93c0b314a66a'.
    
    Auto-fix using:
      $ lint_llvm_commit.sh -f <path/to/stablehlo/root>
    ```

[33mcommit 56c85b073b3d527678fe763dceb11be08831717f[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Feb 2 23:44:59 2023

    Update CI to use faster runners (#1043)

[33mcommit aad57d1cf8c46828df4d40b4725080ac7c4e0c6b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 2 21:33:03 2023

    Standardize formatting in Ops.cpp and Interpreter.cpp (#1039)
    
    Removes braces around functions which only have one line for
    conciseness.

[33mcommit 00bffab1b7f187a59e256809cca76e29a05feed1[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 2 19:22:56 2023

    Migrate away from using MLIR Attributes in `evalTransposeOp` (#1034)
    
    This PR migrates away from using `DenseIntElementsAttr` in favor of
    `SmallVector<int64_t>`. The rationale is that it is harder to construct
    MLIR wrapper classes, and we want to avoid having a mix of custom
    classes (Tensor) and MLIR classes (DenseIntElementsAttr, with ConstantOp
    being an exception for now) so that we can use the `evalFoo` functions
    when a more complex op can benefit from reusing them.
    
    closes #1031

[33mcommit 68673c309417eabf0f049063fad4f0906cebfeae[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Feb 2 17:05:20 2023

    Also rename legalize_stablehlo_to_vhlo.mlir (#1035)
    
    The pass was renamed in #895, but I forgot to rename the test file.

[33mcommit a235e9c1415e586940e32afaef2ed86374e121d4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 2 05:44:03 2023

    Handle bounds in the Select shape function (#961)
    
    For all dimensions, it infers the most specific of the `onTrue` and
    `onFalse` bounds, following the same inference rule in
    `inferMostSpecificTypeComponents`, so there is no change required to the
    inference rule except that the code for
    `inferMostSpecificTypeComponents` is updated to also include bounds
    information.
    closes #750

[33mcommit e28fac48833374bd7686407718953d3801f8dd79[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Feb 2 02:03:02 2023

    Update lint_llvm_commit.sh to also verify SHA256 (#1032)
    
    In our previous LLVM Integrate, the SHA256 was calculated incorrectly.
    This PR addresses this by also verifying that the SHA256 value is also
    correct even if the llvm commit matches.

[33mcommit 08897ad748bccb5db548b8077eec391068fc3fa0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Feb 1 00:54:15 2023

    Integrate LLVM at llvm/llvm-project@f58de2125caf (#1024)

[33mcommit 846b3c1ab9e11bdf4b4b63817f6f2eecc713b91d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Feb 1 00:52:04 2023

    Clean up GitHub workflows (#1014)
    
    This PR contains a number of assorted cleanups:
      1) Alignment between buildAndTestCMake.yml and buildBazel.yml
         (including workflow and job names as well as commits).
      2) Adding license headers next to copyrights, to be consistent with
         other files in the repository.

[33mcommit 36b0c51c8de90f1b37a27e4be403721f00f3bcf4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 31 20:28:39 2023

    Update signature of `infer<Op>` function for re-usability (#1019)
    
    Based on the proposal discussed @
    https://github.com/openxla/stablehlo/issues/1000, this PR implements the
    second step: "Sharing the shape inference code from TypeInference.cpp"
    
    Some of the salient  features of the PR:
    1. Addresses updating the signature of the type-inference-methods of 4
    four ops (`concatenate, dot_general, reduce, slice`) which, according to
    #1000, could be shared for type-inference and reference implementation.
    
    2. The shared code (e.g. `inferDotGeneralOp, inferConcatenateOp,
    inferSliceOp, inferReduceOp`) do have code related to verification of
    input/output constraints. That means, the reference implementation will
    execute that verification code
    whenever it wants to re-use the eval for those ops. Those verification
    checks are redundant, as they must have already been executed while
    parsing the code, and affect the performance of the interpreter. Despite
    of that I decided to keep the code as is based on following reasons:
    - Performance is a non-goal for the "readable" reference. After all it
    affects only 4 ops.
    - Some part of type inference is tied closely with the verification code
    (e.g. usage of [newDimensions at
    inferReduceOp](https://github.com/openxla/stablehlo/blob/211bd058c88a0c1164b1e77c0dd1e067c6a1bb63/stablehlo/dialect/TypeInference.cpp#L2567)).
    Isolating just type inference counterparts involve duplication of code
    affecting the performance of the verification/inference which is
    critical. I note that this observation is true only for `inferReuceOp`.
    For the remaining ops, the split seems possible.
    
    @burmako @zhouxin913 Please let me know your opinion on this.

[33mcommit b8f97cf15495617e8fbaea98b2f3977df66ecfa8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 31 16:43:53 2023

    Update reference_checklist: Add constraint labels in ODS (#1025)
    
    Adding constrain-labels, from specification, in the ODS. This is along
    the same line with the checklist items of adding the labels in
    `TypeInference.cpp` and `StablehloOps.cpp`.

[33mcommit 7be911a1a8468248efdc16e2a51ed5ff30c1cfbb[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jan 31 02:31:36 2023

    Improve type inference for DynamicGatherOp (#1012)
    
    If slice_sizes are constant, then we can infer static dimension sizes
    for the offset dims parts of the result shape.

[33mcommit 3afb0a3764d25757a9708787e4fa62b3a4e2f2ca[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jan 30 21:47:27 2023

    Remove type inference for ReturnOp (#1011)
    
    I added a shape function for ReturnOp when working on an initial version
    of #622 which was relying on --tf-shape-inference.
    
    TensorFlow's shape inference needs all ops which are not hardcoded in
    the pass to have shape functions, so I added one for ReturnOp even
    though it is has unclear utility beyond this specific use case (ReturnOp
    doesn't have any results, so its shape function returns an empty vector,
    which isn't super useful).
    
    Now that we're working towards a dedicated shape refinement pass for
    StableHLO, we no longer need a shape function for ReturnOp, so I'm
    proposing to remove it in this PR.

[33mcommit 391e4f69ab506aef793ce88ede89cea7af518e8a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jan 30 21:35:41 2023

    Fix reifyGatherShape to return tensors of index (#1017)
    
    MLIR-HLO commit: tensorflow/mlir-hlo@0c65568.

[33mcommit 5150724620e1b5cde46a27bf14123758a05c6e75[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jan 30 21:23:29 2023

    Rename BUILD -> BUILD.bazel (#1023)
    
    Plenty of related issues across GH projects - Bazel supports build files
    named either `BUILD` or `BUILD.bazel`. In
    https://github.com/bazelbuild/bazel/issues/4517 the community recommends
    the more explicit `BUILD.bazel` naming. This will also help us
    disambiguate bazel and blaze build file scripts when necessary.

[33mcommit 4332cca0824b443653de369bcb1a4de62d666b16[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jan 30 21:06:43 2023

    Document TorchIndexSelectOp (#1018)
    
    MLIR-HLO commit: tensorflow/mlir-hlo@6353126.

[33mcommit 442e89d5eb07830e01cb83d04b591aab286af237[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jan 30 19:19:10 2023

    Support ReduceWindowOp creation with a Region (#1016)
    
    MLIR-HLO commit: tensorflow/mlir-hlo@f95a903.

[33mcommit 7a3a2a853f843c2663af88debed9cafab5645bd8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Jan 30 19:18:49 2023

    Mark ReducePrecisionOp as [Pure] (#1015)
    
    MLIR-HLO commit: tensorflow/mlir-hlo@539446d.

[33mcommit f14bb210a9730b03c361d750c7439a4ad66e2022[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jan 30 19:18:35 2023

    Ensure consistent naming for spec and reference document files (#1020)
    
    Renaming the interpreter checklist, so that we have reference.md and
    reference_checklist.md just like we have spec.md and spec_checklist.md.

[33mcommit 211bd058c88a0c1164b1e77c0dd1e067c6a1bb63[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Jan 29 20:50:20 2023

    Update eval signature for re-usability (#1007)
    
    Based on the proposal discussed @
    https://github.com/openxla/stablehlo/issues/1000, this PR implements the
    first step: "to replace `eval(AddOp, ...)` with `eval_add(Type
    returnType, ...)`.
    
    This changes immediately enables reusing the `eval_op` for an `op` whose
    type can be trivially inferred (like most of the elementwise ops).

[33mcommit a867ce18ff83d6f70b4dde8b4828405c8d9b6eca[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Jan 28 00:20:27 2023

    Interpreter implementation and review guidelines (#951)
    
    Provides a doc to be followed while introducing the interpreter
    implementation of an op.

[33mcommit ccc28c607cbda0c18983ed638b46e4b82a437347[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jan 27 21:13:51 2023

    Integrate LLVM at llvm/llvm-project@c4c5e79dd4b4 (#1003)
    
    Now generated code unwraps the `FailureOr` type before calling into
    parse method:
    
    ```
    auto odsCustomResult = parseDimSizes(odsParser,
          ::mlir::detail::unwrapForCustomParse(_result_bounds));
    ```
    
    This changes the interface requirement from
    `FailureOr<SmallVector<int64_t>> &` to `SmallVector<int64_t> &`. The
    change is NFC, I shifted the implementation from one method to another
    since it makes it more concise and avoids a copy/move.

[33mcommit 9137eb65356cbf723aafc46a5711e2779ce032bd[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Jan 27 03:07:48 2023

    Verify compare_type for compare_op. (#958)
    
    closes #571

[33mcommit a8130088ac3c432650a31b19b75eec595e1d4ac5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 27 01:19:22 2023

    Handle bounds in the FftOp shape function (#911)
    
    Bounds of the result are inherited from the bounds of the operand,
    except for the bound of the last dimension for RFFT/IRFFT which is ?
    because it is computed statically.
    
    closes #805

[33mcommit 2636119c7c61b1e1814cdb7e30a38d341c05e1e8[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jan 25 23:12:59 2023

    Create a Bazel build (#788)
    
    Also fixes a cron bug that runs every minute past 12 to once a day at 12
    closes #14

[33mcommit 17af8e5c82d98cb22f62768a26ae1072eec81826[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Jan 25 20:45:58 2023

    Remove InferReturnTypeComponents test pattern from hlo_test_infer. (#960)
    
    * Move all usages of get_return_type_componets to get_return_types.
    * Move `CHECK` directives in tests before the corresponding MLIR code.
    * Simplify`CHECK` directives to read `type0 = <someType>, types1 = <someType>`
    
    closes #938

[33mcommit 24f7d4f13b7e7d17908b4ff539590e72cd24e2ef[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 24 21:48:41 2023

    Handle bounds in `TriangularSolveOp` shape function (#930)
    
    The shape and bounds of batching dimensions is inferred using
    inferMergedDimAndBound.
    
    If non-batching dimensions of both A and B are dynamic with bound, then
    the following inference rule applies. The shape is propagated the same
    way bounds are propagated. Arbitrary bounds are provided for A, B, and X
    to visually aid in how the they are moved:
    ```
    A * X = B, R=rank
    
                    | A   | X   | B   | X[R-2], X[R-1]
    ---------------------------------------------------------------
    left_side=false | 3x3 | 3x5 | 3x5 | min(A[R-1], A[R-2]), B[R-1]
    
    X * A = B, R=rank
    
                    | X   | A   | B   | X[R-2], X[R-1]
    ---------------------------------------------------------------
    left_side=true  | 2x3 | 3x3 | 2x3 | B[R-2], min(A[R-1], A[R-2])
    ```
    
    closes #807

[33mcommit e663680e6952825e3684920f3b88170debec3db8[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Jan 24 20:29:42 2023

    Handle bounds among branches (#819)
    
    This PR proposes an approach to infer the sizes and bounds for ops with
    branches.
    Back to months ago, https://github.com/openxla/stablehlo/pull/229 tried
    to resolve this exactly same question in a restrict approach, inferring
    the most restrict shape from the result type among all branches: but
    that PR does not submit, leaving an open question of this topic.
    
    Inspired by how HLO handle this problem
    https://github.com/tensorflow/tensorflow/blob/v2.11.0/tensorflow/compiler/xla/service/shape_inference.cc#L2988
    ```
    // For each subshape, If any of the branch is dynamic, we say result is
      // dynamic:
      //   true_branch  (s32[<=4])
      //   false_branch (s32[4])
      // Result is s32[<=4].
    ```
    this PR proposes a most least specific inference approach with rules as
    below (assuming dim of lhs and dim of rhs are compatible):
    ```
    // Inference rules for conditional branches (lhs/rhs are commutative):
    //       Dim of lhs     Dim of rhs      Infer
    //  c0:  X              X               X
    //  c1:  X              ?               ?
    //  c2:  X              ?, B            ?, max(X, B)
    //  c3:  ?              ?               ?
    //  c4:  ?              ?, B            ?
    //  c5:  ?, B           ?, C            ?, max(B, C)
    ```
    Note that C2 is a superset of the XLA code above.
    
    The reasoning behind:
    1. For binary op `stablehlo.add`, the result type must match with both
    `lhs` and `rhs`, thus the inferred type is of cause generated from the
    most restricted way.
    2. However, for ops with branches, it is unknown whether every branch
    will be actually executed in the runtime, thus inferring a restrict
    return type would raise false positive errors, even tho it may help
    reduce false negative errors. For example a `case` op with branch_0
    return `tensor<3x?x>` and branch_1 return `tensor<?x4x>`: inferring
    `tensor<3x4x>` is too strong if branch_0 is always executed in runtime
    with a valid output with shape `<3x100x>` and consumed by the following
    ops who don't care branch_1 at all and assume the input are always like
    `<3x?x>`.
    
    Add reviewer: @smit-hinsu

[33mcommit 37adb87021e5183daf62a0744d6814352067aef7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 24 18:06:22 2023

    Handle bounds of Gather op (#908)
    
    The dimension sizes of result, corresponding to offset dimensions,
    depend on attributes (like `collapsed_slice_dims` and `slice_sizes`) and
    hence are always static.
    
    Whereas, the dimension sizes of result, corresponding to batch
    dimensions, depends on input `start_indices` and could be dynamic. The
    corresponding bounds, in that case, are propagated from the
    `start_indices`.
    
    
    
    Side notes:
    I deliberately avoid making any changes in `inferGatherShape` which is
    currently shared by `GatherOp::inferReturnTypeComponents` and
    `GatherOp::reifyReturnTypeShapes`. The reason being I do not
    `reifyReturnTypeShapes` needs to handle the bounds.
    
    Because of the decision, we have some duplicate code in line 1237
    (duplicate of
    https://github.com/openxla/stablehlo/blob/afcbb953c12a2a3a8564cab73278f4b47021ec01/stablehlo/dialect/TypeInference.cpp#L1149).
    
    PTAL @smit-hinsu

[33mcommit d0b59f6b3fa286d26c3d1df5b6b0bfbe0d63bf8c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 24 17:25:50 2023

    Verify dimension in GetDimensionSizeOp (#927)
    
    Of the two constraints mentioned in #790:
    * (C1) is already checked by
    [verifyDimAttr](https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/StablehloOps.cpp#L117-L120),
    but have added a negative test
    * (C2) is not added per discussion
    [here](https://github.com/openxla/stablehlo/pull/791#discussion_r1053899143)
    
    closes #790

[33mcommit 51f005f0a8ff6e28f535adfec4de936cb4097aa4[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Jan 24 01:27:48 2023

    Integrate LLVM at llvm/llvm-project@171e7b831227 (#957)

[33mcommit 005aa34e52f38bcfed5aedf3cd693e715f7bfaad[m
Author: Tres <tpopp@users.noreply.github.com>
Date:   Mon Jan 23 22:15:38 2023

    Update dialects to use the folder adaptors (#936)
    
    This is just an NFC change, and the folders should be cleaned up
    afterwards.

[33mcommit 993de9b159e6c4ce903eb534b41c8c5bb6f12491[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Jan 23 22:14:28 2023

    Add local code coverage generation tooling (#950)
    
    ### LCOV Tooling
    
    ```
    Usage:
    $ ci_build_stablehlo_code_coverage.sh [-g][-o output_dir] <llvm_build_dir> <stablehlo_build_dir>
        -g               Generate HTML report (default false)
        -o <output_dir>  Set the output to generate report (default /tmp/ccov)
    ```
    
    ### Example:
    
    ```
    ./build_tools/github_actions/ci_build_stablehlo_code_coverage.sh ~/path-to/llvm-build/ ~/path-to/stablehlo-build/
    -- Building StableHLO standalone
    -- Using MLIRConfig.cmake in: /usr/local/google/home/gleasonk/Coding/llvm-build//lib/cmake/mlir
    -- Using LLVMConfig.cmake in: /usr/local/google/home/gleasonk/Coding/llvm-build/lib/cmake/llvm
    -- Building with -fPIC
    -- Configuring done
    -- Generating done
    -- Build files have been written to: /usr/local/google/home/gleasonk/Coding/stablehlo-build
    [0/1] Running the StableHLO regression tests
    
    Testing Time: 1.01s
      Passed: 47
    Subroutine read_intermediate_text redefined at /usr/bin/geninfo line 2637.
    Subroutine read_intermediate_json redefined at /usr/bin/geninfo line 2669.
    Subroutine intermediate_text_to_info redefined at /usr/bin/geninfo line 2717.
    Subroutine intermediate_json_to_info redefined at /usr/bin/geninfo line 2806.
    Subroutine get_output_fd redefined at /usr/bin/geninfo line 2886.
    Subroutine print_gcov_warnings redefined at /usr/bin/geninfo line 2914.
    Subroutine process_intermediate redefined at /usr/bin/geninfo line 2944.
    LCOV data at:
      /tmp/ccov/ccov_2023_01_20_18-50-05/cov.info
    
    Summary:
    Reading tracefile /tmp/ccov/ccov_2023_01_20_18-50-05/cov.info
                                              |Lines       |Functions  |Branches
    Filename                                  |Rate     Num|Rate    Num|Rate     Num
    ================================================================================
    [/usr/local/google/home/gleasonk/Coding/openxla/stablehlo/stablehlo/]
    dialect/AssemblyFormat.cpp                |98.4%    247| 100%    32|    -      0
    dialect/AssemblyFormat.h                  | 100%     14| 100%     6|    -      0
    dialect/Base.cpp                          |96.4%    197| 100%    16|    -      0
    dialect/Base.h                            |86.4%     66|84.8%   269|    -      0
    dialect/BroadcastUtils.cpp                |70.3%     37| 100%     4|    -      0
    dialect/ChloBytecode.cpp                  |72.4%     58|76.5%    17|    -      0
    dialect/ChloOps.cpp                       |90.2%    307|76.1%    88|    -      0
    dialect/ChloOps.h                         | 100%      1| 100%     2|    -      0
    dialect/Register.cpp                      | 100%      4| 100%     1|    -      0
    dialect/StablehloBytecode.cpp             |95.0%    317|96.9%    64|    -      0
    dialect/StablehloOps.cpp                  |87.3%   2099|76.2%   277|    -      0
    dialect/StablehloOps.h                    | 100%      1| 100%     3|    -      0
    dialect/TypeInference.cpp                 |97.2%   2521|98.5%   130|    -      0
    dialect/TypeInference.h                   | 100%      8| 100%     1|    -      0
    dialect/Version.cpp                       |90.0%     30|71.4%     7|    -      0
    dialect/Version.h                         | 100%      9| 100%     6|    -      0
    dialect/VhloBytecode.cpp                  |87.6%    315|92.2%    64|    -      0
    dialect/VhloOps.cpp                       |85.3%     34| 100%     5|    -      0
    dialect/VhloOps.h                         | 100%      3| 100%     4|    -      0
    reference/Element.cpp                     |84.8%    244|86.2%    80|    -      0
    reference/Element.h                       | 100%      9| 100%     7|    -      0
    reference/Errors.h                        | 0.0%      2| 0.0%     4|    -      0
    reference/Index.cpp                       |77.4%     31|75.0%     4|    -      0
    reference/Index.h                         |87.5%      8|80.0%     5|    -      0
    reference/Interpreter.cpp                 |91.0%    122| 100%     3|    -      0
    reference/Ops.cpp                         |98.8%    162| 100%    20|    -      0
    reference/Tensor.cpp                      |96.5%    345|93.3%    30|    -      0
    reference/Tensor.h                        | 100%      6| 100%     8|    -      0
    reference/Types.cpp                       | 100%     20| 100%     6|    -      0
    tests/TestUtils.cpp                       |97.7%     88| 100%    14|    -      0
    tools/StablehloInterpreterMain.cpp        |90.9%     22| 100%     5|    -      0
    tools/StablehloOptMain.cpp                | 100%     10| 100%     1|    -      0
    transforms/StablehloLegalizeToVhlo.cpp    |96.0%    126|99.7%   352|    -      0
    transforms/TypeConversion.cpp             | 100%     13| 100%     4|    -      0
    transforms/TypeConversion.h               |88.1%     59|66.7%    21|    -      0
    transforms/VhloLegalizeToStablehlo.cpp    |97.5%    120|99.7%   352|    -      0
    transforms/VhloToVersion.cpp              |93.3%    150|83.8%    68|    -      0
    ================================================================================
                                        Total:|92.6%   7805|90.8%  1980|    -      0
    ```
    
    If more specifics are required, use generated HTML files to figure out
    unhit lines:
    
    ```
    ./build_tools/github_actions/ci_build_stablehlo_code_coverage.sh -g ~/path-to/llvm-build/ ~/path-to/stablehlo-build/
    ...
    Overall coverage rate:
      lines......: 92.6% (7228 of 7805 lines)
      functions..: 90.8% (1798 of 1980 functions)
    HTML report at:
      /tmp/ccov/ccov_2023_01_20_18-49-10
    LCOV data at:
      /tmp/ccov/ccov_2023_01_20_18-49-10/cov.info
    ...
    ```

[33mcommit 4c07853142e0dc599a8ad18edefedfc506ae9758[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Fri Jan 20 22:32:50 2023

    Make build instructions repeatable (#953)
    
    use `mkdir -p`, so that it doesn't fail if the dir already exists

[33mcommit 3ad8afc77c7c6ad49ede967de7eed8b6ce0d19ab[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Fri Jan 20 22:30:29 2023

    Fix Reduce op bounds inference for scalar results (#935)
    
    encoding attribute is not required if the result is a scalar but the
    current code will end up incorrectly setting it to the input encoding
    attribute.
    
    For the added test, the inferred result type was `tensor<f32,
    #stablehlo.type_extensions<bounds = [3]>>` but scalar tensors shouldn't
    have a bounds attributes. This was happening because boundsToEncoding
    returns the original encoding if the provided bounds are empty.
    boundsToEncoding doesn't differentiate between bounds being empty for
    scalars or bounds being empty for unbounded tensors.
    
    I also checked that the other inference functions are not having a
    similar issue. (May have missed recently submitted shape functions.)
    
    Reduce op inference also doesn't deal with sparse encoding attribute so
    we don't have to worry about that here.
    
    cc @zhouxin913

[33mcommit baf004481fb9333a078c2fda6159e267a19266fd[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Fri Jan 20 22:14:51 2023

    Integrate LLVM at llvm/llvm-project@afca08a567e3 (#948)
    
    Requires fixes from https://github.com/openxla/stablehlo/pull/949 to
    build.

[33mcommit 2ec1f729d7fab0c705d1cf41b987bf480c38d5af[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 20 21:04:33 2023

    Verify that start_indices have the same type in DynamicSliceOp (#942)
    
    `inferDynamicUpdateSliceOp` also has the same verification logic, so
    I've moved the duplicate logic to `variadicTensorsHaveSameElType`
    function.
    closes #557

[33mcommit 0c95648c531a20bbb2a4c0d7447fa9d79177d266[m
Author: Benjamin Kramer <benny.kra@gmail.com>
Date:   Fri Jan 20 19:01:21 2023

    Fix tblgen rules in CMake after the VHLO reorganization (#949)

[33mcommit f5eb7e7f018cf17623287c1e91299ca76ea5b587[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 20 02:18:21 2023

    Add "Constraints" column to Inputs/Outputs (#947)
    
    Some arguments are missing constraints as they are mentioned in the
    semantics section and/or are part of the Type column
    
    Steps I followed to add the constraints for all ops (including ops that
    already have constraints column):
    1. Add (missing) constraints for all instances of the Name occurring in
    the constraint
    2. Abbreviate 3 or more consecutive constraints to save horizontal space
    3. Omit adding Constraints column if none of the sections have
    constraints
    
    Exception to the above steps:
    * bitcast_convert: (C2) does not pertain to specific inputs/outputs
    
    Other notes:
    * Fixed typo in OrOp
    * Minor formatting change in ScatterOp
    
    closes #614

[33mcommit c9515d76a6e3b3c4467b3cff83a15dcc0176cdcf[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 19 23:20:31 2023

    Update infer_stablehlo.mlir (#946)
    
    Adds missing file split marker

[33mcommit 337b4d1cc82bc869fe4d06058db4eb69e6235584[m
Author: Scott Main <smain@google.com>
Date:   Thu Jan 19 22:24:35 2023

    Add new intro and build steps for readme (#933)
    
    More to come...

[33mcommit 5ded1b1e3e99df37fdbf876b0d11e03a758a5bcf[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Thu Jan 19 22:05:38 2023

     Reorganize VHLO .td files (#945)
    
    Fixes #944

[33mcommit 687a6c71e4265d3016bc71882bfa8bb55457905d[m
Author: Scott Main <smain@google.com>
Date:   Thu Jan 19 21:32:19 2023

    Add github action to run markdownlint on pull requests (#941)
    
    This will resolve issue #792

[33mcommit a5e53ce995c5471251648606a6ce93208bdb0136[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jan 19 00:36:41 2023

    Add interpreter testing guidelines (#816)

[33mcommit 46b6e52fa3709f0156521d0ff3debe64b1461a1e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 19 00:01:57 2023

    Update ODS to sync supported types to the spec (#924)
    
    Notes:
    * Atan2: As discussed in #500,
    [shape_inference.cc](https://github.com/tensorflow/tensorflow/blob/2c1b3efe573befdda3f3db399eebb875e690082c/tensorflow/compiler/xla/service/shape_inference.cc#L1028-L1043)
    does allow integers, but our spec doesn't. However, since the ODS is now
    in sync with StableHLO spec, I'm marking the verification status to
    `yes`.
    
    closes #262
    closes #499
    closes #512
    closes #513
    closes #587

[33mcommit 11bb08a8babb10a57a6e81d9da495a08dadc0550[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Wed Jan 18 20:46:19 2023

     Integrate LLVM at llvm/llvm-project@b3eb004ca78f  (#939)

[33mcommit 4a4856a266fece2f3fbcc0d888badc89775edb77[m
Author: Scott Main <smain@google.com>
Date:   Wed Jan 18 20:39:41 2023

    Mention that mardkownlint does not check ol indentation (#940)

[33mcommit 92b33d8d275b50a4a565b8e8034394de1b82f93a[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Jan 18 02:29:34 2023

    Infer bounds for BatchNorm* Ops. (#900)
    
    closes #809, closes #810, closes #811, closes #869
    
    Unify verification and inference for BatchNorm* Ops
    
    * For all `batch_norm_*` ops infer all shapes and bounds from `operand`
    only.
    * Verify `operand` feature dimension compatibility with `dim[0]` of
    `scale`.
    * Remove `AllShapesMatch` and convert `AllTypesMatch` to
    `AllElementTypesMatch`.
    * Check for shape compatibility instead of equality.

[33mcommit 02a37a538b54d3ecf37e82d74365b8f759a19ecc[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 17 21:52:40 2023

    Handle bounds in the ReduceWindowOp shape function (#916)
    
    The bounds are dictated by how the shape is manipulated during the
    reduction on a window moved by the strides. `inferWindowOutputShape`
    takes the shape and inferred window to calculate the output shape.
    Conveniently, we can replace the shape with the bound and get our
    desired inferred bounds.
    
    closes #753

[33mcommit a87554414632b16b1163e01fc7a848575f27fc07[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Jan 17 21:29:18 2023

    Infer bounds for AllToAllOp. (#921)
    
    closes #801
    
    Infer bounds for `all_to_all` op
    * Infer bounds information from operand type.
    * Update bounds and dimension sizes for split and concat dimensions according to the op logic.

[33mcommit c60b687808d6f63c084d55f4510681afec7f52a6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Jan 17 17:22:38 2023

    Remove remaining usages of llvm::Optional (#937)
    
    llvm::Optional has been replaced with a legacy alias of std::optional
    and is planned to be removed after LLVM 16. Let's migrate away.

[33mcommit 067750f1bfaf3361611571fb2f545f5db0351d2e[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Sun Jan 15 18:41:48 2023

    Support bounds in hlo_test_infer.get_return_type_components rewrite pattern. (#902)

[33mcommit 1ce14742540481019a7931e0bd9db082febff840[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sun Jan 15 18:39:13 2023

    Added missing detail for remainder op (#877)
    
    For more details please refer to the ticket below.
    
    fixes https://github.com/openxla/stablehlo/issues/846

[33mcommit f1419c330e431a2a469bab67c9d0ac2c2b570430[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Jan 14 01:49:52 2023

    Handle bounds in the SelectAndScatterOp shape function (#917)
    
    SelectAndScatterOp's bound inference is very simple. There are two
    inputs: `operand` and `source`. The `source` contains the indices to the
    `result` tensor to place the reduced values from `operand`, so it does
    not affect the bound of the `result` tensor. As for `operand`, the type
    is equal to the `result` tensor. There is only a movement of values
    happening in this op (after reduction is applied, but this also does not
    affect the bounds), so we can propagate the bounds with no further
    inference done.
    
    closes #754

[33mcommit da8e1c50a94b6cd45ca903d5383b352c82f7ef93[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 13 22:30:39 2023

    Relax remaining dimension checks to better support dynamism (#922)
    
    Notes
    * `verifyReduceScatterOp` function terminates early if the `operandType`
    or `resultType`'s `scatterDimension` is dynamic, so this particular
    change is not needed, but still revised it just to be safe.
    * rng_bit_generator tests were defining unused constants, so those are
    removed.
    
    closes #572
    closes #873

[33mcommit 0db4a2ab026d054b201a6aab64204f01ac772b65[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Jan 13 21:18:36 2023

    Handle bounds for ScatterOp shape function (#920)
    
    fixes #749
    
    For the ScatterOp, where the
    (C16) inputs[k] and result[k] have the same type for any k  [0, N)
    
    and this is how the result type is inferred.
    
    (https://github.com/openxla/stablehlo/blob/81d520685c5cfe2620f995dbb09ac4399d000c9a/stablehlo/dialect/TypeInference.cpp#L2513).
    
    IMO, we do not need to do anything special to handle the bounds as the
    bounds should get copied as part of copying the types. I wrote a test to
    check that input bounds are indeed propagated. Also I note that I do not
    have to use `inferReturnTypeComponents` at all, for this case.

[33mcommit f7359589c8d6561e555ff54f13e398b0a874ac6a[m
Author: Scott Main <smain@google.com>
Date:   Fri Jan 13 20:10:36 2023

    Increment all headings under Ops (#929)
    
    This is to fix incorrect heading level nesting. All op names are now
    nested under the "Ops" H2 heading.

[33mcommit def86ab362b7c2629bc093f3bcd149997a8e7d35[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 13 19:51:43 2023

    Verify dimensions in ReverseOp (#928)
    
    Now that all the constraints are verified, the status.md can be flipped
    to `yes`.
    closes #273

[33mcommit f8fd330bae7ec26b657a34b929577f788d64a221[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Jan 13 08:13:23 2023

    Infer bounds for SortOp. (#913)
    
    Infer bounds for `SortOp`
    * For ranked tensors infer bounds from corresponding tensors in `inputs`
    to` results`.
      * For unranked tensors copy the shape as is.
    
    Additional context - A stricter inference can be implemented where we
    can infer the most specific shape from all tensors in the input, as of
    now having a simple solution that is standard across most ops tops the
    requirement of inferring most specific shape.

[33mcommit 884da41a1af4e69586a287801f8f5639ec1938cf[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Jan 12 05:48:06 2023

    Handle bounds in the WhileOp shape function (#909)
    
    WhileOp has trivial shape function which use operands types as result
    type. So nothing changes in this PR except providing a unit test cover
    that the result types are same as operand types with bounds.
    
    @smit-hinsu for review.

[33mcommit fe3d8e623d8aefbe603f2637fe638b53981874f7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 12 04:54:26 2023

    Rename grid to "process grid" (#923)
    
    Renames all instances of "grid" to "process grid"
    closes #669

[33mcommit b9d28e332e216693d7afd27fb047b33993a9bc05[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Jan 12 04:43:21 2023

    Verify exponent and mantissa in ReducePrecisionOp (#919)
    
    closes #488

[33mcommit 81d520685c5cfe2620f995dbb09ac4399d000c9a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jan 12 00:31:50 2023

    Replace makeArrayRef with ArrayRef in Python bindings (#915)
    
    The recent integrate PR (#912) missed fixing a few deprecated
    makeArrayRef calls because our build infrastructure didn't flag them
    (see #544 for the ticket that talks about the details).
    
    This PR handles the missed calls.

[33mcommit 1ef5065d19129e068e1a9eff20241523cff8acca[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Jan 11 23:40:02 2023

    Make precision config optional in VHLO (#905)
    
    Found this while converting some models to VHLO. `PrecisionConfigAttr`
    in stablehlo is an optional attribute. Missed this since it appears
    without the `OptionalAttr` tag in op definitions, because the attribute
    itself is defined using it.

[33mcommit 52b6d47e4db763708634415247ab47f4a6c180bd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Jan 11 21:06:39 2023

    Handle bounds in the CholeskyOp shape function (#887)
    
    Cholesky op's [constraint
    (C3)](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cholesky)
    requires the last two dimensions to be the same. The bound rules are as
    follows (3 being an arbitrary static shape):
    ```
    dynamic type inference rules for the last two dims of A (case0 is sanity
    check for dynamic batch dims):
           dim R-2 | dim R-1 | inferred R-2 | inferred R-1
    case0: 3       | 3       | 3            | 3
    case1: ?       | 3       | 3            | 3
    case2: ?       | ?       | ?            | ?
    
    dynamic bound infererence rules for the last two dims of A (case0 is
    sanity check for dynamic batch dims):
           dim R-2 | dim R-1 | inferred R-2 | inferred R-1
    case0: 3, ?    | 3, ?    | 3, ?         | 3, ?
    case1: ?, ?    | 3, ?    | 3, ?         | 3, ?
    case2: ?, A<3  | 3, ?    | error        | error
    case3: ?, A>=3 | 3, ?    | 3, ?         | 3, ?
    case4: ?, ?    | ?, ?    | ?, ?         | ?, ?
    case5: ?, A    | ?, ?    | ?, A         | ?, A
    case6: ?, A    | ?, B    | ?, min(A,B)  | ?, min(A,B)
    ```
    
    The rules proposed above are reviewed and agreed upon, but there will be
    no immediate implementation in the current PR to follow recently updated
    guidelines.
    
    closes #804

[33mcommit d2cc1fa54d09611692c6650ba084a0e8f6542bfc[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Jan 11 07:44:28 2023

    Integrate LLVM at llvm/llvm-project@d414c2ad0cac (#912)

[33mcommit 643c7f966b1f15f7500e9b3ae045430e19b80ba4[m
Author: Scott Main <smain@google.com>
Date:   Wed Jan 11 01:39:46 2023

    Lint cleanup for list indentation (#841)
    
    Clarify the style/lint rules for lists and apply new no-indentation for
    list items (except nested lists).

[33mcommit 8a8865a58bf0571c80a1276269b66aa021ffbaec[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Jan 10 23:27:39 2023

    Fix typo in spec.md (#910)

[33mcommit afcbb953c12a2a3a8564cab73278f4b47021ec01[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Jan 9 18:43:30 2023

    Update spec status for convolution op: `revisit` to `yes` (#904)
    
    As per
    https://github.com/openxla/stablehlo/pull/441#discussion_r1041744301,
    the spec should be a `revisit` for #730 which is now fixed.

[33mcommit 395f63efee5f0427be1bf6dac36fba5db215069d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 02:42:57 2023

    Create inferFooOp functions for some infeasible ops (#898)
    
    In this PR, we introduce inferFooOp functions for ops where result's
    element type is load-bearing: convert, convolution, dot, dot_general,
    uniform_quantize.
    
    These functions return SmallVectorImpl<ShapedTypeComponents>& and only
    fill in the shape part. Even though these functions aren't be usable
    from the existing type inference framework (missing element types will
    mean that type inference will fail), they will allow sharing shape
    inference logic with #622.
    
    bitcast_convert is an odd one. It does belong in this category, but its
    shape curiously depends on the relationship between the operand type and
    the result type, so implementing inferBitcastConvertOp is not possible,
    even if it returns ShapedTypeComponents.

[33mcommit c94aba3e6528e496ecc37fbd56d5d1d8fdcef549[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 02:02:48 2023

    Simplify some dimension checks by using verifyCompatibleDims (#897)
    
    Refactors TypeInference.cpp to use
    `verifyCompatibleDims(dimSize1, dimSize2)` instead of the equivalent but
    more verbose:
    
    ```
    isDynamicDimSize(dimSize1) || isDynamicDimSize(dimSize2) ||
    dimSize1 == dimSize2
    ```

[33mcommit 66dbb530afb5e275e67c51cb68642bc27717609e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 02:02:37 2023

    Consistently use HLO_StaticShapeTensor (#891)
    
    Some ops in the StableHLO dialect have "load-bearing" result shapes,
    meaning that these shapes cannot be inferred from op's operands,
    attributes and regions.
    
    For these ops, the current design philosophy involves one of the two
    approaches:
      1) The shape is specified statically in the result type, e.g. see
         ReshapeOp.
      2) The shape is passed dynamically as an operand (or can be computed
         indirectly from shape-related operands), e.g. see DynamicReshapeOp.
    
    In the future (#8), we want to have one unified approach in these
    situations, but we're not there yet, and this PR is about an incremental
    improvement to the state of the art.
    
    One important piece of the design philosophy for 1) is that these ops
    need to be restricted to produce HLO_StaticShapeTensor, otherwise it is
    unclear what their semantics is.
    
      %1 = "stablehlo.reshape"(%0) : (tensor<2x4xf32>) -> tensor<?x?xf32>
    
    For example, the code snippet above doesn't make sense because we need
    an actual shape to execute the op - either passed at compile time or
    passed at runtime - and neither is happening here.
    
    When working on #622, I discovered that there are some ops which fall
    into 1) but don't have their result types restricted to be static:
    InfeedOp, IotaOp, RecvOp and RngBitGeneratorOp.
    
    This means that StableHLO producers can currently produce these ops with
    dynamic result types, which would be unsound. This PR fixes this issue
    by changing the result types to HLO_StaticShapeTensor or equivalent.

[33mcommit f9086f5717750ca9648db4395938bf67060c6636[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 01:35:11 2023

    Relax dimension checks in TriangularSolveOp (#893)
    
    inferTriangularSolveOp currently directly compares dimension sizes for
    constraints C2 and C3.
    
    This was working fine before we introduced the notion of
    isCompatibleForHloTypeInference, but nowadays this is too restrictive.
    We should use verifyCompatibleDims instead.

[33mcommit 654dece7bf53464ab86aa0e1d8b988d529560ad3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 00:46:53 2023

    Make CHLO ops inlinable (#894)
    
    When working on #622, I discovered that functions with CHLO ops aren't
    getting inlined, which is a miss because there's no reason for these ops
    to not be inlinable. For comparison, StableHLO and MHLO both implement
    this interface.
    
    This PR fixes that and adds tests to make sure that things stay this way
    in the future as well.

[33mcommit 50ba566bb60596bbffb48a602eade4ac0ca5e5a6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 00:45:34 2023

    Rename .cpp files for legalize passes to match pass names (#895)

[33mcommit bcc6be5abb5bddf6407e2627531f1aa39c53228c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Jan 7 00:06:48 2023

    Remove outdated #defines in Passes.h (#896)

[33mcommit 7be79ea26cf86ae41b0c9c3edc14d4bae602e196[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jan 6 23:56:53 2023

    Fix clang-format lint action to skip deleted files (#901)
    
    Changed to `git diff $BASE_BRANCH HEAD --name-only --diff-filter=d`
    
    In plain terms this reads:
    
    Diff the base branch (`origin/main` by default) with `HEAD`, only show
    file names, and skip any deleted files.
    
    The lint action was failing since `clang-format FileNotFound.cpp` exits
    with a bash error code.
    
    I made the changes in #895 locally to test the fix. Should work after we
    merge this in.

[33mcommit 17ea052cf9b89e077fc2d913776c15e5bf0789bd[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jan 6 20:56:59 2023

    Integrate LLVM at llvm/llvm-project@7504e9a19346 (#889)

[33mcommit 4b8f875ccca34011171928760fbcad7f4420c2c8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jan 6 20:07:20 2023

    Remove AllShapesMatch from DynamicUpdateSliceOp (#892)
    
    DynamicUpdateSliceOp's ODS currently says:
    
    ```
    def StableHLO_DynamicUpdateSliceOp: StableHLO_Op<"dynamic_update_slice",
          [Pure, AllElementTypesMatch<["operand", "update", "result"]>,
           AllShapesMatch<["operand", "result"]>, InferTensorType]> {
      ...
    }
    ```
    
    This was working fine before we introduced the notion of
    isCompatibleForHloTypeInference, but nowadays this is too restrictive.
    We should just drop AllShapesMatch<["operand", "result"]> because shape
    compatibility between operand and result is anyway checked by our type
    inference infrastructure.

[33mcommit 1b3976546cfe9039499ce20e766e17dc03af0477[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Jan 6 20:00:52 2023

    Fix typos in infer_stablehlo.mlir tests (#881)
    
    I found a couple of inconsistencies in the tests in regards to what
    value to return (and lack thereof), so this PR fixes it.

[33mcommit 6b92de6e272cbc43069cfdabb532a1c2c7c102c8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jan 6 19:09:58 2023

    Also allow one item in precision_config (#890)
    
    MLIR-HLO commit: tensorflow/mlir-hlo@35ee344.

[33mcommit 2ddd566e62561e44d929d2f02d4d4d1bf9ee8358[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Jan 6 18:43:15 2023

    (NFC) Reorganize bytecode interface implementations (#888)
    
    This PR just moves code around to keep read/write implementations next
    to each other.
    
    Having spend a bit of time reading through the
    [BuiltinDialectBytecodeInterface](https://github.com/llvm/llvm-project/blob/c48e0cf03a50bb8a2043ac4bb5e9a83ff135247a/mlir/lib/IR/BuiltinDialectBytecode.cpp)
    it has an organization that makes finding implementations much much
    easier than our layout.

[33mcommit 1740851603e6efc9f9989796fb743bc21201b811[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Jan 6 02:28:53 2023

    Expose verifyConvolutionAttributes in TypeInference.h (#882)
    
    This enables code sharing opportunities with StableHLO.
    Related MLIR-HLO commit:
    https://github.com/tensorflow/mlir-hlo/commit/b90f997f15d7e9d19714a8ac6886f8edf9472661.

[33mcommit 4899d385a27e8d625db062b47a766dfb42dcdc6a[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Jan 5 21:21:55 2023

    Reinstate util functions in TypeInference.h (#883)
    
    The declaration of these util functions are removed in previous PRs
    because they are used in `TypeInference.cpp` only. But it turns out that
    they are used by MHLO as well: so need reinstate them.

[33mcommit 9b65bb5f0d89155cad21221716a5357210de7a0f[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Jan 5 21:19:04 2023

    Handle Dynamic Bounds in Shape Function of DynamicUpdateSliceOp (#824)
    
    The PR fixes #799 (please refer this for more details)
    
     Adding @smit-hinsu for review.

[33mcommit 6890318d60ed69a91224fece94291ef4692a8477[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Jan 5 03:31:08 2023

    Integrate LLVM at llvm/llvm-project@9cd113a2a8fa (#880)

[33mcommit 77dca286f5ada8de7db4be4f7546358afdc33fd4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Jan 3 18:54:43 2023

    Add missing constraint for selectAndScatter op (#876)
    
    fixes https://github.com/openxla/stablehlo/issues/857

[33mcommit e2aa7fe97cd09f44d864079c4e8be98064e5b425[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 29 21:44:08 2022

    Integrate LLVM at llvm/llvm-project@9b59207a9850 (#854)

[33mcommit d17bba386b594208e40efae9b297a743a1513ad2[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 29 15:25:39 2022

    Move inferFftOp and verifyRngOp with Enums (#853)

[33mcommit 48e7059a63308ed06fca0433eedcefa689905fef[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 29 01:37:39 2022

    Move verifyRecvOp and verifyInfeedOp with TokenType  (#852)

[33mcommit 5798b442ff6aba75c81d2ec6f698ccb19022f6e1[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 28 18:51:45 2022

    Add verification for DotOp precision config (#839)
    
    closes #838

[33mcommit fd5ca98b303ea5cc0fa14acd821b136cda0715ad[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Dec 28 02:27:55 2022

    Reduce the boilerplate in specs of numerical ops (#847)
    
    Compresses the definitions of numerical ops to:
    * Drop mentions of implementation-defined behavior (in the Errors
    section, we already say that errors lead to implementation-defined
    behavior).
    * Drop mentions of numerical accuracy (at the moment, we always say that
    it's implementation-defined, so we might as well say nothing to avoid
    boilerplate).
    * Prefer oneliner bullets to describe behavior for individual categories
    of element types.
    
    I didn't quite understand the spec for RemainderOp, so I left it mostly
    alone and opened #846 to track it.

[33mcommit 5a6ab05b7dc637c296889b9cb59e274ef290fd13[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Dec 28 02:07:09 2022

    Update the spec of AllToAllOp (#826)
    
    This PR follows up on #794 and updates the spec which I forgot to update
    originally.

[33mcommit 0c176bc15f2f32e9804b7a1de69ff0160d1019ef[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 28 00:02:21 2022

    Fix MD table spacing (#845)

[33mcommit 55856467bac91ff148225cb1be48548ea0d1e9f3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 27 22:57:52 2022

    Replace "foo of any supported type" with "foo" (#844)
    
    This is a convention that dates back to the very first specs that we
    wrote, but upon second look I think it's just boilerplate. E.g. we don't
    say "function of any supported types" or "tuple of any supported types".
    
    The newly introduced formalization for types makes it very clear what
    are the various supported tensor types, value types, etc, so let's make
    things easier to read.

[33mcommit 1748a36da94e49c776800b947ec6b5fe7ef701bb[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 27 22:56:14 2022

    Remove "stablehlo" from "#stablehloadd" links (#843)
    
    The "stablehlo" part of spec sections and spec permalinks is redundant.
    It's a StableHLO spec, so it is reasonable to assume that it specs
    StableHLO ops.

[33mcommit b58084a843482bf2274743f45e3cd69882290554[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 27 22:54:00 2022

    Remove the index of ops from the spec (#842)
    
    ToCs are automatically maintained by GitHub when there are 2 or more
    headings:
    https://github.blog/changelog/2021-04-13-table-of-contents-support-in-markdown-files/.

[33mcommit 57c6015077db05977231fbc81e38bc67ef0fcc3a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 27 22:07:05 2022

    First step towards turning spec.md into a programming language spec (#840)
    
    As the first step towards implementing #484, this PR introduces an EBNF
    grammar for the StableHLO language, modelling it after the generic
    assembly format of the StableHLO dialect.
    
    In addition to creating a foundation for future work as described in
     #484, this PR immediately improves clarity of the Types and Constants
    sections of the specification.
    
    Next steps involve: 1) writing the Notation section along the lines of
     #289, 2) formalizing execution of the StableHLO language in the
    Execution section.

[33mcommit b4e5527e9a7f0f52b3305e9c047bf04282cfc9f9[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Dec 27 22:00:09 2022

    Integrate LLVM at llvm/llvm-project@35762d073910 (#836)

[33mcommit 4a895afbf765a273397b57ec844652656a44237b[m
Author: Scott Main <smain@google.com>
Date:   Tue Dec 27 21:13:17 2022

    Fix MD lint errors for spec.md (#820)
    
    Fixes to the markdown include:
    + Use a consistent 2-space indentation for unordered lists.
    + Set line lengths to 80 characters, except for cases in which doing so
    would adversely affect readability for the source file such as for long
    formulas (in which case, the rule is temporarily disabled with the
    `markdownlint-disable` tag). (Unfortunately the line-length rule cannot
    be customized for code font‚Äîonly for pre-formatted code blocks.)
    + Specify code language for all code blocks.
    
    Modify the configuration such that:
    + Allow duplicate heading names as long as the headings are not siblings
    (separate sections can use the same sub-headings).
    + Allow images without alt-text, due to the complexity of images and
    relatively small number of readers (certainly something to revisit if
    a11y is an issue).
    + Prefer 2-space indentation for unordered lists (there is no matching
    config for ordered lists, so those should use no indentation for the
    top-level list items)
    
    Co-authored-by: Sandeep Dasgupta <sdasgup@google.com>

[33mcommit efbc09992344bd5934e06a46b679d355aba8f6e9[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Sun Dec 25 05:09:36 2022

    Move 13 verifiers shape functions to shared target (#825)
    
    This PR tries to finish a full scan of all verifiers and shape functions
    need to be moved to `TypeInference.h/cpp`, following
    https://github.com/openxla/stablehlo/pull/269 and
    https://github.com/openxla/stablehlo/pull/783.
    This PRs moves:
    
    1. inferCompareOp
    1. inferDynamicGatherOp
    1. inferGatherop
    1. inferImagOp
    1. inferReplicaIdOp
    1. inferUniformDequantizeOp
    1. verifyAllGatherOp
    1. verifyDynamicPadOp
    1. verifyReducePrecisionOp
    1. verifyReshapeOp
    1. verifyRngBitGeneratorOp
    1. verifyScatterOp
    1. verifySelectAndScatterOp
    
    Leave alone these complicated TODOs in the future with new issues: so
    this PR fix https://github.com/openxla/stablehlo/issues/270
    method to move      | why difficult  | ticket
    ----------- | ----------- | ---
    Customcall::verify() | has list of `OutputOperandAliasAttr` |
    https://github.com/openxla/stablehlo/issues/829
    FftOp::inferReturnTypeComponents() | has `FftType` from
    `StablehloEnums.td` | https://github.com/openxla/stablehlo/issues/830
    RngOp::verify() | has`RngDistribution` from `StablehloEnums.td` |
    https://github.com/openxla/stablehlo/issues/830
    InfeedOp::verify() <br/> RecvOp::verify() | has `TokenType` defined in
    `StablehloOps.h` | https://github.com/openxla/stablehlo/issues/831
    | SetDimensionSizeOp::verify() <br/> GetDimensionSizeOp::verify() <br/>
    SetDimensionSizeOp::inferReturnTypes() | wait Spec for clear
    verification | https://github.com/openxla/stablehlo/issues/832

[33mcommit 60c8b142219cf6a865abeb5bc9428ad140757cfb[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 23 00:27:00 2022

    Fix some typo in  dynamicUpdateSlice (#822)
    
    The PR fixes a typo in the specification of `DynamicUpdateSliceOp`
    related to how `max` argument of `clampOp` is determined.
    
    
    As a side note:
    Recommended one improvement related to
    dynamic_slice/dynamic_update_slice ops in
    https://github.com/openxla/stablehlo/issues/289#issuecomment-1363409092

[33mcommit 93b3300d1ca7605b31fdbbd2a7fd959677d17563[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Dec 22 22:45:00 2022

    Add missing copyright (#823)

[33mcommit 738d3926237954c96f9bec5f792a7e501dae592e[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Dec 22 20:54:20 2022

    Integrate LLVM at llvm/llvm-project@2916b9918275 (#821)

[33mcommit f41632074d102ca08bc6fe7cfe4b186ae178eb26[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Dec 21 19:11:59 2022

    Separate VhloTypes into separate tablegen file (#797)
    
    This is similar to how MHLO manages their custom types in
    [hlo_ops_typedefs.td](https://github.com/tensorflow/mlir-hlo/blob/master/mhlo/IR/hlo_ops_typedefs.td).
    
    These typedefs will also grow in upcoming submissions as other types are
    forked.
    
    This allows us to use `VhloBase.td` as a tablegen target in a bazel
    build since it does not have complicated dependencies (adding typedefs
    makes a new dependency to VHLO_Dialect from VhloOps.td).

[33mcommit 2ef2f994492304c4871489bf846c3965343a8d80[m
Author: Scott Main <smain@google.com>
Date:   Wed Dec 21 18:26:14 2022

    Add shell script and GitHub Action to run markdownlint (#814)
    
    As per #792, here's a script to run `markdownlint`. The bash script
    includes usage instructions. You just need to pass it a file, directory,
    or glob pattern for the MD files you want to lint. It uses a Docker
    image because I fear the npm package version dependency hell, but the
    docker image is versioned.

[33mcommit 2085d7e6c8ceb88ef88be3693872dd8e69a0f18e[m
Author: George Necula <gcnecula@gmail.com>
Date:   Wed Dec 21 13:40:53 2022

    Small fix to a link in StablehloOps.td (#818)

[33mcommit e8c1c049d39afa957f5419c17ab285ff53329257[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Dec 21 06:06:01 2022

    Disable shape functions for ConvolutionOp, DotOp and DotGeneralOp (#817)
    
    In their current formulation, these ops cannot have shape functions
    because their lhs/rhs element types can be different from their result
    element type.
    
    Perhaps in the future we will add the `preferred_element_type` attribute
    to these ops (see #600), and then shape functions will be feasible to
    write again, but today we have to disable them.

[33mcommit 3a3788ad2d6894f65db1917e1f86d53b07e98184[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Wed Dec 21 05:20:26 2022

    Add spec for GetDimensionSizeOp (#791)
    
    Verification is missing two constraints. Filed
    https://github.com/openxla/stablehlo/issues/790 for that.
    
    RFC discussing this op: https://github.com/openxla/stablehlo/pull/194

[33mcommit d69627753411cc06de4ba72877ed1b27792d6be8[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Wed Dec 21 05:19:30 2022

    NFC: Use rank($value) for constraints in spec wherever possible (#815)

[33mcommit fb65c58bed2efdad23ad6ea7109ee5500babfba5[m
Author: Scott Main <smain@google.com>
Date:   Wed Dec 21 01:26:03 2022

    More MD syntax fixes, mostly line length (#795)
    
    More fixes for #792 with expanded scope to MD files not under /docs.
    (Still not touching the spec yet.)
    
    This includes markdownlint fixes that could not be fixed by
    `markdownlint-cli`, such as line length (80 char), consistent list item
    indications, proper heading levels, removal of bash "$" character when
    not necessary
    ([rationale](https://cirosantilli.com/markdown-style-guide/#dollar-signs-in-shell-code);
    if the snippet includes multiple commands, then add a line break).

[33mcommit 68eb0fd4afb636f966e17ceb14ce9a5bf8fa1913[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Dec 21 01:25:46 2022

    Move shape inference for PartitionIdOp to shared file. (#785)

[33mcommit d6998add35ddaa06c685fd63e909af95166e526e[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Dec 21 01:24:27 2022

    Add tests, increase coverage, cover reifyReturnTypeShapes for ops. (#777)
    
    fixes #624

[33mcommit 93e7cf81bf8258021ef5d96c31682ab893cb82da[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 20 21:50:42 2022

    Add channel_id to AllToAllOp (#794)
    
    Rationale for approval is the same as in #272, #388, #403 and #673: this
    is a non-controversial backward-compatible change, and accepting it
    doesn't violate any of the existing commitments (it sticks to existing
    HLO semantics, and it is compatible with the extent of the current
    compatibility commitments).
    
    MLIR-HLO commit: tensorflow/mlir-hlo@bd07cb9.

[33mcommit 7c268a82316a8dbcb3111bfb93240c48ae783f6d[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Dec 20 21:08:36 2022

    Move 26 verifiers shape functions to shared target (#783)
    
    To continue deduplicate the verifiers and shape functions among
    StableHLO and MHLO, this PR, following
    https://github.com/openxla/stablehlo/pull/26, continue move 24 verifiers
    & shape functions from `StablehloOps.cc` to `TypeInference.h/cpp` with
    change to static methods and organize with alphabetical order.
    
    This PR also includes:
    1. Unify emit error methods with static `emitOptionalError()`
    2. Similar to the discussion of
    https://github.com/openxla/stablehlo/issues/481, in the signuature of
    the new `inferAllToAllOp()`, I use `int64_t` instead of `uint64_t` to
    pass all the parameters to avoid verbose "static_cast to int" for each
    of attr in the method body.
    5. **Few or trivial tests changes**: No semantic changes or
    optimization.
    
    Includes moves for (they are select in the first half of code for
    `StablehloOps.cc`) : strikethrough means the verifiers are merged into
    corresponding shape functions.
    1. inferAbsOp
    1. inferAllToAllOp
    1. inferBroadcastOp
    1. inferCholeskyOp
    1. inferClampOp
    1. inferComplexOp
    1. inferConstant
    1. inferDynamicSliceOp
    1. inferIsFiniteOp
    1. inferRealOp
    1. inferGetTupleElementOp
    1. inferTupleOp
    1. verifyAllReduceOp
    1. verifyBitcastConvertOp
    1. ~~verifyBroadcastOp~~
    1. verifyBroadcastInDimOp
    1. ~~verifyClampOp~~
    1. verifyCollectivePermuteOp
    1. verifyDynamicBroadcastInDimOp
    1. verifyDynamicReshapeOp
    1. ~~verifyDynamicSliceOp~~
    1. ~~verifyGetTupleElementOp~~
    1. verifyIotaOp
    1. verifyRealDynamicSliceOp
    1. verifyReduceScatter
    1. ~~verifyTupleOp~~
    
    Expect there are ~30 methods remaining to be moved. Why not include them
    all before send this PR out? That will need more time and it's better to
    send PRs early and frequently to avoid code conflict burden.

[33mcommit cba63b4766fa5adb58c20df9b4b29bf1fa9d2005[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Dec 20 20:13:53 2022

    Use AnyType and AnyAttr for all types and attributes in VHLO (#765)
    
    VHLO is intended to be a representation of the IR format, not an exact
    typed dialect, as such use AnyType/AnyAttr everywhere. This change makes
    use of AnyType for all types.
    
    The definition of AnyType can be updated to only include VHLO types,
    `AnyVhloType`, but this isn't necessary and should follow the work
    regarding forked types and attrs (#674).
    
    Could not use `AnyAttr` for `DefaultValued` fields since the constructor
    of the attribute is required to accept the default value, so there are
    some remnants of `DefaultValuedStringAttr`. I also did not update
    `Shape_WitnessType` since that type needs to be forked. That update will
    come in the forking types PR.
    
    Closes #734.
    
    Note this PR is built on top of versioned attributes/types (#736) and
    should be reviewed/submitted after. Can [use this
    diffbase](https://github.com/openxla/stablehlo/pull/765/files/cee7cd6ca7c96525239d51e199a079348054d221..0c0eb99b94bbd1d5883c1fbaeb83617564aa96d9)
    or wait until other is merged to review.

[33mcommit 7f7786c54a3b31943a27c84a8e2f4040fc44bbb6[m
Author: Scott Main <smain@google.com>
Date:   Tue Dec 20 19:31:34 2022

    Various markdown syntax fixes (#793)
    
    This is a first pass at some simple issues found by `markdownlint`, but
    more will come as I gradually enable some of the lint rules that are
    currently disabled. Also, fixing the spec will be a massive change so
    that will be a separate PR perhaps done once the spec is near a more
    final state.
    
    Partially fixes #792

[33mcommit a280bdb3d3436fdc5b63d9e833eb9d141243db72[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Dec 20 15:22:10 2022

    Add versions to attributes, enums, and types (#736)
    
    - Moved `TokenType` to ODS in VHLO to add consistent implementation for
    versioned interface. We can consider moving StableHLO/MHLO TokenType
    decls to ODS as well.
    - Added a `Versioned(Op|Attr|Type)Interface` declaration to version
    ops/attrs/types.
    - Moved `Vhlo(Enums|Types|Attrs).h.inc` to bottom of `VhloOps.cpp` since
    the version interfaces had a dependency on
    `VhloDialect::getCurrentVersion()`.
    - Updated legality checks to include checking attr and type versions
    satisfy the target version.
    - Added test point for legal types/attrs, currently only
    `OutputOperandAlias`, added in `0.4.0` is the only attribute that can be
    tested for illegality.
    
    Currently, there are only `V1` attributes and types. This means we do
    not need to worry about type or attribute converters in the current
    StableHLO release. To avoid introducing technical debt for the sake of
    testing type/attr conversion, I think it is best we submit conversion
    code after we have a use case for it.
    
    I have confirmed that Type and Attribute conversions are possible, and
    have prototyped a few machineries for these conversions. See:
    https://github.com/openxla/stablehlo/commit/334b88ba190fc8127e76e60f2072a7b0f5f2ef2b
    
    Closes #676.

[33mcommit 7c351db936c2c72f1b735de9ab7290bb8993d8d7[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Tue Dec 20 04:48:24 2022

    NFC: Fix spec for stablehlo.case op (#789)
    
    stablehlo.case index input is an operand and not a constant.

[33mcommit ede79fd5c509c4905d9c57d111c56f504a611f6a[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Dec 20 02:46:24 2022

    Integrate LLVM at llvm/llvm-project@02988fce76d8 (#786)

[33mcommit c3fefd5fa932c30f71c9adf206ddd1898941e2f2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Dec 19 20:04:56 2022

    Add spec for ConvolutionOp::window_reversal (#772)
    
    fixes https://github.com/openxla/stablehlo/issues/730
    
    
    [Reference](https://github.com/tensorflow/tensorflow/blob/ffefd39ca3f3ebdab56883dd962c2ba7a2f80eeb/tensorflow/compiler/xla/hlo/evaluator/hlo_evaluator_typed_visitor.h#L1024)

[33mcommit 60bc8a10f67d77c41c041da83cd08b20d4762b4c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Dec 16 21:50:39 2022

    Remove DotOp verifier (#780)
    
    Removes negative DotOp test since the verifier is removed.
    closes #682

[33mcommit 6f65a248fcf627596f230abbb06c669d15839173[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Dec 16 04:23:52 2022

    Add support for f8E4M3FN and f8E5M2 (#774)
    
    The FP8 RFC has been approved
    (https://github.com/openxla/xla/discussions/22), so I'm following up and
    adding support for the proposed FP8 data types to StableHLO.
    
    As discussed in RFC comments, there are other FP8 data types
    (https://github.com/openxla/xla/discussions/22#discussioncomment-4337622),
    and they can be included in StableHLO as well in the future, following
    the same RFC process. Please let us know if you're interested in working
    on this, and we will be happy to help!

[33mcommit a0c8588b641fba2313a24f6f0f7bda15fc81d078[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Fri Dec 16 02:15:17 2022

    Infer bounds for Reduce op (#737)
    
    Extended reduce op shape function to also propagate bounds for
    dimensions that are not reduced.
    
    Also, moved verifyCompatibleShapeWithBounds to public in Base.h
    
    cc @zhouxin913  for review

[33mcommit 60f1c83da2a0292e2bac911fd8346605943d98c4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Dec 16 01:03:36 2022

    Use empty() method to check for emptiness (#776)

[33mcommit 66ff2bf7456bcde4bfecdc45dfa58a84e16fb54e[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 15 05:16:21 2022

    Update type inference doc about regions (#763)
    
    Fix https://github.com/openxla/stablehlo/issues/762
    
    1. Clarify the updated split rules among verifiers and shape function
    2. Clarify how to write / maintain the tests.

[33mcommit 0dc8e2e327dcf0415675a37e75ad511a6ab8362e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Dec 15 05:14:10 2022

    Minor updates to the CustomCallOp spec (#758)
    
    1) Change api_version from enum to integer constant (since we're calling
    it implementation-defined anyway).
    
    2) Change the example from using `%inputs0` to the more conventional
    `%input0`. (Also fix the outfeed example to follow the same convention
    as well).

[33mcommit f6e6730e96a2ec50b22ccdbc8b4b32072171e637[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 15 04:24:43 2022

    Fix crashing builders for CaseOp/IfOp/MapOp (#760)
    
    Shape function can be called directly from autogenerated `build()`
    function, which may not guarantee the added region(s) in
    `odsState.regions` to be non-empty. Need check it here to avoid a crash
    for the ops that need regions in type inference, like
    `IfOp/CaseOp/MapOp`.
    
    An alternative way is to consider disable the autogenerated `build()` at
    all: so far we have no strong evidence that this is a better way and
    have not find a solution to this approach. While, This PR avoids
    surprised crash in dev of these ops and gives the error instead.
    
    Note: in the tests of `ops_stablehlo.mlir`, a ODS-valid op must have at
    least 1 block in each region: so that this error will not trigger in the
    tests.
    
    Doc is updated separately in
    https://github.com/openxla/stablehlo/pull/763

[33mcommit d1b8c4ca4201fba8e4b9fed2cbe8eb26b379f7ea[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 14 23:21:21 2022

    fix ordering of ops in spec (#773)

[33mcommit 252b7d08ec8dccc24e1ebcb5b874ca3b4273c20d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 14 22:59:23 2022

    Integrate LLVM at llvm/llvm-project@339a7687e1c0 (#770)

[33mcommit 4402d37b9f217975819addda2363f54b7cfc98a4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Dec 14 18:37:05 2022

    Introduce inferSendOp (#769)
    
    The implementation is a oneliner, but I think it'll still be good to
    share it with MHLO for consistency reasons.

[33mcommit 2a80139fa592177270b1060b6d10d0e4858b81b6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 14 02:56:52 2022

    Fix the spec of slice op's attribute params (#768)

[33mcommit 6ca337612864a49a1028f095d47b96fa1bc0440c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 14 02:10:21 2022

    Temporarily allow empty `replica_group` for MHLO parity (#767)
    
    [Here](https://github.com/openxla/stablehlo/issues/498#issuecomment-1331396793)
    we decided on StableHLO to dis-allowing empty `replica_groups` but that
    needs some extra work to achieve that.
    
    If we want to share
    [verifyReplicaGroups](https://github.com/openxla/stablehlo/blob/59b68c586ea1381a7cb53e1217904f31755bb390/stablehlo/dialect/TypeInference.cpp#L326)
    logic with MHLO, we cannot just prevent empty `replica_groups` as there
    exists MHLO based tests
    [allowing](https://github.com/tensorflow/tensorflow/blob/1c83485345d1d7ff3e9c3afcce55d303e793e216/tensorflow/compiler/xla/mlir_hlo/mhlo/IR/hlo_ops_common.h#L67)
    that.
    
    The current PR will temporarily preserve the same behavior of MHLO to
    allow code sharing till #498 is fixed. Note the verification for all the
    collective ops are already marked as `revisit`.

[33mcommit 8007c55ac35722f9feff777a8d3bde72072424ab[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Dec 14 02:03:47 2022

    Introduce spec.md (#766)
    
    Earlier today, in conclusion of the Q3/Q4 speccing marathon, we have
    finished speccing HLO semantics for the StableHLO ops.
    
    This was a huge effort that involved writing 93 specs, including digging
    deep into involved semantics of ops like batch_norm_grad, convolution,
    dot_general and more. Congratulations to everyone who contributed to
    this important milestone!
    
    The idea of this project was to create a baseline from which the
    StableHLO opset will evolve in the future. Our immediate next steps will
    be writing a dynamism RFC (#8) and speccing quantization (#588) on top
    of this baseline.
    
    Also, this speccing marathon has uncovered a lot of future work - both
    in cleaning up the opset and improving the implementation to fully
    conform to the spec. This is something that we're aiming to address in
    the next year.

[33mcommit 59b68c586ea1381a7cb53e1217904f31755bb390[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 14 00:53:38 2022

    Add spec for ConvertOp (#421)
    
    closes #418

[33mcommit d607fcba5ca440ae735722be41798a83d326bad3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 14 00:00:48 2022

    Remove unused parameters (#761)

[33mcommit c998e2dd65ff3487603e67a86d7d696f3984adc6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 13 22:11:31 2022

    Minor bookkeeping for recently approved RFCs (#759)
    
    Added "Status", "Initial version", "Last updated" metadata. Also added
    an FP8 RFC which redirects to the OpenXLA RFC to keep track of it.

[33mcommit ea4366435886f64f1680745b4445506d2dd7e4b8[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Dec 13 21:53:35 2022

    StableHLO Compatibility RFC v2 (#115)

[33mcommit 914ccd57b325cb9a894eec861777b8b02ec52897[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Dec 13 21:23:58 2022

    Add spec for CustomCallOp (#636)
    
    closes #518

[33mcommit 66d8481e7e37d32660da7b45a7e5b5b606bb3ea0[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Dec 13 18:48:48 2022

    Add spec for ConvolutionOp (#441)
    
    fixes #359
    
    The PR addresses the followings:
    1. Spec of ConvolutionOp
    2. Clarify the semantics of `precision_config` : The precision_config
    parameter is a array of enums without any constraint on its size. Need
    to resolve this.
    - update: Added constraints on the parameter. With that the verifier is
    in sync with this spec. Also added
    https://github.com/openxla/stablehlo/issues/445 for further exploration.
    4. Fix
    https://github.com/openxla/stablehlo/pull/360#issuecomment-1282808168
    5. Avoid disabling clang formatting in StablehloOps.cpp.
    6. Address https://github.com/openxla/stablehlo/issues/399
    
    Only missing peice:
    
    The constraint between output feature size and input batch size. Working
    on getting a better understanding on this: Done
    
    Type inference should be "revisit" as well because of #600.

[33mcommit a0b804edafccbed7e9e4d7d41d9989c2b3780ae3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 13 07:45:31 2022

    Support dynamic shapes in DynamicUpdateSliceOp's type inference (#757)
    
    https://github.com/openxla/stablehlo/pull/686 implemented type inference
    for DynamicUpdateSliceOp using the currently statically-shaped spec. To
    support the previous dynamically-shaped uses of DynamicUpdateSliceOp,
    this PR adds support for dynamic shapes as well.

[33mcommit 0a6428c544f0a01213cc639e731c0c66962c6416[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Dec 13 07:33:05 2022

    Add spec for DotGeneralOp (#307)
    
    Regarding `status.md`:
    * Verifier is set to `revisit` since the verifier and the XLA semantics
    do not account for `precision_config`.
    
    closes #299

[33mcommit 614c025791fe0dc717f67dd3d71beb05466133b2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Dec 13 00:20:06 2022

    Integrate LLVM at llvm/llvm-project@32cc7d349750 (#747)

[33mcommit c62e71ff65a14af94226bcbd3294548449f51ea7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Dec 12 21:55:05 2022

    Add spec for ReducePrecisionOp (#482)
    
    closes #451

[33mcommit e21a6bece0e2fb471c04d588fc1c393ca5408f3a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Dec 12 07:10:53 2022

    Add spec for CollectivePermuteOp (#568)
    
    fixes #523

[33mcommit 37c14dcd3723f396405c1e80d8af21af3415ae23[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Dec 12 07:09:54 2022

    Add spec for ReduceScatterOp (#564)
    
    fixes #524

[33mcommit 02bc8067a896ce5575e42a9c6bfc51f4eac3ccca[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Dec 12 03:29:40 2022

    Fix typo in the spec of AllReduceOp (#745)
    
    I noticed that something breaks markdown highlighting, and it was this
    typo.

[33mcommit 6ee83d2e55ea158f955be479196a284f7b6e9f3a[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Sat Dec 10 06:30:50 2022

    Split verifiers and shape functions for ops with regions (#401)
    
    This is a pure refactor PR which fixes
    https://github.com/openxla/stablehlo/issues/400 (deprecated) and
    https://github.com/openxla/stablehlo/issues/623 (new)
    
    As in the https://github.com/openxla/stablehlo/issues/623 , "we only
    need this for ops with regions", a full list of ops with region extract
    from
    https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/StablehloOps.td
    includes **11 ops** in total:
    
    | Op | what's done |
    | --- | --- |
    | AllReduceOp | No change (already split) |
    | CaseOp |  No change (region is indispensable for type inference)  |
    | IfOp |  No change (region is indispensable for type inference)  |
    | MapOp |  No change (region is indispensable for type inference)  |
    | ReduceOp |  Split |
    | ReduceScatterOp | No change (Type Inference implementation on hold see
    https://github.com/openxla/stablehlo/issues/725) |
    | ReduceWindowOp |  Split |
    | ScatterOp | No change (already split) |
    | SelectAndScatterOp |  No change (already split) |
    | SortOp |  Split |
    | WhileOp |  Split |
    
    The ideal split is that verifiers contain as almost all verifications,
    and the shape functions are simple as possible, but note:
    1. `IfOp/CaseOp/MapOp`: We need info from region(s) to infer the return
    type, so an init function without regions is always invalid and should
    not exist. No change for them in this PR.
    2. As both verifier & shape function need verification of the
    inputs/attrs, so we need put them in a separate utils functions.
    `ReduceOp`: introduce new util `verifyReduceOpInputsAndInferShape()`
    `ReduceWindow`: introduce new util
    `verifyReduceWindowOpInputsAndInferWindow()`
    In each op, verifier does (1) call this new util function (2) verify
    region
    shape function: (1) call this new util function (2) generate inferred
    type from the intermediate result from (1)
    3. Besides, the verification logic needs further fix see
    https://github.com/openxla/stablehlo/issues/394, but this is out of
    scope of this PR.

[33mcommit 63813b31940e7123c9d070bf542a61a2c901a140[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sat Dec 10 05:21:41 2022

    Prettyprinting for Einsum and UnaryEinsum (#727)
    
    Had this change in an internal repo from awhile ago, but did not submit
    since I was unsure if these would move to CHLO. This change can be
    submitted independently to that work.
    
    ```
    %0 = "stablehlo.einsum"(%arg0, %arg1) { einsum_config = "ab,bc->ac" } : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
    %1 = "stablehlo.unary_einsum"(%arg0) { einsum_config = "ab->a" } : (tensor<8x16xf32>) -> tensor<8xf32>
    
    -->
    
    %0 = stablehlo.einsum %arg0, %arg1, config = "ab,bc->ac" : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
    %1 = stablehlo.unary_einsum %arg0, config = "ab->a" : (tensor<8x16xf32>) -> tensor<8xf32>
    ```

[33mcommit 570b11208cfe31d50992478d504abd74ddba0e15[m
Author: Scott Main <smain@google.com>
Date:   Fri Dec 9 23:10:04 2022

    Change some MD syntax for compatibility with Python Markdown (#729)
    
    This has no effect on the appearance on GitHub but fixes some things for
    rendering with other Markdown processors, particularly Python Markdown,
    which we will probably use for the OpenXLA website. (I'll squash the two
    changes when merging.)

[33mcommit e35ff33081c95e468e9ae7e02e78005660309813[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Fri Dec 9 04:51:06 2022

    Integrate LLVM at llvm/llvm-project@b2505ca2ece3 (#728)

[33mcommit 917f486b7c3db3727d78f18b931bfd647541ea5d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Dec 8 22:02:23 2022

    Update type inference status for CrossReplicaSumOp (#726)
    
    The type inference already exists under
    [StablehloOps.cpp](https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/StablehloOps.cpp#L302),
    so this PR only updates the status.md to reflect that, marking it as
    `yes*` since we do not have a spec for it yet.
    closes #701

[33mcommit 319edfe59d39461b6dda664d17f36eb7915d0a92[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Dec 8 20:18:34 2022

    Add support for PartitionIdOp (#673)
    
    closes #632

[33mcommit 29b16547e34ffb119582269a7423be4d44c8e904[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 8 18:49:22 2022

    Implement VHLO Bytecode Interface (#717)
    
    Closes #675.
    
    Favoring code duplication given the nature of the dialects, that they
    will diverge over time as any differences are introduced. StableHLO
    serialization only needs to support reading the latest version of the
    dialect. VHLO needs to support reading all versions of attributes and
    types. Also the only good way to share this code is via heavily
    templated methods.

[33mcommit e11dc6d215e040fd2bface10617babc3f6f863db[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Dec 8 18:48:42 2022

    Sever the dependency from TypeInference.h to StablehloOps.h (#721)
    
    I missed something important when reviewing #711.
    
    We cannot have a dependency from TypeInference.h to StablehloOps.h. It
    should be the other way around - both StablehloOps.h and hlo_ops.h
    depending on TypeInference.h
    
    This PR severs the dependency by introducing an interface that abstracts
    away creating token types.

[33mcommit 9ebc9c55e559c85dcfb6da4de2333dd7bb0d9490[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 8 18:41:43 2022

    Add type inference for stablehlo.selectandscatter (#405)
    
    Like https://github.com/openxla/stablehlo/pull/404 This RP does not
    touch verify() or verifications:
    https://github.com/openxla/stablehlo/pull/402 decides that we handle
    shape function as simple as possible without verifications, thus the
    verify() will be handled separately.
    
    inferReturnTypes() interface is used as SelectAndScatterOp is in the
    list of bounded dynamism plan and this interface is the only choose we
    have: see detail in description of
    https://github.com/openxla/stablehlo/pull/375

[33mcommit a064a4d81c3842b5e32ef245a7c7d89329970727[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Dec 8 17:13:27 2022

    Add type inference for stablehlo.scatter (#404)
    
    This RP does not touch verify() or verifications:
    https://github.com/openxla/stablehlo/pull/402 decides that we handle
    shape function as simple as possible without verifications, thus the
    verify() will be handled separately.
    
    `inferReturnTypes()` interface is used as ScatterOp is in the list of
    bounded dynamism plan and this interface is the only choose we have: see
    detail in description of https://github.com/openxla/stablehlo/pull/375

[33mcommit 0cdff3842a5fa2349bb4971353a70c18056be650[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Dec 8 04:21:38 2022

    Add type inference for GetDimensionSizeOp (#718)
    
    closes #700

[33mcommit 590b1106509237b7091bec86f8d8e999f6a5a752[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 8 04:21:00 2022

    Add bytecode implementation for OutputOperandAlias (#716)
    
    Based on MHLO implementation in:
    https://github.com/tensorflow/mlir-hlo/commit/984b124ecf96d3c0d1d447ffb70c1baca3d9868c

[33mcommit 71d12233b7093c8938ebdbd20c6b00ecd9ac4b7c[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Thu Dec 8 02:38:28 2022

    Minor change in build/readme, add git fetch instruction to llvm build command. (#720)

[33mcommit 01bbf2ae747f0ab95347d368368b44fefd34fb08[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 7 23:59:03 2022

    Integrate LLVM at llvm/llvm-project@279d294d26c3 (#715)

[33mcommit 4390409e3384e104da792af97ddbb2ec9c7da815[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 7 23:05:13 2022

    Add spec for SelectAndScatterOp (#376)
    
    fixes #302
    
    Verification is "revisit" because of #369.
    Type inference is `no` based on
    https://github.com/openxla/stablehlo/issues/212

[33mcommit d545ee792219d3d3056b6b43cce0fd9ac71971e3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 7 22:08:14 2022

    Add type inference for OutfeedOp (#713)
    
    In addition, it updates the semantics of OutfeedOp and SendOp to
    explicitly mention that the 'result' is a token type.
    Also removes adaptors from CreateTokenOp and AfterAllOp that are not
    used.
    closes #699

[33mcommit 4ec8209ec79b6fd6c4f0541309cd9fe2e046b72b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 7 19:52:04 2022

    Add type inference for AfterAllOp (#708)
    
    closes #697

[33mcommit 43437c781e18e4ec8f5f56bf49adc68c26a562ab[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 7 19:21:28 2022

    Add type inference for CreateTokenOp (#711)
    
    closes #698

[33mcommit a05b8c0a5375063983888ceb1038e5856ff56018[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 7 18:32:23 2022

    Preparation change for MLIR to only produce prefixed accessor. (#712)
    
    fixes https://github.com/openxla/stablehlo/issues/709

[33mcommit ff55f9346d54e9e38de807a79f8ae03faffda274[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Dec 7 02:56:26 2022

    Add type inference for DynamicUpdateSliceOp (#686)
    
    This PR deletes current `verify` and merge with the new
    `inferReturnTypeComponents()` function.
    Added additional verifications to cover (C3), (C4), (C6) from the spec.
    Added additional tests to cover all constraints.
    closes #656

[33mcommit 15295fbf6c3640c7ee909f291850f8df3d809759[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Dec 7 02:34:06 2022

    fix clang-tidy suggestions (#707)
    
    The fixes are suggested by internal presubmits.

[33mcommit 284b4a3aadb20582d07267c447b8e3ba7141684a[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Tue Dec 6 18:23:44 2022

    Verify bounds of ranked tensors (#696)
    
    Implements TODO to add verification for bounds.
    
    This is in preparation for moving bounds out of experimental as part of
    https://github.com/openxla/stablehlo/pull/194
    
    I considered doing this in the RFC itself but better to do the code
    change separately and immediately given that these invariants are
    already assumed during type inference.

[33mcommit 3ad306bad6a4ced1295ca3b781052326ad3167c5[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Dec 6 17:23:09 2022

    Add spec for ReduceWindowOp (#354)
    
    fixes #301
    
    Verification is "revisit" because of #369.

[33mcommit d3fe9922da99211a005a19ba8b3efa5104725701[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 6 16:08:25 2022

    Implement type inference for ReturnOp (#691)
    
    Cannot write a FileCheck-based test because ReturnOp doesn't have result
    values.

[33mcommit 5f191579fee7245836bcc6a72302236e8f000535[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 6 07:49:46 2022

    Improve type inference for SelectOp (#692)
    
    The implementation says "the output shape should be the most general of
    the operand shapes at each dimensions", but falls a bit short of
    delivering that. This PR fixes it.
    
    Also merges the verifier into the shape function, to follow the design
    pattern that we've established for type inference, and switches to
    `inferMostSpecificType` which is used for the same purpose in several
    other ops.

[33mcommit 8b4db62ffff25cc0feb72f2eae1e9c5944388b8c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Dec 6 05:02:29 2022

    Integrate LLVM at llvm/llvm-project@f9048cc1310a (#702)

[33mcommit 2f60538447726dc90dfc201957e43afb20c07071[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Dec 6 05:02:10 2022

    Fix broken HEAD (#705)
    
    PR #672 was based off HEAD which didn't yet have PR #510, and the former
    PR happened to make an incompatible change to an API
    (verifyReducerShape) used by the latter PR.
    
    This PR fixes the incompatibility and fixes HEAD.

[33mcommit 003a651820a240ca09b8475c6e4c411273093cad[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Tue Dec 6 03:49:47 2022

    Fix verifier check disallowing dynamic shapes in reduce and related ops (#671)
    
    In the verification of the reducer function, while verifying that the
    block argument shape is a sub-sequence of the computed output shape, it
    doesn't consider dynamic shapes.
    
    The original intention here might have been to check that
    `allowedDimensions[argShapeIdx]` was dynamic and not
    `argShape[argShapeIdx]`. Keeping the original check for now but I wonder
    if the op should simply disallow dynamic block arguments. Those are not
    going to be refined based on the input shapes. If so, such a check could
    be added later on.

[33mcommit ecf64636929b835d43c998007eac234723fa0753[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Tue Dec 6 03:09:39 2022

    NFC: Remove unnecessary argument  in reducer function verification (#672)
    
    This output argument is only used by the reduce op verification and
    there also it is only using the element type. But, the function already
    verifies that the element types match the input types so we could
    directly use those and remove the extra output argument.

[33mcommit ce4b89e796e44685892c67d0c3f0e6243505bd13[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Dec 5 19:18:08 2022

    Fix verification of DotGeneralOp (#693)
    
    Batching dimensions of lhs/rhs must match (same for contracting
    dimensions), but that check should account for dynamic dimension sizes.

[33mcommit 444f6f269136a5c493713e77138a47b271839351[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Dec 3 23:16:57 2022

    Fix typos in AllGatherOp's status (#688)
    
    Type Inference is set to "no", because "revisit" implies that there is
    an implementation that needs to be revisited.

[33mcommit ca1227aa2175f592ed4e0b1d17fda4d1e0aafbe9[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Dec 3 23:15:28 2022

    Move "Streaming communication" next to "P2P communication" (#689)
    
    They are related to each other, and "Streaming communication" even calls
    out similarities and differences with "P2P communication". I think it
    would be good to have them next to each other.

[33mcommit 0c24fb7965c44eccca2e5a7e418a3e259cca963d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Dec 3 03:05:24 2022

    Fix typo in DynamicUpdateSliceOp's constraint (#687)
    
    The dimension sizes of the `update` tensor be equal to the dimension
    sizes of the `operand` tensor.

[33mcommit 3321245037c199b29f0470bd7a19d215c921d17a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Dec 3 02:30:49 2022

    Add spec for AllReduceOp (#510)
    
    fixes #504
    
    Address the followings:
    1. Adds verification checks for AllGather w.r.t.
    https://github.com/openxla/stablehlo/issues/498
    2. Fixes https://github.com/openxla/stablehlo/issues/508

[33mcommit 9c81a9b48932864c2fa87f2b044f311685e5b478[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Dec 3 01:23:01 2022

    Minor fixes to recently merged specs (#684)
    
    This is the feedback that I noticed in final stages of PR review but
    didn't want to introduce extra back and forth rounds of reviews for.

[33mcommit 4fb042e90b3b72d65ca247a44cea04b0a0905386[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Dec 3 01:15:17 2022

    Add spec for InfeedOp and OutfeedOp (#640)
    
    fixes #525
    
    verification of infeed op is revisit based on
    https://github.com/openxla/stablehlo/issues/639
    
    Note that for outfeed op we are using the output name as `results` based
    on the implicit name [introduced by the tablegen
    file](https://github.com/openxla/stablehlo/blob/dbf9964c6ce980e1976041700807bd94e6adf849/stablehlo/dialect/StablehloOps.td#L1030
    ) even though it should be explicitly called out as `result` (singular).
    This will be addressed in
    https://github.com/openxla/stablehlo/issues/351#issuecomment-1326914173,
    item 4.

[33mcommit 1f2aff3b6586a60a6955db6f759079eb769acf70[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sat Dec 3 00:17:45 2022

    Add CI lint for invalid whitespace in files. Fix existing issues. (#681)
    
    ```
    # Errors on whitespace issues in '*.{h,cpp,td,txt,mlir,yml}' files
    $ lint_whitespace_checks [flags]
       -f  Auto-fix whitespace issues
    ```
    
    Perform the following whitespace checks:
    - No trailing whitespace (clang-format covers this for `h/cpp`, but not
    `md/yml/td/txt/mlir`)
    - Files must end in EOL (many editors enforce this, but not
    clang-format/tidy)
    
    Can auto-fix issues using the `-f` flag.
    
    Example of failure message when I added whitespace and removed EOL from
    end of file:
    
    https://github.com/GleasonK/stablehlo/actions/runs/3605629099/jobs/6076206738

[33mcommit f2440c01fece5a5f9fff54cfca6ebb8f462a0e27[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sat Dec 3 00:01:40 2022

    StableHLO Versioned Dialect and Compatibility Passes (#278)
    
    Add StableHLO compatibility dialect and passes for reading and writing with forward/backward compatibility guarantees.
    
    **Note: This is still a prototype implementation and should not be used in production until RFCs have been approved and types have been forked.**
    
    ```bash
    stablehlo-opt [flags] file.mlir
        --stablehlo-legalize-to-vhlo
        --vhlo-to-version='target=[version]'    Translate versioned dialect to target version (current, 0.3.0, ...)
        --vhlo-legalize-to-stablehlo
    ```
    
    Change description:
    - Introduce VHLO, the Versioned StableHLO Dialect.
      + This dialect is a shallow copy of StableHLO's in-memory layout. It does not include verifiers or constraints.
      + Once an op is added to VHLO it must remain unchanged so that it can be guaranteed that a VHLO op is identical across versions.
      + The first version of VHLO is `0.3.0`.
    - Conversion passes for compatibility
      + StableHLO <--> VHLO legalizations. StableHLO is always able to be legalized to/from the latest version of VHLO.
      + VHLO-to-version. Target previous versions of VHLO ops for forward compatibility. Upgrade to the latest version of VHLO ops to emit StableHLO.
    - Testing for legalizations and version conversions.
    
    Future work (these items will be made into individual GH issues before submit):
    - Think more about a scalable way to test this as StableHLO evolves.
    - Additional feature work on the tool.. any missing flags? Pass pipeline for simplicity?
    - Improve user experience.
    - See open [compatibility issues](https://github.com/openxla/stablehlo/labels/Compatibility)
    
    Closes #255

[33mcommit 8a2ce229d15d5d5cc649b0ec462a5e327c91126a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Dec 2 23:39:36 2022

    Add spec for SendOp and RecvOp (#580)
    
    fixes #527
    
    verification is revisit on
    https://github.com/openxla/stablehlo/issues/579 and
    https://github.com/openxla/stablehlo/issues/667

[33mcommit 8f4dfc29bc69b73a35e0e0fda8ab2a6eb494eaca[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Dec 2 02:16:42 2022

    Fix header guards (#668)

[33mcommit 85a7efd7cdf0b9aea56cebcf37709aacda846802[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Dec 2 00:24:13 2022

    Update CMakeLists with new dependencies from #569 (#665)

[33mcommit dc75efccf3e14a3988288fb33c01bd4113efa736[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 1 19:09:11 2022

    Integrate LLVM at llvm/llvm-project@61aed52c9ec0 (#663)

[33mcommit 84cf1646a667b9e44e667f26f2a992d10dfa6aa7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Dec 1 18:24:28 2022

    reorder AllGather and AllToAll Ops (#664)
    
    fixing the order of Ops.

[33mcommit 10111159dfac2572cbe933fceb75fa891e98f83c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Dec 1 18:13:47 2022

    Change -1 to ? printing when possible (#569)
    
    Prepare for dynamic dim size to be changed from -1 to INTMIN.
    
    This is a first step at removing the reliance on upstream
    `kDynamicSize`. There is more refactoring that can be done, but this
    change will at least make it so that some of our error messages will
    display `?` instead of the value, and our infer tests now print "?" as a string for consistency.

[33mcommit 707e1728bd925e57e02dc8ac66deeb3c43fd7d2b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Dec 1 04:24:46 2022

    fix size constraint on replica_groups for AllGatherOp (#662)
    
    Fixed the fact that size(`replica_groups`) is not always `num_replicas`
    and depends on the process group formation strategy.
    
    Added an action item to #498 to verify these facts.

[33mcommit 008d66d366a49912828099d71da48264aa3aaa87[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Dec 1 04:23:44 2022

    Add spec for OptimizationBarrierOp (#575)

[33mcommit 29594ce33ab8d6cc773d84f25ca0cc255ece4252[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Dec 1 02:42:18 2022

    Add spec for RngBitGeneratorOp (#415)
    
    Raised issue #643 to track which types are supported for the
    inputs/outputs.
    Verification is set to `revisit` due to #486.
    closes #414

[33mcommit a7904c40f6d198f8151ac4bdb739560e71bf4c2b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Dec 1 01:10:19 2022

    Add spec for AllToAllOp (#555)
    
    fixes #511
    
    Address the followings:
    1. fixes https://github.com/openxla/stablehlo/issues/498
    2. fixes #550
    
    Note that HLO verifier does not have `split_count` parameter. Instead it
    derives the value from subgroup size of `replica_groups`.
    [link](https://github.com/tensorflow/tensorflow/blob/49959d2f7d5e497edf90d6cdebc9afa3247234b7/tensorflow/compiler/xla/service/hlo_verifier.cc#L560).
    Also from [hlo to mhlo
    importer](https://github.com/tensorflow/tensorflow/blob/49959d2f7d5e497edf90d6cdebc9afa3247234b7/tensorflow/compiler/xla/translate/hlo_to_mhlo/hlo_function_importer.cc#L1326),
    we can see that the `split_count` in MHLO is created based on the
    subgroup size of AllToAll HLO instruction.
    
    For StableHLO it makes sense to add a constraint on the subgroup size
    (i.e.e it is equal to `split_count`).

[33mcommit 84ea260778e46d929cc6adbe32883ea0b89da1d6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 30 23:56:55 2022

    Add spec for AllGatherOp (#503)
    
    fixes #462
    
    Address the followings:
    1. Adds verification checks for AllGather w.r.t.
    https://github.com/openxla/stablehlo/issues/498
    2. fixes https://github.com/openxla/stablehlo/issues/491
    
    A few points
    - Type Inference is marked `infeasible` as the return type of the op
    depends upon the
    [shard_count](https://github.com/tensorflow/tensorflow/blob/20c6943d3cd7e07da162f7778a0af5d3776274b4/tensorflow/compiler/xla/service/hlo_verifier.cc#L452)
    which [depends on the result
    type](https://github.com/tensorflow/tensorflow/blob/20c6943d3cd7e07da162f7778a0af5d3776274b4/tensorflow/compiler/xla/service/hlo_verifier.cc#L426).
    Note that the `shard_count` is a parameter in HLO spec, where as in MHLO
    it is
    [derived](https://github.com/tensorflow/tensorflow/blob/a1acd6a6466f58ed4197b7beaa2f3e0b6fcfc32a/tensorflow/compiler/xla/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc#L766)
    using result type before exporting to HLO.
    - With (1) and (2), the Verifier is a `yes`. Note that we still do not
    have the
    [check](https://github.com/tensorflow/tensorflow/blob/20c6943d3cd7e07da162f7778a0af5d3776274b4/tensorflow/compiler/xla/service/hlo_verifier.cc#L436)
    for `shard_count == subgroup_size`. The `subgroup_size` depends on
    [module
    configuration](https://github.com/tensorflow/tensorflow/blob/20c6943d3cd7e07da162f7778a0af5d3776274b4/tensorflow/compiler/xla/service/hlo_verifier.cc#L95)
    which manages the settings and values which affect the compiled
    executable outside of the HLO code itself and I am not sure if that info
    is available in StableHLO IR.
    
    upd:
    - With https://github.com/openxla/stablehlo/issues/650, the type
    inference should be made feasible. Marked it `revisit` until that is
    fixed.
    - The verifier should be `revisit` based on
    https://github.com/openxla/stablehlo/issues/652

[33mcommit b7bdf932a8cf7a224299b4c39a1bb9f990ccaec0[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 30 22:47:03 2022

    Add headers to parallel execution semantics (#659)
    
    Added headers to the process_group paragraphs in parallel execution
    semantics, so they can be linked from other parts of the spec.

[33mcommit 0ddf1f9edc372a5ac9ae4dae6d4e53df9c1d31d4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 30 21:34:41 2022

    Add spec for DynamicUpdateSliceOp (#553)
    
    Verification is left `revisit` due to #554.
    Type Inference is left `no` since no type inference exists currently and
    added issue #656.
    Updates spec of ScatterOp to mirror this op.
    closes #520

[33mcommit 7f18330d1e2a31ad91fd8423d92066aaafc583c7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 30 21:05:28 2022

    Add spec for DynamicSliceOp (#556)
    
    Verification is set to `revisit` due to #557
    Updates semantics of SliceOp to match DynamicSliceOp
    closes #519

[33mcommit 58553fbfb822296382e31717daaee031f231ea57[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 30 19:52:01 2022

    Add spec for ReplicaIdOp (#576)
    
    closes #529

[33mcommit 336eac4526f90e787c119fb47f3522fe5c40f701[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 30 02:15:25 2022

    Add spec for parallel execution semantics (#634)
    
    I think that recent PRs with specs for collective ops (e.g. #503) would
    benefit from a more formal definition of parallel execution semantics of
    StableHLO programs.
    
    That will provide additional clarity which is needed at the moment (see
    discussions in the PRs) and reduce the duplication between the specs of
    individual collective ops.
    
    The description of execution semantics isn't yet as formal as I'd like,
    but it can already benefit specs for collective ops. I'll leave the rest
    to follow-up work within #484.

[33mcommit c7d8fd81b84cbc5fd5aa958b4317db4ae72e3a5e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Nov 29 22:20:50 2022

    Fix typos in the PowOp spec (#645)

[33mcommit c4fb013a7ec7570c617ea45da14cf3141157be6e[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Nov 29 22:13:50 2022

    Add spec for BatchNormGrad (#416)
    
    Closes #345

[33mcommit e74eaab98185afa2c5d42aff990414a0f7dde9f0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 29 21:21:25 2022

    Add spec for SignOp (#494)
    
    closes #480

[33mcommit 582ff33a18109e514290db3bd48659793d08ed1b[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 29 20:06:36 2022

    Fix merge conflict on status.md (#651)

[33mcommit 47f52f64b2de7b5520f8f39830736016f5c9fdff[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Nov 29 19:30:55 2022

    Slight rewording of the BitcastConvertOp spec (#646)
    
    I merged #434 and only then realized that the explanation of why `bits`
    has implementation-defined behavior could've been updated to reflect the
    latest conversation on the pull request.
    
    It's not just the floating-point representation is
    implementation-defined, it's also integer representation and boolean
    representation.

[33mcommit 18b669751b863252196fa59ae8988b058c73f787[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 29 04:18:41 2022

    Add spec for BitcastConvertOp (#434)
    
    closes #417

[33mcommit 32cd1de238bbf5202270f744b964d12538ef90b4[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Nov 29 03:56:19 2022

    Add spec for CompareOp (#429)
    
    closes #419
    Ticket for verification #571

[33mcommit f05f39cd59aa1232153312069fe35109b225c5df[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Nov 29 02:23:36 2022

    Add spec for PowOp (#505)
    
    closes #473

[33mcommit 73aeefbe8704db5b3b6945eece2c9e2f6c4cd4ac[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Nov 29 02:06:59 2022

    Integrate LLVM at llvm/llvm-project@f51170bffd50 (#638)

[33mcommit dbf9964c6ce980e1976041700807bd94e6adf849[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Nov 26 16:51:53 2022

    Fix typos in step names of GitHub workflows (#578)
    
    Unless I'm missing something, these look like typos.

[33mcommit e8b8de42d92c753bc060ac7edd070fa5fea1a9a5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Nov 26 06:58:08 2022

    Add spec for RoundOp (#558)
    
    Updates the description of ceil and floor op to call out IEEE-754
    operation names.
    closes #475

[33mcommit e17a9711f54b0f6f019f7adcabc34997fc0896e6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sat Nov 26 02:50:26 2022

    Expand the checklist to include HloInstruction::CreateFromProto (#625)
    
    This is really useful (see #565 for details).
    
    Also remove the "Structure of an Op‚Äôs Specification" section of the spec
    since it has been superseded by the checklist a while ago.

[33mcommit 5e4d1d42952d6f6fbd622167fff58a64aa37bbfd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Nov 24 01:14:42 2022

    Add spec for CbrtOp (#495)
    
    closes #466

[33mcommit dbdf0c5f92b3b980c18878801bf07458246ea51d[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 23 02:32:17 2022

    Add spec for ClampOp (#460)
    
    closes #452

[33mcommit f9d70d14bc59bfa1aa746f80f3c968adcf1854b2[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 23 01:07:42 2022

    Add spec for TupleOp (#562)
    
    closes #516

[33mcommit 955da418fc655a7e4b5cede01d773bee00ecf7ce[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 23 00:57:48 2022

    Add spec for GetTupleElementOp (#563)
    
    closes #517

[33mcommit 78da5ee2d1ac29036a9118b491568c1a7b3133a0[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 23 00:24:29 2022

    Add spec for ShiftLeftOp, ShiftRightArithmeticOp and ShiftRightLogicalOp (#496)
    
    closes #477
    closes #479
    closes #478

[33mcommit efbd99463f386acc40887c59140e71a5419481d8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Nov 22 19:24:22 2022

    Integrate LLVM at llvm/llvm-project@163bb6d64e5f (#577)
    
    In this PR, I used the easiest fixes to make code compile and tests pass
    leaving long-term fixes to #569.

[33mcommit 51c09e6f36b6601982865453578ca916a8d53f83[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 22 19:08:50 2022

    Add spec for RoundNearestEvenOp (#552)
    
    closes #476

[33mcommit c9772039b38aaf63f10f5c0338434a095a9fd2dd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 22 19:07:57 2022

    Add spec for IsFiniteOp (#501)
    
    closes #471

[33mcommit 522ccbd6bfd754652543d86c16b57ac5d24eff69[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Nov 21 15:49:10 2022

    update spec checklist: Adding ticket link in PR (#570)
    
    For tickets like https://github.com/openxla/stablehlo/issues/369 which
    is applicable to many PRs and prevents the status of one or more
    implementations from being a `yes`, it would be useful to track all the
    Ops which needs to be tracked before closing this issue. Linking the
    ticket in the spec could be a solution and hence the proposal.

[33mcommit 99fd437e8b8c79cbd93cb541401df90354b14b15[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Mon Nov 21 05:03:20 2022

    Add spec for Logp1Op (#507)
    
    closes #472

[33mcommit 5d5d317ebd1fa1ee76e4f5d5393e2a4864efe505[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Nov 21 05:00:19 2022

    Add spec for Atan2Op (#500)
    
    closes #465

[33mcommit 5b94ed2b123d287692ca67ec3ca1b558453d08da[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 18 01:12:12 2022

    Integrate LLVM at llvm/llvm-project@750d17bb72ad (#567)

[33mcommit f2b9975830b49021e272baaaf941e92c2b299656[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Nov 17 19:02:29 2022

    Migrate MHLO changes for printing dimensions (#566)
    
    - Migrate unknown dimension printing `printDimensionSizes`.
    - Move `parseIntArray` to `AssemblyFormat.h` to be shared.
    
    This unblocks work to make a few other attributes declarative. Also
    opens the door to make some printing infrastructure communal between
    MHLO/StableHLO. Will look into removing `parseDims` today.

[33mcommit 4aff405bc1936e4121d25d2cd674166275f9c4e1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 16 04:58:30 2022

    Improve CHLO code coverage for verifiers, type inference, reify (#551)
    
    Decided to take a little time to focus on code coverage. Specifically
    focused on `ChloOps.cpp`.
    
    `ChloOps.cpp` coverage improvements:
    - Before: [55% line, 39% func
    coverage](https://htmlpreview.github.io/?https://raw.githubusercontent.com/GleasonK/stablehlo/ccov/reports/ccov_2022_11_15_16-34-38/stablehlo/dialect/ChloOps.cpp.gcov.html)
    - After: [91% line, 77% func coverage](
    https://htmlpreview.github.io/?https://raw.githubusercontent.com/GleasonK/stablehlo/ccov-test/reports/ccov_2022_11_15_19-08-26/stablehlo/dialect/ChloOps.cpp.gcov.html)
    
    This brings our entire repo code coverage from `84.6% -> 86.5%`. We have
    a goal of hitting `90%` in Q4.

[33mcommit 9da3674c5257c68bf156ded622a1d3ebfac5d29a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 16 04:53:38 2022

    Add spec for ScatterOp (#330)
    
    fixes #246

[33mcommit 2b223de950ce9e73b4710d048bdeddd3b636154f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 16 04:43:39 2022

    Add spec for TriangularSolveOp (#410)
    
    Verification is set to revisit due to #534
    closes #358

[33mcommit e3bd86f51f3a4b9eac123e691be75058e2838922[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 16 04:29:00 2022

    Delete redundant newlines from gather's spec (#559)

[33mcommit 0634713b10e6c9cfb84ea0ad7366f663be703ce2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 16 04:22:05 2022

    add spec for AfterAllOp (#461)
    
    fixes #443
    
    The shape inference is not implemented : Hence `no` in its status.

[33mcommit 7123b6716957496d3f5349e56cf4f1eb4c2e827d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 16 04:14:13 2022

    Add spec for Expm1Op (#502)
    
    closes #469

[33mcommit c9065102b27ee7508b587a9c764e039a2a61c0fc[m
Author: Anish Tondwalkar <atondwal@google.com>
Date:   Wed Nov 16 03:51:18 2022

    Add parser for StableHLO_Dim (#546)
    
    Pulling in patch from tensorflow/third_party/stablehlo, which is needed
    for stablehlo-to-mhlo roundtrip littests

[33mcommit 6614a4522eb0c6a66f555a10edafa0a7e35749bc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Nov 16 03:50:47 2022

    renaming interpreter.md (#549)
    
    fixes https://github.com/openxla/stablehlo/issues/514

[33mcommit 2813676bbbc8ce714babeef12791abdfc7f8ffba[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Nov 15 03:09:07 2022

    fix gather spec (#548)
    
    The changes in the PR is suggested as part of reviewing ScatterOp spec
    https://github.com/openxla/stablehlo/pull/330#discussion_r1020955043.
    
    The changes include
     - The chances as suggested in the above link.
    - Drop constraint (C13) which can be derived from (C14) motivated by
    simialr change in ScatterOp
    https://github.com/openxla/stablehlo/pull/330#discussion_r1020951558.
    Another motivation for the removal is: The result rank formula is never
    a constraint to be verified.
    https://github.com/openxla/stablehlo/blob/7ff29f02da6556204ea3da64739cf59d2642e05d/stablehlo/dialect/StablehloOps.cpp#L1184,
    but used
    
    - Update the diagram as per
    https://github.com/openxla/stablehlo/pull/330#discussion_r1020948455
     -  Update the diagram: Drop the derivation of result rank.

[33mcommit 7ff29f02da6556204ea3da64739cf59d2642e05d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Nov 14 21:17:34 2022

    Integrate LLVM at llvm/llvm-project@0b94525ddcfc (#515)

[33mcommit 48290cacb2b4a40c4ba5fac3fb572510ff034afa[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 14 21:16:06 2022

    Revert #457 embedded build change (#545)
    
    Revert the bit of #457 that caused build failures in mlir-hlo.
    
    Created #544 to build embedded properly with warnings as errors enabled.

[33mcommit a74123319538adfbc33198147b3b0c30c3ad9c2d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Nov 12 20:33:29 2022

    Add spec for MapOp (#450)
    
    Update semantics of ReduceOp to align with MapOp semantics.
    closes #437

[33mcommit e9cacdccbb73b8b03b4de6e8be110eec94dd3d31[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Nov 11 02:13:44 2022

    Add spec for RealOp and ImagOp (#489)
    
    Closes #474
    Closes #470

[33mcommit e617057f5dd7726cdbd07292ccf944dd8836dfca[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Nov 11 02:05:15 2022

    Add spec for ComplexOp (#493)
    
    closes #468

[33mcommit 283702a1378d4319b0ac551c3b4502a8e22bc386[m
Author: Anish Tondwalkar <twilightrook@gmail.com>
Date:   Wed Nov 9 20:28:21 2022

     Integrate LLVM at llvm/llvm-project@625796f408cc7 (#490)

[33mcommit 5d6a76e7629cb74b97f6699e82a7d4fb3a4322a1[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 9 01:13:09 2022

    Add input output aliasing to CustomCallOp (#403)
    
    Closes #235

[33mcommit 8034c8abf137f1868526f646874eb8e72b6bc155[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 9 01:12:21 2022

    Build StableHLO as a part of LLVM in CI (#457)
    
    A suggestion made in #444, build StableHLO as a part of LLVM in CI. Many
    clients do this so we should make sure the build is clean.
    
    Other minor cleanup: Remove matrix, we only run tests on ubuntu
    currently. No need for a matrix, no need to build for anything except
    host.

[33mcommit 139bf690ba4c0d7e3ab8aa83c862adcf9320022c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 9 00:33:00 2022

    Add spec for RngOp (#413)
    
    closes #412

[33mcommit 0218443f7de91838c8531f69ac9104135cc73976[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Nov 9 00:17:54 2022

    Add spec for BatchNormTrainingOp (#398)
    
    closes #346

[33mcommit 2988c0f5ed660d5007b8a780525bbda94c6ffb18[m
Author: Anish Tondwalkar <twilightrook@gmail.com>
Date:   Tue Nov 8 02:36:02 2022

    Add SparseTensorDialect to  registration (#483)
    
    fixes #296

[33mcommit 4eb136437a5c90c14ad5440c1d2df7d0da310ebc[m
Author: Anish Tondwalkar <twilightrook@gmail.com>
Date:   Tue Nov 8 02:30:28 2022

     Integrate LLVM at llvm/llvm-project@6c6dff7e2c27c (#464)

[33mcommit 618d84d654fd2965f87a81540a6387f40b0694a5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Nov 7 23:05:53 2022

    Fix discrepancy between names in Outputs and in the ODS (#463)
    
    Thank you, Sandeep, for discovering and documenting this in #351!

[33mcommit a095c07b39acf6269116f384236bae416e56aa4f[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Nov 7 22:28:20 2022

    (abs..constant) Align examples in spec/ODS with spec_checklist.md (#453)
    
    In #439, we documented the conventions for examples in spec/ODS, and I
    went to check how closely the previous work on the spec follows these
    conventions.
    
    I discovered quite a few divergencies, so I started going through the
    spec fixing things. This PR covers 10 out of the currently specced 43
    ops.

[33mcommit 3bdbefe42b9f7836c1c13667f95d220b2ad900fd[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 7 19:31:25 2022

    Use new github actions output environment files (#459)
    
    GitHub Actions is deprecating save-state and set-output commands, need
    to update to use new Environment Files:
    
    https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/
    
    I made a PR on my forked repo to test, and it had the following in the
    logs:
    
    ```
    Run ./.github/actions/setup-build
      with:
        llvm-version: d4c61314420ca8794ad6d2588566c64109482505
      env:
        LLVM_PROJECT_DIR: llvm-project
        LLVM_BUILD_DIR: llvm-build
        STABLEHLO_BUILD_DIR: stablehlo-build
        STABLEHLO_PYTHON_BUILD_DIR: stablehlo-python-build
    ```
    
    This confirms that the new syntax is working. The `llvm-version` is set
    by the output of the previous command.

[33mcommit cabc2a092d531322b8176dfe0d6d37eba31783b5[m
Author: Chengji Yao <yaochengji@hotmail.com>
Date:   Mon Nov 7 19:20:11 2022

    Fix the LINK_LIBS of StablehloTypeInference (#456)
    
    Found it in an [onnx-mlir
    pr](https://github.com/onnx/onnx-mlir/pull/1829)

[33mcommit 91ac9a0d4e44d0ac619a60e4ff393a8bd0d82b92[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Nov 7 18:35:55 2022

    Fix pedantic build warnings in ChloOps (#455)
    
    Closes #444.

[33mcommit 03cb05f4dc97d29c2e4d0c1f30cec3fbaef7f493[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Nov 5 07:26:52 2022

    Add spec for ClzOp (#449)
    
    closes #448

[33mcommit 5aa50c5674299df585c1cc730f1d3f25f66c22d3[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Nov 5 05:20:14 2022

    Add spec for ReduceOp (#353)
    
    closes #300

[33mcommit 574bda89f8a6fc8bb5a5499bff35d12db8f34e39[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 4 23:21:02 2022

    Update status.md to reflect conventional use (#446)
    
    Updates the wording that explain in details what various labels means,
    consistent with how we've been using them in the last two months.

[33mcommit 30ee1d901bb4fd255a1ba3a4e976b4b7bfbf798b[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Nov 4 18:01:13 2022

    Add spec for BatchNormInferenceOp (#377)
    
    closes #347

[33mcommit a795aa58d69641aec1740c3078ded0c3f0b67669[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Nov 4 16:55:01 2022

    Update spec checklist to account for prettyprinting conventions (#439)
    
    This is something that we've been following for some time now, and I
    figured it'd be good to document explicitly and have a chance to
    discuss.

[33mcommit 10803f39193580bace4d215dc37d6712cdded8d3[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Nov 4 16:46:00 2022

    Add spec for SelectOp (#428)
    
    closes #420

[33mcommit e816de07e8f2a56b34e6fcc691a14d598bd3aae7[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Nov 3 23:19:02 2022

    Fix the reference to the "Operands" section in the spec checklist (#440)
    
    It no longer exists and has been long replaced by the "Inputs" and
    "Outputs" sections.

[33mcommit 8c3feeee32e42899b3759b19bafb5758be2299e5[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Nov 3 22:15:26 2022

    Add spec for PopulationCountOp (#436)
    
    closes #435

[33mcommit 46e99f875a1b2ca7bbf17187b388904f8803e3e7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Nov 3 20:14:16 2022

    Add spec for FftOp (#380)
    
    Updates ODS to use `HLO_FpOrComplexTensor` instead of `HLO_Tensor` for
    `operand`.
    closes #357

[33mcommit 78bd06c2468a98dd977c1fd566fceb9e0fe88805[m
Author: Tres <tpopp@users.noreply.github.com>
Date:   Thu Nov 3 19:15:27 2022

    Remove explicit references to kEmitAccessors_Prefixed (#438)
    
    This is the default and the option will be removed by
    https://reviews.llvm.org/D136727

[33mcommit ca7ef4f80152e9d5314e4c9750590d018553e91c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Nov 2 22:34:01 2022

    Prettyprint CustomCallOp target using SymbolRef printing (#432)
    
    This is consistent with `func.call` printing, and relies entirely on
    upstream machinery. Using this printing is cleaner and more consistent,
    and if we eventually consider modifying what is permitted in
    CustomCallOp target names, this printing will not need to be updated.
    
    ```
    %0 = stablehlo.custom_call @foo(%arg0) ...
    %1 = stablehlo.custom_call @"foo-not-id"(%arg0) ...
    ```
    
    This is a little messier for non-identifier target names, but there are
    _very_ few cases that target non-C or non-MLIR identifier names. The
    only ones I was able to find were in a single test file in mlir-hlo.
    
    The implementation directly calls upstreams
    `AsmPrinter::printSymbolName`/`AsmParser::parseSymbolName`. These
    methods are able to handle identifier and non-identifier targets.
    
    Closes #431

[33mcommit 65b9091d71b990fd4efe0d7bac7ec0b0fabcc888[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 2 18:42:23 2022

    Integrate LLVM at llvm/llvm-project@d4c61314420c (#430)

[33mcommit f3e4126ce166aed08a75e60383fd6a820af3a16a[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Nov 2 01:31:39 2022

    Add spec for CholeskyOp (#367)
    
    closes #356

[33mcommit 3a89d59082b2a0bb8593931f974256594dadae3a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Nov 2 00:15:21 2022

    Allow inlining StableHLO ops (#411)
    
    This hasn't been discussed much, but as part of defining the
    relationship between StableHLO and MHLO, it looks reasonable to avoid
    doing transformations on StableHLO and leave that to MHLO.
    
    This way, we don't have to duplicate stuff like constant folding,
    canonicalization, etc etc.
    
    Inlining is one of the transformations that's in a gray area because
    this is something that we get for free from upstream, so I don't think
    that the duplication argument applies here.

[33mcommit 7bf0e198e61f3bd9d6ee06193a3d5a48a4ca4c05[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Nov 1 22:24:46 2022

    Fix typo in spec_draft.md (#426)
    
    "implementation-dependent" => "implementation-defined".
    
    As we're making our language around implementation-defined and undefined
    behavior stricter, rewording this looked appropriate.

[33mcommit caae7b5d39a12a1390ccda8e69de65a03bde5d89[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Nov 1 21:41:14 2022

    Lexicographically order GatherOp (#422)

[33mcommit 264ce43632925cedb12e41d96d680dae437e4d4f[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Nov 1 17:08:36 2022

    spec for GatherOp (#275)
    
    fixes #245

[33mcommit 963ae2c040553e7ac041203ffd68496e3008d0af[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Oct 31 20:09:13 2022

    Add type inference for stablehlo.convolution (#360)
    
    This PR  includes:
    1. Delete current `verify()` and merge to new
    `inferReturnTypeComponents()`
    2. Prepare the shape function ready to be merged to `TypeInference.cpp`
    in coming PR.
    3. Double checks verification against the XLA
    [`shape_inference.cpp`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc):
    All covered.
    
    It does not include:
    1. Move to `TypeInference.cpp`: it will make this PR extremely hard to
    review. Track in https://github.com/openxla/stablehlo/issues/361
    2. Adding new logic: the only change is add assert `num_dims =
    num_spatial_dims + 2` same as XLA, see D1 below.
    
    ## Differences with
    [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc):
    - D1. This PR aligns StableHLO with XLA: revert the support for unknown
    dimensions inherited from MHLO
    
    Existing MHLO (and StableHLO) supports unknown dims, but XLA does not:
    [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc#L1698)
    like this:
    ```
    const int num_dims = num_spatial_dims + 2;
    if (lhs.rank() != num_dims) { // error out
    ```
    but now in StableHLO the input rank can be larger than num_spatial_dims
    + 2: see
    [test](https://github.com/openxla/stablehlo/blob/main/stablehlo/tests/ops_stablehlo.mlir#L3731).
    Note that the "2" means "batch_dim + feature_dim". How to infer the
    extra dims? The existing
    [inferConvolutionOpReturnShape](https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/StablehloOps.cpp#L1593)
    inits every dims directly from the "actually return type" and then do
    the inferences.
    After discussion, this PR changes StableHLO to that do not support
    unknown dimensions at all. The unknown dimension in MHLO is only used
    for CustomCallOp when `hlo.custom_call` is translated back into
    `mhlo.convolution`, and should be maintained seperately. StableHLO does
    not need in involve this.
    
    The test is `@conv2d_generic` in `verify_conv.mlir` which is a negative
    test (which is a positive test in MHLO).
    
    - D2. XLA has [layout
    check](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc#L1710)
    ```
    TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));
    TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));
    ```
    I assume StableHLO have not and do not need it.
    
    - D3. @sdasgup3 Are you aware that why StableHLO/MHLO have checks in
    both
    [verify_conv.mlir](https://github.com/openxla/stablehlo/blob/main/stablehlo/tests/verify_conv.mlir)
    and
    [stablehloOp.mlir](https://github.com/openxla/stablehlo/blob/main/stablehlo/tests/ops_stablehlo.mlir)
    at the same time?
    Sandeep's reply: merge them to 'verify_conv.mlir` and will check them
    with Spec later.
    
    - D4. XLA has error "Dynamic output feature dim on convolution kernel is
    not supported"
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc#L1886
    But StableHLO supports it by design in
    [test](https://github.com/openxla/stablehlo/blob/main/stablehlo/tests/verify_conv.mlir#L746).

[33mcommit f70b37a3aa578541f8f83d4d0230b86ea5bc77f6[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 31 14:30:48 2022

    Integrate LLVM at llvm/llvm-project@b4db15a94964 (#406)

[33mcommit d496ed2f938c063831d512ef72ce6c19b9742990[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Oct 31 05:23:33 2022

    Handle bounds in the ConcatenateOp shape function (#389)
    
    `ConcatenateOp` has variadic inputs, so `inferMostSpecificType` need to
    be used to infer multiple dims (size, bound) from multiple input
    tensors, which set the rules of how to infer the return bounds for
    binary ops like `AddOp`.
    However, `ConcatenateOp` is more complicated:
    1. For the dim that will be concatenated, we need a new set of different
    rules to infer the dimension sizes and bounds.
    2. For the rest N-1 dimension, the existing rules used by binary ops can
    be re-used.
    More details for (1) from the new code comment:
    ```
    // Inference rules to concat dimensions with bounds (lhs/rhs are commutative):
    //       Dim of lhs     Dim of rhs      Infer
    //  c0:  X              Y               X+Y
    //  c1:  X              ?               ?
    //  c2:  X              ?, B            ?, X+B
    //  c3:  ?              ?               ?
    //  c4:  ?              ?, B            ?
    //  c5:  ?, B1          ?, B2           ?, B1+B2
    ```
    So, in the PR, `inferMostSpecificType` become large but refactored to be
    more easy to follow:
    1. `inferMergedDimAndBound()`: The old kernel part of how to handle each
    dim for binary ops is moved to this separate util function. NO logic
    change but rewrite with more readability.
    2. `inferConcatenatedDimAndBound()`: The new rules are implemented by
    this new dedicated method.
    3. `inferMostSpecificType()` is still the public entry point: with a new
    argument `concatenateDims` which indicates which dims will be
    concatenated instead of merged.
    
    I would expect that in the future more categories of inference rules
    will be introduced for interface `inferMostSpecificType()`, and it makes
    sense to me to refactor further when that happens.
    I prefer not to split the (1) and (2) to two different PRs, as (1) is
    more meaningful when staying with (2) (3).
    
    In the final PR, the code is split and expect to reuse again in https://github.com/openxla/stablehlo/issues/409
    
    Add reviewer @smit-hinsu .

[33mcommit 905648d490742e657261a2ce0358b07c3a130f60[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Sun Oct 30 22:02:28 2022

    Doc update for splitting the verifiers (#402)

[33mcommit 44503ec55a164d232f7dc500e7530720138674b1[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Sat Oct 29 00:18:44 2022

    Add spec for WhileOp (#344)
    
    closes #326

[33mcommit 07578f02f7a6d1477e43a3c8f52db6c5ec33eb68[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Oct 28 15:33:29 2022

    Update SetDimensionSize type inference to recognise constant sizes (#396)
    
    closes #385

[33mcommit 83d90a3bc449761145ff38ab8cde250f22228911[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Oct 28 03:47:17 2022

    Expose channel_id in CollectivePermuteOp (#388)
    
    Closes #340

[33mcommit 885b8e32afbb57bc1ee8a148420f609716440d54[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Oct 27 18:41:23 2022

    Fix error handling and add testing for printDenseI64Array (#397)
    
    The code `DenseI64ArrayAttr::parse(parser,
    Type{}).dyn_cast<DenseI64ArrayAttr>();` is problematic since if parsing
    fails, the null attribute will crash on `dyn_cast`.
    
    Move the `dyn_cast` to after error handling. Add test case.

[33mcommit bbc5dd1fdd73747fc7e7d0b3fa9569cf9abef56a[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Wed Oct 26 21:17:11 2022

    Integrate LLVM at llvm/llvm-project@12204429f2f9 (#393)

[33mcommit 5e41ad2d2bf309ba1d856247c256a2b3e90962bf[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Wed Oct 26 18:19:54 2022

    Remove ReduceOp::build() to sync with MLIR-HLO (#392)
    
    fix https://github.com/openxla/stablehlo/issues/391
    Please see the detailed link in the related bug.

[33mcommit 8d51fe2631c6c25ab4295721a69102fe8ddac534[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Oct 26 18:11:54 2022

    Add code comments for print functions (NFC) (#395)
    
    Move code comments for `printComplexOpType` to header file. Add
    description for `printSelectOpType`.
    
    Noticed during MHLO integration work.

[33mcommit 72cbcdf530620c321aac46506447b8eb1f3a69d8[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Oct 26 03:50:36 2022

    Pretty print for dimension/permutation attributes (#373)
    
    Added custom directive for printing `I64ElementsAttrs` which are
    verified to be Rank-1:
    
    ```
    custom<DenseI64Array>($dimensions)
    ```
    
    To avoid re-writing custom parsers for Rank-1 tensors, this CL leverages
    the printer/parser for `DenseI64ArrayAttr`.
    
    The `parse/printDenseI64Array` methods can be removed once we migrate
    these Rank-1 `I64ElementsAttrs` to instead use that type. I have begun
    this work on the
    [dense-array](https://github.com/GleasonK/stablehlo/tree/dense-array)
    branch, but the integration work required to migrate those types is
    _huge_. I'm not sure it is the best use of time right now, especially if
    we want this migration in both MHLO/StableHLO.
    
    Closes #24

[33mcommit ec168644f4e8b28fe3bc4ce0536f4a94801ed980[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Oct 26 02:12:32 2022

    ODS cleanup and fix typos (#390)

[33mcommit 1bd686a9c5c9e9ce5ef2222c410a596d0af4f141[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 25 06:11:15 2022

    Handle bounds in the PadOp shape function (#375)
    
    Add bounds support for `PadOp`
    Why change the shape function interface from `inferReturnTypeComponents`
    to `inferReturnTypes`? Appending `inferredReturnShapes.emplace_back(...,
    encoding_attr)` inside `inferReturnTypeComponents` will trigger error in
    [upstream](https://github.com/llvm/llvm-project/blob/main/mlir/lib/Interfaces/InferTypeOpInterface.cpp#L190)
    ```
    assert(shapeAndType.getAttribute() == nullptr && "attribute not supported");
    ```
    
    This PR follow the verification of in dim loop similar to
    https://github.com/openxla/stablehlo/pull/378/files.
    
    Also add @smit-hinsu as reviewer.

[33mcommit 69cdb6464528d7aa01894032da1cf7264dc766a5[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 25 02:24:48 2022

    Merge verify and shape function of BatchNorm ops (#387)
    
    fix https://github.com/openxla/stablehlo/issues/370

[33mcommit 0760eaf381f319cf0c579acf5d9d24f03d96cfea[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Oct 25 01:27:57 2022

    Integrate LLVM at llvm/llvm-project@0051b6bb7877 (#386)

[33mcommit 8b49e34d73752d10d94d929a5952f4bbf3eb7f1d[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Oct 24 02:48:39 2022

    Refine the inferred bounds of SliceOp (#378)
    
    This PR refines `SliceOp`: the existing logic of bounds is kind of
    tricky:
    https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/TypeInference.cpp#L1120:
    it is using `inputBounds[i], start[i], limit[i], strideVals[i]` but jump
    over all the verification for them in later lines. This looks danganous
    and refactored in this PR. Now the logic for for dim `i` the loop is:
    1. Do checks for each attr[i]
    2. If the dim is dynamic and NO bounds, set result_dim=-1, continue
    loop;
    3. For static dim, calculate result_dim; set -1 for result_bound if need
    4. For dynamic dim with bounds, set -1 for result_dim; calculate
    result_bound if need.
    
    Also add @smit-hinsu as reviewer.
    
    NOTE: the (1) above may have **MHLO impact**, so need fix MHLO tests
    accordingly if some "positive" tests error out: they are invalid
    actually.
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit 4152bb871975ee1575071808a02b144dea80d951[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Oct 20 22:28:27 2022

    Quick fix reduce and reduce_window tests (#371)
    
    Fix with better error messages and fully align with MHLO.

[33mcommit 37be062b5f2f159204f329ea42bd4f00c1b0fc43[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Oct 20 20:45:36 2022

    Fix a bug in 'hasDuplicates` which unintentionally modifies its input arg (#372)

[33mcommit 3b9d52d01ebcf2d1167af4f192253929bcd49fda[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Oct 20 05:27:49 2022

    Move shape functions of pad and concatenate (#366)
    
    Move the shape functions of 2 ops: pad and concatenate to
    `TypeInference.cpp`. They will add bounds support soon in later PRs. For
    concatenate: the old `verify()` method is removed.
    This PRs include:
    1. Pure code move and minimal signature change.
    2. Trivial unit test change. Some code style refinement.
    
    No included:
    1. Any logical change: they are already both labeled "yes" in
    `status.md`. And was revisited fully in MHLO as well.

[33mcommit 813c9fde5ed09ab54f91b066caada21beda512a4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Oct 20 00:33:36 2022

    Integrate LLVM at llvm/llvm-project@82c820b95cf7 (#364)

[33mcommit a1bf7f5bc16afad1a19da0e2a25085023a2613db[m
Author: Benjamin Chetioui <3920784+bchetioui@users.noreply.github.com>
Date:   Wed Oct 19 23:58:30 2022

    Clarify the semantics of StableHLO's sort operation. (#363)
    
    The comparator must satisfy the same requirements as C++'s Compare
    concept, i.e. two elements `v1` and `v2` are considered to be equal by
    the comparator iff `comparator(v1, v2) = false && comparator(v2, v1) ==
    false`. This precision is important to exclude comparators such as LE or
    GE, for which setting is_stable to true does not produce a stable sort.

[33mcommit 8ec92006166d0602d0dca32e0267d169c2078e0d[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Wed Oct 19 05:49:30 2022

    Handle bounds in the Slice op shape function (#343)
    
    Also move common parts among this op and Transpose op as common method.
    
    @smit-hinsu add as reviewer as well.

[33mcommit 61d0ddfb19f8cc4408a0bec04c40b02c8e091a26[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 17 21:59:06 2022

    Integrate LLVM at llvm/llvm-project@335f94b (#352)

[33mcommit de509d11c0e755ee8d41b3c00f6c981b8c2a8e63[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Mon Oct 17 18:52:40 2022

    Add spec for SortOp (#310)

[33mcommit f2712e3284a416f9b3e0b8f66dee2b9ec5bd20cc[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Mon Oct 17 18:32:00 2022

    Add spec for CaseOp (#324)

[33mcommit c22158a8a434217c6a535dacc23e1a8fbe6d05b4[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Mon Oct 17 18:17:39 2022

    Add spec for IfOp (#325)
    
    closes #311

[33mcommit 0a080a4b6c23ef69ebea5c99f4df68cc85af19f4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 17 18:16:38 2022

    Minor optimization for inferMostSpecificType (#341)
    
    Switch from SmallVector to ArrayRef and save on llvm::to_vector.

[33mcommit 850a3a8c2a1178778c41acfccce35a48d3bdd166[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 17 17:43:00 2022

    Update verification status for PadOp to `yes` (#350)
    
    closes #329

[33mcommit b546dbb08bc43348c0513dc0e997f30d6f89ec0c[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 17 03:28:43 2022

    Fix typo in the spec for broadcast_in_dim (#342)
    
    result and operand don't have to have the same rank.

[33mcommit f781051ca508caba2b3db4cd7b77f082f0309dc2[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sun Oct 16 21:10:36 2022

    Refactor pretty printing machinery to separate files (NFC) (#332)
    
    Created `stablehlo/dialect/AssemblyFormat.h` with shared code for pretty
    printing. This only depends on `StablehloBase` and upstream MLIR.
    
    Skipping the move of print operations that rely on specific attribute
    types. Will reconsider these in a later PR. Custom printer machinery
    requires functions to have a certain signature, so there is a good
    change these will need to be templated.
    `StablehloOps.cpp` has many `using mlir::hlo::print*` declarations. An
    alternative is to `using namespace mlir::hlo` in the file scope, but
    that is not super desirable and clang tidy complains.

[33mcommit ad07b845de51824a646bea963b9c7f203dd8876e[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Sun Oct 16 04:04:17 2022

    Change shape function interface for branch ops (#339)
    
    For shape functions, the interface`inferReturnTypeComponents()` infers
    `SmallVectorImpl<ShapedTypeComponents>& inferredReturnShapes` while
    `inferReturnTypes()` inferes`SmallVectorImpl<Type>&
    inferredReturnShapes`. However `ShapedTypeComponents` dose not fit for
    the Token case for the result type `HLO_TensorOrToken` for if/case/while
    ops. Thus, we need use the `Type` as inferred type: choose
    `inferReturnTypes()`.
    
    Besides, these functions will be called by with MHLO, so this PR is
    expected to check in before shared with MHLO.

[33mcommit dc52de7010e2954d49a6ba6bd1befd0818d8aa75[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Oct 15 02:46:59 2022

    Remove duplicate code (#338)

[33mcommit 33df56a59cfe09cfb66d21a3a2bcfb36cb8b36ea[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Oct 14 18:28:10 2022

    Audit status.md and fix mismatches with reality (#331)
    
    When looking into https://github.com/openxla/stablehlo/pull/253, I
    realized that I have no idea how to reliably review it in its Markdown
    form.
    
    I noticed a few values which didn't reflect reality, and I was concerned
    that there are more, and I'm going to miss them because I don't have a
    way to systematically review status.md.
    
    I decided to convert status.md into spreadsheet form to automate some
    aspects of the review, and I'm very happy with the outcome. Now I feel
    confident that I haven't missed anything. There's no easy way to share
    the Google Sheets spreadsheet that I used because we don't yet have an
    OpenXLA-specific Google Drive, but this pull request at least shares the
    outcome.
    
    Summary of data changes:
      1) "Specification" column didn't need any changes.
      2) "Verification" column was updated based on the rules provided in
         #253, plus I also accounted for open tickets for verification of
         some of the specced ops.
      3) "Type Inference" column needed about a dozen changes with no rhyme
         or reason to them - I went through all the ops, looked at their
         status in the ODS and in StablehloOps.cpp, made changes if needed.
      4) "Pretty Printing" column didn't need any changes.
      5) "Interpreter" column needed one update - "yes" for "multiply".
    
    Summary of style changes:
      1) Rows are now sorted alphabetically (previously they were sorted
         almost alphabetically).
      2) Rows are now aligned left, so that we don't need to think whether
         to align "yes" one whitespace to the left or one whitespace to the
         right to keep it centered.
      3) "yes(need-revisit)" has been renamed to a more concise "revisit".

[33mcommit faa5df1a89a51e79271e2a7cfb6388c931b6bb93[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 14 18:17:03 2022

    Clarify wording for variadic types and fix format (#333)
    
    Changes ConcatenateOp's Type description from `variadic number of
    tensors of any supported types` to `variadic number of tensors of any
    supported type` to clarify only one supported type is allowed for the
    variadic tensors.

[33mcommit 91e1ae41fb25feaf06f3c7a5aec3a97135efb7f6[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 14 17:34:22 2022

    Refactor error handling code (#232)
    
    Error handling is needed in several files, and currently, we are
    repeatedly defining the wrapper error handling function in each file,
    which duplicates code:
    * Element.cpp
    * Interpreter.cpp
    * Tensor.cpp
    
    This PR refactors the error handling code in `Errors.h`

[33mcommit e42a79130f0a9b3d21fba4d193f831f166b98e6e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Oct 14 02:16:50 2022

    Refactor spec to have consistent usage of "any supported element types" (#317)
    
    refactor to have consistent usages of "any supported element types"

[33mcommit 6875390aad4d71d3f3fee3f19a81515229dea6af[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Oct 13 20:34:25 2022

    Move shape function of TransposeOp to shared file (#322)
    
    This PR moves the shape function of TransposeOp including the bounds to
    shared files. This is an example how we add bounds and move code for
    other ops in the near future.
    
    Question: Does the existing code
    https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/StablehloOps.cpp#L4189
    mean that we can't have bounds and sparse tensor encoding at the same
    time? (Eugene: Correct.)
    
    The reason we prefer `inferReturnTypes` over
    `inferReturnTypeComponents`:
    inferReturnTypeComponents does not work: this assert block it:
    
    [inferReturnTypeComponents](https://github.com/llvm/llvm-project/blob/main/mlir/lib/Interfaces/InferTypeOpInterface.cpp#L190)
    So I will keep using current inferReturnTypes
    
    @smit-hinsu PTAL, thanks!

[33mcommit 6deb04afd154c6e70caf1d527bacc5fa6b08d99f[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Oct 13 20:28:06 2022

    Move SliceOp to shared file before we add bounds to it (#328)
    
    This PR purely move the code without change, I expect to add the bounds
    to the it directly in the shared file. @smit-hinsu
    
    Following PRs will move more ops for which we will add bounds soon, but
    more refactoring code needed, so let's keep this PR simple to review.

[33mcommit 86ca73b97851e42b7d5ecc513bc5b52dae4462bd[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Oct 13 17:29:51 2022

    Introduce function types (#323)
    
    This supports ongoing work on sort (#259) and reduce (#300).

[33mcommit 424819cc8939b9db1f7d02dbd76f8358a9e234ef[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 12 22:29:52 2022

    Tweak the description of ConcatenateOp::dimension (#320)
    
    Instead of "`si64`", expand to "constant of type `si64`" to be in line
    with other places in the spec.

[33mcommit 099976109d5b2c9852c02c16c2c53cf9f04f6fb5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 12 20:06:42 2022

    Improve Python bindings for CHLO and StableHLO (#283)
    
    1) Fixes chlo.py and stablehlo.py to properly export constructors for
    attributes and types.
    
    2) Aligns Python accessor names with C++ accessor names. E.g. renames
    `ComparisonDirectionAttr.comparison_direction` to `value`. We don't
    promise source compatibility for Python bindings at the moment, so no
    compatibility shims are introduced.
    
    3) Adds exhaustive tests for ChloModule.cpp and StablehloModule.cpp.
    
    (As promised in the code review which replaced MLIR-HLO's CHLO with
    StableHLO's CHLO).

[33mcommit 7dc041e6803a6a59b69d19635e93ff1c980dfa55[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Oct 12 18:28:33 2022

    Fix markdown table format (#319)

[33mcommit 163f2fd4af8ea5cd513d65b8a3c1a0faf87e263a[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Oct 12 17:59:57 2022

    Consistent spacing between attributes types and names (#318)
    
    As part of integrate from MHLO to StableHLO.

[33mcommit e2afbbe8b429cb63a87fe7278df2caf914e5a7c8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 12 01:14:11 2022

    Fix up the old Operands/Results notation (#316)
    
    I've just merged the slice spec and noticed that it is using the old
    notation (which is fine because it was introduced very recently). This
    PR fixes it up.

[33mcommit f2e2710896e0565ffe1bbf5a34f1e6d09ec8e5d7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Oct 11 23:42:02 2022

    Add spec for SliceOp (#280)
    
    closes #244

[33mcommit 94baff5f9149aa855684378123136762ca7ff731[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Tue Oct 11 23:41:02 2022

    Update inference test to use stablehlo dialect instead of mhlo (#304)
    
    This prevents reduce and reduce_window shape functions' testing.

[33mcommit 5541c69e55807ecba70087479b254297078544a5[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 11 23:06:37 2022

    Delete redundant calls of isCompatibleReturnTypes (#315)
    
    These parts should be part of
    https://github.com/openxla/stablehlo/pull/145 but somehow did not get
    deleted. This PR fix this issue.
    IfOp, CaseOp, ConcatenateOp, RngOo.
    The corresponding MHLO parts are well maintained.

[33mcommit a1ee03a5d101ff4f1118555c29c8ad1529366f8e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 11 22:16:41 2022

    Add spec for BroadcastInDimOp (#284)
    
    Adds the specification as  part of #122
    
    Fixes https://github.com/openxla/stablehlo/issues/277

[33mcommit 80f06d586847d9dfa2ba848d2f80b86cca7fdc00[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Oct 11 21:57:52 2022

    Rename Operands/Results to Inputs/Outputs (#313)
    
    This allows us to avoid thinking about whether something is an operand,
    an attribute or a region. See #310 for an inspiration.

[33mcommit 66943d3aeb36c239cd5d433aea44916a4f08731a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Oct 11 21:52:58 2022

    Rename "array of type si64" to "tensor constant of type si64" (#314)
    
    I remembered that in the spec we already define "tensor constant", so
    let's use that definition.

[33mcommit 760f22378aa324e5643538e60bb21054ab9e0a09[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Tue Oct 11 19:55:00 2022

    Add specification and interpreter for MulOp (#240)
    
    Wording for add and subtract is updated: integer overflow/underflow ->
    overflow.
    closes #199

[33mcommit 27749acb9af8dc8f1e95274737784b5cc02fd71e[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 11 07:34:00 2022

    Update interpreter of add, maximum, and minimum w.r.t supporting bool type (#287)
    
    **Diff based on : https://github.com/openxla/stablehlo/pull/286** (For
    convenience please review that first)
    
    
    
    fixes https://github.com/openxla/stablehlo/issues/285
    partially solves https://github.com/openxla/stablehlo/issues/262

[33mcommit deebb4ab018ea2e79bedd124842d4f3b6b90d101[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 11 06:44:13 2022

    Integrate LLVM at llvm/llvm-project@d9404d6d012f (#309)

[33mcommit 03ce69d73b6db3c853b0f3044ccd4a4d2d0ecb3b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 11 05:36:36 2022

    Add interpreter and tests for bitwise ops (#286)

[33mcommit e23416e28e9b6530b49310cdb103ceb92eb6ab08[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Oct 11 00:51:53 2022

    update spec of add, maximum, and minimum w.r.t supporting bool type (#264)
    
    Partially solves https://github.com/openxla/stablehlo/issues/262 w.r.t
    the ops add, maximum, and minimum.
    That way these ops will be aligned with the ODS and the HLO semantics.

[33mcommit ecba6c06ad715a2a9071d5b5e6ad5d47b2f7c8ce[m
Author: Smit Hinsu <1990079+smit-hinsu@users.noreply.github.com>
Date:   Mon Oct 10 21:48:44 2022

    Handle bounds in the Transpose op shape function (#292)
    
    This makes it possible to propagate the input bounds while creating the
    Transpose op.

[33mcommit 804eadfaa6040d27532b39d3d35f19029c1bda8e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 10 20:47:42 2022

    Add formal notation for PadOp (#291)
    
    Our discussions in #280 provided some additional notational inspiration,
    and I think I've got something that's readable and correct.

[33mcommit cf403f3c2a09635231c532ccce08f185ec21494c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Oct 10 20:41:05 2022

    Fix clang tidy name mismatches (#294)
    
    Closes #293.

[33mcommit 9722639875d8fd41b58c3f227fc949eb6ee903c4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 10 20:27:35 2022

    Update CHLO to use prefixed accessors (#282)
    
    Follows up on #203 to complete the migration of the StableHLO repository
    to kEmitAccessorPrefix_Prefixed.

[33mcommit b69808599c7c312c5490c94ee785a8f5900dfdd4[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Oct 10 16:40:08 2022

    Update spec of AbsOp to match HLO semantics (#288)
    
    fixes https://github.com/openxla/stablehlo/issues/263

[33mcommit 26b771e5865a6b3b92d889fc789d7ab067ed53da[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 10 05:06:34 2022

    Anchor evals to result's index space iterator (#290)
    
    This better matches the spec, where our formal notation always says
    `result[...] = operand[...]`. The only exception so far is transpose
    where the spec derives result index from operand index.

[33mcommit f716a0a227909274aefdb837d8c2aa99baa4b4cf[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 7 23:34:06 2022

    Clean up format and fix typos (#281)
    
    Cleans up formatting in `spec_draft.md` and `status.md`. Fixed typo
    `max` in `stablehlo.min` semantics.

[33mcommit 39979735cea344d2a5c861e00c73988766c1e8af[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 7 22:13:56 2022

    Add spec for PadOp (#267)
    
    closes #243

[33mcommit fdd47908468488cbbb386bb7fc723dc19321cb83[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Oct 7 21:59:11 2022

    Integrate LLVM at llvm/llvm-project@b9898e7ed1ce (#279)
    
    Comparable changes to:
    
    https://github.com/tensorflow/mlir-hlo/commit/44ffec7bce15ef17f5264aadc36f3c6e89642d8a

[33mcommit 35ded40adf9c9b1bb6fee6bbd0956294c886b62c[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Fri Oct 7 21:22:59 2022

    Move 12 new shape functions to separate target (#269)
    
    The new `TypeInference.cpp` contain the shape functions which
    will be reused by MHLO after StableHLO is synced back to MHLO.
    This PR is pure refactoring. No verification logic change.
    
    The 12 shape functions are are:
    1. batch_norm_grad: https://github.com/openxla/stablehlo/pull/67.
    2. batch_norm_inference: https://github.com/openxla/stablehlo/pull/67.
    3. batch_norm_training: https://github.com/openxla/stablehlo/pull/67.
    4. case: https://github.com/openxla/stablehlo/pull/135.
    5. dot_general: https://github.com/openxla/stablehlo/pull/153.
    6. if: https://github.com/openxla/stablehlo/pull/135.
    7. map: https://github.com/openxla/stablehlo/pull/116.
    8. reduce: https://github.com/openxla/stablehlo/pull/50.
    9. reduce_window: https://github.com/openxla/stablehlo/pull/91.
    10. sort: https://github.com/openxla/stablehlo/pull/221.
    11. triangular_solve: https://github.com/openxla/stablehlo/pull/114.
    12. while: https://github.com/openxla/stablehlo/pull/207.

[33mcommit 927578a001aa54c06acb908a917afc4c8393fbcc[m
Author: Subhankar Shah <subhankarshah@google.com>
Date:   Fri Oct 7 20:39:00 2022

    Add spec for ReverseOp (#258)
    
    Closes #247

[33mcommit 430fe5dc3588328250627cb794fa3673d599cfd8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Oct 7 19:36:28 2022

    Flip Type Inference status of abs, add, minimum and maximum (#276)
    
    As established in #250, #262 and #263, the spec for these ops is incorrect
    so we cannot say "yes" for these ops just yet.

[33mcommit 7babe1eb2dd85fd4c8888d1c0bd8fc28b9cd9283[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Fri Oct 7 17:34:18 2022

    Add `DefaultValuedOptionalAttr` and `use_global_device_ids` (#272)
    
    This PR has two patches that close #236 and #237.

[33mcommit 285ef28bafad3dda27159765ee4a2d0f430b527d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 7 17:22:47 2022

    Update ODS links to reference StableHLO spec (#250)
    
    Updates the ODS description to include:
    * One sentence description from the spec
    * Link to the spec
    * Pretty print syntax
    
    The changes in `status.md` for Verification and Type Inference columns
    are also now in sync with the spec.

[33mcommit c5efe94c99b9828e05e4daa978efd40955b8cc1c[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Oct 7 16:54:50 2022

    Add specification and interpreter for IotaOp (#234)
    
    closes #225

[33mcommit a0b632881e7cf925aa045a339b817916ae4229bb[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Oct 7 05:20:02 2022

    Unconditionally update `description` in op's ODS (#274)
    
    The current version of the checklist recommends to update `description`
    only if the implementation of the op conforms to the spec.
    
    When I wrote that recommendation, it was deliberate. I thought that the
    process could be: 1) if the implementation doesn't conform don't update
    the description, 2) fix the tickets, 3) update the description.
    
    But now I think that this simply introduces an extra step into the
    process (step number 3) without a good reason. If there's a spec for the
    op, it's already useful to make it part of documentation. And if there
    are some bugs in the implementation, that's fine as long as we have
    tickets for them.

[33mcommit 3bff283a7476489d0fea2fffad0e9bdec3587950[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Oct 6 22:48:17 2022

    Add spec for ConcatenateOp (#248)
    
    Closes #242

[33mcommit a4fb4f70e22a5dd310bb2f2aa8f09d3b61943f97[m
Author: Ashay Rane <ashay@users.noreply.github.com>
Date:   Thu Oct 6 22:29:55 2022

    Fix CMake build (#148)
    
    This patch reorders the libraries linked into the stablehlo-interpreter
    target, since otherwise, the CMake build fails with the error:
    
        StablehloOps.cpp: undefined reference to `mlir::DataLayout::closest`
    
    This patch also adds `target_include_directories()` for the
    `StablehloBase` and `StablehloBroadcastUtils` so that path to the
    Stablehlo header files is added to the include path of downstream
    targets.

[33mcommit 71a5b6b4e70ff4c7136f04c827dccd89df72b686[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Oct 6 15:19:41 2022

    Integrate LLVM at llvm/llvm-project@11897708c022 (#271)
    
    [TranslateFromMLIRRegistration](https://mlir.llvm.org/doxygen/structmlir_1_1TranslateFromMLIRRegistration.html)
    signature changed to include a description.

[33mcommit 9f984143df73c88ab7b1341b489e2c29a6876eb1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Oct 6 01:21:11 2022

    Avoid using DenseElementsAttr::getRawData (#117)
    
    fixes https://github.com/openxla/stablehlo/issues/83
    
    The PR implements the underlying data storage of the tensor using an
    `mlir::HeapAsmResourceBlob` object represents a mutable blob of data.
    This provides two improvements:
    1. No need for using `DenseElementsAttr::getRawData()` which cannot be
    relied to provide contiguous representation of underlying data in cases
    like boolean and splats.
    2. An `mlir::HeapAsmResourceBlob` object, being mutable, simplifies how
    we store the underlying Tensor data.

[33mcommit 31a3ef37e4a7561818dd757cb5e96ad9654466f1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Oct 6 01:16:35 2022

    Interpreter for MaxOp and MinOp (#187)
    
    fixes #185

[33mcommit c3588936a6554e5ec5e167393a7e658fdcae0206[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Oct 6 00:00:14 2022

    Fix lit checks with ops having implementation defined precision (#265)
    
    The current mode of testing the implementation is to use LLVM's lit-test
    framework to syntactically match the output of the operation with a
    (user provided) golden output. We do have some ops (like sine, cosine,
    tanh) which might produce results with implementation defined precision,
    failing the lit checks.
    
    To avoid that this PR allows the lit checks to match the accuracy up to
    12 places after the decimal point. The number 12 is chosen arbitrary but
    provides a descent approximation of the checked values.

[33mcommit 4f1ea0722eadd90148f6af3f1d4b65ea0fa60428[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Oct 5 18:47:21 2022

    Remove count from status.md column (#266)
    
    Currently, we are updating the count for each column when we are
    updating the status of the columns. This can be useful, but it is
    error-prone and removing this avoids tedious work verifying the count.

[33mcommit a5a6cf2779bd45828d87e028a9df52bb9caf0f37[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 5 07:35:00 2022

    Add a StableHLO specification checklist (#260)
    
    This will hopefully help with reviewing StableHLO spec pull requests.

[33mcommit e2ba7b6ce5746a2ee7619fbb6556e1260c254c58[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 5 05:41:49 2022

    Stop using signed integer types in MLIR examples (#261)
    
    As Gunhyun pointed out in a nearby PR, we should do that until #22 is
    fixed.

[33mcommit 1115e9c8aac5d40168decae8b44166b2cb769ac3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Oct 5 03:40:47 2022

    Reorder status.md alphabetically (#254)
    
    Now that we've switched from op C++ names to op mnemonics, we need to
    reshuffle the table a little bit.

[33mcommit 4859634f3d95851a7a55c16cf9d753327a57c860[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 4 21:26:52 2022

    Add type inference for stablehlo.sort (#221)
    
    Align the sort verification logic with
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo_verifier.cc
    and
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/shape_inference.cc.
    Remove some duplicated verification as ODS `SameOperandsAndResultShape`
    helps a lot.
    The existing TODO of partial dynamism is removed.
    
    Tests: old tests is comprehensive, and add a dynamics case.

[33mcommit 378f95fc140c354e9be4c366a7ec3d179bf31a20[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 4 21:07:29 2022

    Add type inference for stablehlo.while (#207)
    
    `.td` file: remove `HLO_PairwiseSameOperandAndResultType`: now we get
    the inferred type and compare it against the result type, with the
    support of dynamism. There is no need to maintain this trait and add the
    dynamism support for it.
    
    `cpp` file: With the power of `isCompatibleForHloTypeInference()`, the
    PR optimizes the existing verification logic for `stablehlo.while`: it
    can check the 1) count and 2) each pair of type at once for two
    TypeRange, so that the code can become more concise.
    
    Tests: The old tests are comprehensive but in wrong places:
    1. Negative tests are in `print_while_invalid.mlir` which looks like a
    refactor error. This PR renames the file to `verify_while.mlir`.
    2. For alignment, move the positive tests from `ops_stablehlo.mlir` to
    `verify_while.mlir` well.
    3. Add tests for dynamism and unranked cases.

[33mcommit 01c0c5d7f736180b79f550e8b31adb6ce0a16c7a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Oct 4 20:12:14 2022

    Simplify the language in specs of reshape and transpose (#256)
    
    "is equal to" is replaced with "=".

[33mcommit 246265a89a7d5f657f6eee97839cac7a7a53364c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Oct 4 05:48:18 2022

    Integrate LLVM at llvm/llvm-project@df7606a066b7 (#241)
    
    Bumping to match:
    https://github.com/tensorflow/mlir-hlo/blob/master/build_tools/llvm_version.txt

[33mcommit 7dd4230cc64fe8ffde1e8cbbe666e22f3aef999b[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Oct 4 02:55:55 2022

    Type Inference Proposals. (#197)
    
    Add the Type Inference implementation proposals for StableHLO.
    It contains public links, and revised phrasing for public reviewing.

[33mcommit a45b8e9838bf21c41d7f7c597ad5f6a27f89a05d[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 3 22:26:36 2022

    Fix MLIR code examples in the spec (#252)

[33mcommit 67bdf773878338c8e63e43443d7605efaef8e281[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Oct 3 20:09:16 2022

    Consistently use StableHLO mnemonics in the spec (#251)
    
    * Rename "max" and "min" in the TOC to "maximum" and "minimum" to follow
    the StableHLO mnemonics.
    * Rename "exp" in the TOC and in the actual spec to follow the StableHLO
    mnemonics.
      * Reorder `sqrt` and `subtract` to keep alphabetical order.
      * Also, remove the deprecated way of specifying op syntax.

[33mcommit c5c213a0d2c92577ffa2a7aac3ce3ea51f462ee4[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Oct 3 19:35:46 2022

    Clean up format and trailing spaces (#249)

[33mcommit faea8fa9bbbd71c532e826b860b8dd68cde44cf1[m
Author: Subhankar Shah <shubh0153@gmail.com>
Date:   Mon Oct 3 16:06:46 2022

    Fix back-link to ops section in spec draft. (#228)

[33mcommit 75d7b654e51729f1e733af6d7fa593514f14db6e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Sun Oct 2 19:42:18 2022

    Switch to using MLIR mnemonics instead of C++ names in status.md (#239)
    
    The specification draft uses mnemonics as well.

[33mcommit f5a5884dc4ece606e538e26ab96b7309e2a4285f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Sat Oct 1 00:05:54 2022

    Add type inference for stablehlo.dot_general (#153)
    
    Add type inference for `stablehlo.dot_general` and fix trailing spaces
    
    Note: a test for element type mismatch does not exist due to
    quantization for dot, hence why `SameOperandsAndResultElementType`
    interface is not present.
    
    closes #152

[33mcommit ed197d0bdfd4a8105e7e7f64cebfa8c763a3cad0[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Sep 30 23:00:35 2022

    Assorted cleanup (#231)
    
    * Inlines various errors directly into report_fatal_error
      (there doesn't seem to be a use case for having a temporary variable
      which is then immediately moved).
    * Uses SmallVector in permute (Sandeep, you were right - the actual type
      is such a mouthful!).

[33mcommit 188af79f8761084121f6ab9655578a9c58349f18[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Sep 30 17:00:34 2022

    Move type utilities into a separate file (#230)
    
    Also adds detailed comments.

[33mcommit c2fe3cbf31864f6ff499b73790eba371af99648b[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 30 02:03:08 2022

    Specification and Interpreter for  transpose op (#172)

[33mcommit a1c09e6ba76040d01b1f5314372107419c8e1cc1[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 30 01:44:55 2022

    Enforce compatible indices and shape while indexing into Tensor (#215)
    
    Enforce compatible indices and shape while indexing into Tensor

[33mcommit d02bbd6181d5204237ee58ed12f07911e72ebd1c[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Fri Sep 30 00:37:14 2022

     Enhance result capability match for all ops. (#145)
    
    * Enhance result capability match for all ops.
    
    * Add missing result capability match for CHLO.
    
    * Remove unnecessary comments and fix traits.
    
    Comments of method isCompatibleReturnTypes is only need once for each td file.
    Update HLO_PairwiseSameOperandAndResultType  to Compatible version.

[33mcommit 0551383291ae3a9e66fb67a1b6e3a349f2db9d35[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Sep 30 00:16:42 2022

    Add interpreter for NegOp (#214)

[33mcommit b9b01048d4bfbe535e6fae6eda59e73d5b69a6a9[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 29 23:09:54 2022

    Revisit use of mlir::Attribute storage of Element Class (#101)
    
    Revisit use of mlir::Attribute storage of Element Class

[33mcommit 17dc7933c89c9a26534d64a9f8231d8b8437ae30[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 29 18:55:27 2022

    Integrate LLVM at llvm/llvm-project@0fb2676c248e (#220)

[33mcommit e756405d676a2e8c5283be2a878a95a32e6d9df8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 29 18:50:28 2022

    Fix a spec bug for negate and abs op (#222)
    
    Fix a spec bug for negate and abs ops

[33mcommit e2672a5251caa5814a220cdda563eb44ec861f8e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 29 16:18:51 2022

    Update to use prefixed accessors (#203)
    
    Changes built on top of #202.
    
    Adapted from a patch by @tpopp (thank you).
    
    Closes #146

[33mcommit 6c0334f94316a2d81b71ac453840c738086ab899[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 29 15:37:21 2022

    Fix build warning spew. Change $operands->$inputs. (#202)
    
    Currently the build spews many warnings of the form:
    
    ```
    [14/42] Building StablehloOps.h.inc...
    /usr/local/google/home/burmako/stablehlo/stablehlo/dialect/StablehloOps.td:981:1: note: Skipping generation of prefixed accessor `getOperands` as it overlaps with default one; generating raw form (`operands`) still
    def StableHLO_OutfeedOp : StableHLO_Op<"outfeed", []> {
    ^
    ```
    
    This is because we are switching from raw accessors `$a --> a()` to prefixed accessors `$a --> getA()`. This causes a conflict when the variable name `$operands` is used since `getOperands()` is an existing method on the `Operation` class.
    
    Solution is to rename all instances of `$operands --> $inputs`, and all accessors `operands() --> getInputs()`.
    
    Closes #195

[33mcommit 388b134277b19b38293354cf7ae4ac0ad4b37447[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 29 02:16:28 2022

    Fix table format and other typos (#209)
    
    This commit makes markdown tables more readable and fixes few typos and minor improvements in spec_draft.md, interpret_add.mlir, and Interpreter.cpp files.

[33mcommit be42540a288a302b6e4071e7d29c0bf8b911589e[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Sep 28 20:50:24 2022

    Add specification and interpreter for SubtractOp (#179)

[33mcommit 3d40757cce645cd65cd499ee2145984e45f45963[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 28 06:42:27 2022

    Specification and Interpreter for  reshape op (#170)

[33mcommit 75d33f4e333dcb5d61a3ed61d2787f934ffb5b60[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 28 06:20:37 2022

    Use multi-dimensional indices to access tensor elements (#167)
    
    Co-authored-by: Eugene Burmako <burmako@google.com>

[33mcommit 9128054874622fbf8659f3544362c88186e2235f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Sep 28 01:10:30 2022

    Add interpreters for cosine and tanh op (#162)

[33mcommit 441e732a3f63b787d9fa63eae94508c0bc43dcb4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Sep 28 00:04:49 2022

    Fix a few typos in the spec (#200)
    
    Based on feedback on #193

[33mcommit 9e25e415e984423d6f5ffcb7e78797c474640261[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Sep 27 23:21:14 2022

    Fix up headers in the interpreter (#198)
    
    Add missing includes, fully qualify llvm::ArrayRef, llvm::SmallVector
    and llvm::StringRef.

[33mcommit 649e0a39483d6e1ab18af681f029ec885652972e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 26 23:58:21 2022

    Minor fixes to the formatting of the spec draft (#192)
    
    Consistently left-aligns prose, removes double newlines.

[33mcommit 32590fb35fb39d28841d721b10b29d03dd121331[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 26 18:14:34 2022

    Expand the definition of the tensor type (#193)
    
    In #167, #170 and #172, we are running into the limitations of the
    current definition of the tensor type which roughly amounts to
    "n-dimensional array with shape S and element type E".
    
    This definition was fine for elementwise op, where only the element type
    matters, but for ops like `reshape` and `transpose`, we need to
    elaborate the logical representation of a tensor.

[33mcommit 2b44f3ed8c3bd2c4810a15a1a24668519059f15c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Sep 26 16:50:53 2022

    Interpreter for CeilOp and FloorOp (#189)

[33mcommit 2396e7775be2d254e9165fdcc41ed723f557b75a[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 26 16:17:04 2022

    Generalize Element mapping logic (#190)
    
    Recent pull requests #162 and #179 prompt the generalization of the
    currently ad-hoc "if integer, do this, if float do that, etc" logic
    employed in Element's + and sine.
    
    This PR proposes a name for this concept (`map`, to follow the existing
    conventions in functional languages), a design and implementation for
    it (inspired by `std::visit`, `llvm::make_visitor`, `llvm::TypeSwitch`)
    and a refactoring to use it.
    
    For now, let's keep these functions internal to Element.cpp, but if
    they end up being required by Ops.cpp, we can make them part of
    Element.h.

[33mcommit 261aa751a93285ef6d17f1b2e7bb65443b7f8588[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 26 16:16:50 2022

    Refactor op syntax in the spec draft (#191)
    
    When reviewing #170, I was looking at
    `%result = stablehlo.reshape %operand : tensor<3x2xi32>` which was used
    as an example of reshape, and I realized that it might be pretty
    confusing to readers because it doesn't say anything about the tensor
    which is being reshaped.
    
    First I thought to propose using whatever syntax we're currently using
    in StableHLO, i.e `... : (tensor<2x3xi32> -> tensor<3x2xi32>)`, but
    then I realized that we shouldn't expect spec readers to know
    implementation details of the StableHLO dialect, which can also change
    over time.
    
    Given that, I'd like to propose that we consistently use MLIR generic
    syntax in examples, and additionally drop the informal mnemonic +
    signature notation used to introduce ops.
    
    Finally, I'd also like to propose that we align type names in the spec
    with type names in MLIR. Currently, we have some misalignments, e.g.
    `pred` vs `i1`, `c64` vs `complex<f32>`, which don't buy us much and
    make examples would look weird.

[33mcommit 0c188ad621d254e04e0b59281a2aa974a6180744[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Sep 23 22:58:40 2022

    Bootstrap StableHLO governance (#186)
    
    Bootstraps StableHLO governance from the SIG OpenXLA charter:
    https://github.com/tensorflow/community/blob/master/rfcs/20220713-sig-open-xla.md.
    
    In a follow-up pull request, I'll propose a process by which we'll be reviewing
    StableHLO RFCs: https://github.com/openxla/stablehlo/labels/RFC.

[33mcommit df38d90a4700be336a396ffcc6420e1289ca8932[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 23 16:35:33 2022

    adding links to elaborative testing (#155)
    
    Adding hyperlinks in spec to test-files having elaborative tests.

[33mcommit ee505800a5fad92a2f045301e3f17899bf0af87e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 23 15:27:40 2022

    Refactor the lint action into a build script, temporarily update .clang-format (#183)
    
    Two changes in this PR:
    
    1) Refactor the lint action to be a locally runnable bash script. This should give _more_ predictable results in terms of addressing CI failures.
    
    2) Update `.clang-format` to use Google style in the short term. More on this rationale below.
    
    Background on all this is that my editor formatted a large chunk of a file which caused the lint action to fail remotely, and took me quite some time to roll back the individual formatting changes, since google styling appears to prefer the most common spacing of `Type* var` vs `Type *var` in a file as the proper formatting.
    
    This change will allow local qualification / fixing, as well as provide a stop-gap on editor formatting issues while #15 is discussed. While I agree that we should resolve #15, I think more discussion is required, and there is unnecessary friction in the current setup/CI of clang-format in the current repo that should be addressed. Happy to drive the formatting conversation in the near future, but think this should be in place in the short term.
    
    GH Actions run testing that it errors properly in CI: https://github.com/GleasonK/stablehlo/actions/runs/3106772722/jobs/5034066123
    
    Closes #75

[33mcommit fe8c9b472caa15f3430db37e056429e565b2ca78[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 23 15:13:56 2022

    Pretty print for ops with single enum attribute (#161)
    
    See #25 for information, or view test updates for examples of new prettyprint.
    
    Implements prettyprint for the following ops:
    
    ```
    DotOp, RngBitGeneratorOp, RngOp
    ```
    
    Closes #25

[33mcommit 08cb45b35bcdc4856895a6942220811bd26cde53[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 22 20:13:15 2022

    Update prettyprint status in status.md (#184)
    
    These numbers include the changes for enums currently being reviewed (#161) which should be ready for submit shortly.

[33mcommit 67a41fcaa005f3eb19057d03d727c224ddea8bb6[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Sep 22 04:28:38 2022

    Add type inference for stablehlo.if stablehlo.case. (#135)
    
    * Add type inference for stablehlo.if stabehlo.case.
    
    * add unranked and dynamism support for if/case op.
    
    * Address comments.
    
    * Move compatiblity util func and fix type.
    
    * Tiny fix method namespace.

[33mcommit 8269c8e8ab6e17fdb69b3f4849be380aa9fbde42[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 22 03:02:38 2022

    Pretty print for ops with single scalar attributes  (#159)
    
    Details in #21.
    
    Pretty print for the following ops, uses a `key = value` syntax for attributes:
    
    ```
    CholeskyOp, ConcatenateOp, SetDimensionSizeOp, GetDimensionSizeOp, IotaOp, DynamicIotaOp
    ```
    
    Two approaches tried out for disambiguating ConcatenateOp (See comment on #21 for details):
    
    1. Use `with` to disambiguate between last variadic operand and first attribute: https://github.com/openxla/stablehlo/commit/ce6a622d917fa3c18b69227d2f508f092b2ba618
    
    2. Implement a custom parser for variadic operands followed by attributes. There is _technically_ no ambiguity when parsing operands vs attributes with our given format, but the built-in machinery was implemented in a way that expects comma separates lists of operands for variadic operands. We can build a custom implementation which stops at the first non-SSA literal. This will allow us to have a very consistent assembly format. (Current stage of PR).
    
    Approach 2 requires more testing and custom code, but I think that is worth it for a consistent StableHLO grammar.
    
    Closes #21.

[33mcommit 9ba2493d3934fc1ef51554ac07c4bab48dcb1920[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 22 02:46:44 2022

    Add bytecode test for compare_type attr. (#174)
    
    There is currently no bytecode test for `compare_type` in CHLO. See [coverage report](https://htmlpreview.github.io/?https://raw.githubusercontent.com/GleasonK/stablehlo/ccov/reports/ccov_2022_09_21_14-51-15/stablehlo/dialect/ChloBytecode.cpp.gcov.html).
    
    Added tests that uses all attributes of `chlo.broadcast_compare`.

[33mcommit dbd27bc7910f021415878e1e44319768caa97712[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Thu Sep 22 02:45:00 2022

    Populate progress of verifier and type inference. (#168)
    
    The new values in Markdown table of all 114 ops are copied from StableHLO audit sheets with local handy script and the rule to map the old value in sheets to the new value in Markdown table is:
    ```
    convert_value_for_md = {
        'Yes (Declarative)': 'yes*',
        'Yes (Imperative)': 'yes*',
        'Yes (need revisit)': 'yes(need-revisit)',
        'TODO': 'no',
        'Yes (revisit)': 'yes(need-revisit)',
        'No (Infeasible)': 'infeasible',
        'WIP': 'no',
    }
    ```
    The values of `Type Inference` column is clear and fully up to date. The `Verification` column actually means both ODS and `verify()`. As `verify()` are removed for a lot of ops, the value of them are still set to `yes*`.
    If an op's type inference is done or revisited in the future: its value of `Verification` column will also be revisited and updated at the same time (even if it's set to `yes*`) now.
    The sum number in the column title includes all kinds of `yes*` and `infeasible`.

[33mcommit 0f204b21af775d1bb9ea07df9b14f24ac8534036[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 22 00:49:27 2022

    Add additional special test cases for interpret_add.mlir and interpret_constant.mlir (#142)
    
    The current tests do not address all special values for floats.
    
    The list of special values are:
    * +/- 0
    * +/- inf
    * +/- NaN
    * +/- Subnormal
    
    However, the spec does not differentiate signedness of the NaN, so only the +NaN is tested. An investigation will be done in the near future to determine whether the tests include all special cases which are addressed in the spec for all ops in question.

[33mcommit 0c9e261b4bd275253570a5bda798187b23b96c08[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Sep 21 20:39:01 2022

    Use auto-generated pass create functions (#176)

[33mcommit b08a70ced3883994b8c410a6439f7114f4325d85[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 20 21:27:31 2022

    Move warning and error flags into CI build flags. (#160)

[33mcommit 653aa6ce55de959caffe267f5522eda3d840e831[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 20 07:07:11 2022

    fix typos in  AddOp spec (#158)

[33mcommit 2ac7780971543d75f6d15ca71855630f664f29f2[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Sep 20 02:08:22 2022

    remove trailing spaces (#157)

[33mcommit 9d911b3ae380dca5f94fab25580b1d0fe07e673d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 20 00:53:42 2022

    fix a bug in AddOp spec (#156)

[33mcommit 4e64ed15a8db41ae990bed609a48c88cd6d2e055[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Sep 19 20:57:48 2022

    Add interpreter for sine op (#139)
    
    * Add interpreter for sine op
    
    * fix lint errors and modify tests
    
    * rename function name to match stablehlo name (sin -> sine)
    
    * fix syntax
    
    * add additional special values for testing
    
    * default to using double to perform floating-point calculation
    
    * fix floating point precision errors
    
    * Address comments
    
    * address comments
    
    * address comments
    
    * address comments

[33mcommit 55814280317cc6db499bf2137750bcfa46ca452e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Sep 19 20:02:50 2022

    Build with all warnings as errors. (#137)
    
    * Build with all warnings as errors.
    
    * test revert and fix in upstream
    
    * Updated to use LLVM_ENABLE_WARNINGS/LLVM_ENABLE_WERROR.

[33mcommit 59e0ce5791885094d9cdcc239d01e95c33cea5b3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 19 19:46:58 2022

    Switch CHLO and StableHLO to kEmitAccessorPrefix_Both (#150)
    
    https://github.com/openxla/stablehlo/commit/3511b6b2810d2f183ecf2e10c2441cc719874838 was supposed to do this, but I forgot to update TableGen files.

[33mcommit f43b9427a73d5fa2d02a94994421851db2fe4dc9[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Sep 19 16:47:13 2022

    Replace uses of __PRETTY_FUNCTION__ with LLVM_PRETTY_FUNCTION (#149)
    
    Replace uses of __PRETTY_FUNCTION__ with LLVM_PRETTY_FUNCTION

[33mcommit 238b415a63ec79bf37a80e620a67b3dae8c3176e[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Sep 16 23:41:53 2022

    Further tweaking of status.md (#141)
    
    * Fix typos: "FooOP" => "FooOp".
      * Left align the column with op names. I think the table is easier to read this way, but please let me know if you disagree!

[33mcommit 31b8d00a7b37716e9e0d1a780301b52d4b78abf1[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 16 00:38:35 2022

    Remove GEN_PASS_CLASSES (#134)

[33mcommit 6a3c3277e8bfc4de21f4f720f4598665158f74e9[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 16 00:04:29 2022

    updated a link to spec-draft in interpreter.md (#140)
    
    update-spec-link in interpreter.md file

[33mcommit 5c88051f491373b83db526c7031748824d4703c7[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 15 21:21:53 2022

    Add specification for exp operator (#127)
    
    * Add specification for exp operator
    
    * Address comments

[33mcommit a11bb4b7378469bafdc0f3c9d14479e30a38dad4[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 20:40:32 2022

    Format markdown files consistently with MLIR-HLO (#128)
    
    Wrap at 80 characters, use headers correctly (no more than one level-one
    header in one file).

[33mcommit 752eb7907aad9a9cf473be5526ad37344d8275a7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 15 20:36:16 2022

    Integrate LLVM at llvm/llvm-project@70ac46667613 (#132)

[33mcommit 099914e8a862590e0bf882933ac16c46fa59389b[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 19:58:30 2022

    Review feedback for #121 (#136)
    
    * Review feedback for #121
    
    * Forgot to add some more feedback

[33mcommit 74008ba3c10fd41f4ce6b8814a8c0ff12ecc0acc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 15 19:36:57 2022

    Tracking of completeness of the implementation (#121)
    
    * Keep track of completeness of the implementation
    
    * Iteration 1: address review comments
    
    * verifier completeness to be tracked w.r.t sync with XLA and StableHLO semantics

[33mcommit 4954927cf3ea1935707d19c443e556138a7172e7[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 16:46:45 2022

    Backward compat with the old way of registering CHLO (#133)
    
    We aren't promising source compatibility of our APIs yet, but this one is easy to provide, and it was useful for downstream users of MLIR-HLO.

[33mcommit 76dc916940cfe39c1ad951bce75ef8beb21877b0[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 16:26:59 2022

    Remove mlir_runner_utils_dir replacement from lit.cfg.py (#131)
    
    This is going away (or has already gone away) in MLIR upstream.
    Context: https://reviews.llvm.org/D133270.

[33mcommit ed553775a23c8f28ca2acb8fb0d4f0453bcbaf75[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 16:01:10 2022

    Add TableGen targets to mlir-headers (#124)
    
    Without this change, users of dialect headers will have to remember to add these targets to their dependencies, which can be easily forgotten and can lead to hard to reproduce CMake build errors. A similar issue in the MLIR-HLO repository only showed with `ninja -j1`.

[33mcommit 2a48639e914c14affc82942c04335a939db6b33b[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 15:50:39 2022

    Fix build warnings (#126)
    
    We have accumulated quite a few over time, which is not unexpected
    because we don't block on warnings in CI. Related ticket in MLIR-HLO:
    https://github.com/tensorflow/mlir-hlo/issues/43.
    
    In this PR, I'll just fix the warnings, and in #125 we'll discuss
    how to avoid this happening in the future.

[33mcommit 3511b6b2810d2f183ecf2e10c2441cc719874838[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Sep 15 15:27:48 2022

    Migrate away from kEmitAccessorPrefix_Raw (#130)
    
    Raw has been deprecated and will be removed soon. The migration for
    CHLO/StableHLO has started in MLIR-HLO, but the recent internal
    infrastructure changes in tensorflow/mlir-hlo@3227ee1 have rolled it back.
    This progress needs to be restored.

[33mcommit fdd50ff9990a3b07246bf835a677a06928697e40[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 15 01:36:19 2022

    Reorder interpret_add.mlir tests and fix typos in spec_draft.md (#123)
    
    * Reorder interpret_add.mlir tests and fix typos in spec_draft.md
    
    * address comments and fix more typos/grammar

[33mcommit be773acde90b3826f6b20175d6a7de1c42f810fd[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 15 00:47:23 2022

    Add test for constant op (#112)
    
    * Add test for constant op
    
    * update tests to remove splat values
    
    * modify tests to include special case values

[33mcommit b511e33017b0b49c25021d0edfafc98f1cdc38fc[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Wed Sep 14 22:50:12 2022

    Add type inference for stablehlo.map. (#116)
    
    * Add type inference for stablehlo.map.
    
    * address comments for for stablehlo.map.
    
    * Address comments for the 2nd time.

[33mcommit dc597684cc600d65db7bcc594fe82711b655f310[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Wed Sep 14 17:13:45 2022

    Add type inference for stablehlo.triangular_solve. (#114)
    
    * Add type inference for stablehlo.triangular_solve.
    
    * Address comments in stablehlo.triangular_solve.

[33mcommit e3ca65d275e9db8d2e5dd51a565587259d320824[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 13 20:45:30 2022

    Clarification on floating-point exception env. (#111)
    
    * Clarification on floating-point exception env.
    
    * iteration 1: address review comments

[33mcommit 66592b103e8e1ad58c12f35edff8c6de431adde7[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Sep 13 02:39:44 2022

    Add Python API Testing to GitHub Actions (#104)
    
    Add Python API Testing to GitHub Actions.
    
    Closes #62.

[33mcommit 1168307259cab447d2d2e1a88b30a1ece373d2d2[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 13 00:44:57 2022

    Add spec of div/rem (#106)
    
    * Add spec of div/rem
    
    * iteration 1 : Address review comments
    
    * iteration 2 : Address review comments

[33mcommit 2f32aa02c80253598b9a6e3bd4ec21fc3759c518[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Sep 12 23:15:13 2022

    Add type inference for stablehlo.reduce_window (#91)
    
    The new `ReduceWindowOp::inferReturnTypeComponents()` replaces the old `ReduceWindowOp::verify()` with all the checking logic. And expect to cover all XLA checks as well.
    
    And invalid test with `llvm.add` is removed in the same reason as the description of [the PR of "Type inference of ReduceOp"](https://github.com/openxla/stablehlo/pull/50). Please see the reason there.
    
    `ReduceWindowOp` has similar but not same code as `ReduceOp` which is not ideal. So align them to be same in P1~P2 checks.
    
    To address the legacy TODOs, `convertDenseIntAttr` add check for 1-D tensor, and change all caller respectively. Those ops like `SelectAndScatterOp` will be fully addressed in later PRs recently.

[33mcommit 6a06657af96aae40532e7485128ced483adc4766[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Sep 12 20:29:17 2022

    Don't run Build and Test Action if only doc or readme changes. (#113)
    
    Don't run buildAndTest for doc / markdown updates. Closes #110.

[33mcommit 0b86cad38338da9a9dc96fba7dcac5074acb1e3f[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Mon Sep 12 20:12:29 2022

    Add specification for logistic operator (#107)
    
    * Add specification for logistic operator
    
    * fix constraints
    
    * rewrite formula to use IEEE-754 operations

[33mcommit f2fd2aed97a0a6b579cc79303717bc87495d61ad[m
Author: Eugene Burmako <burmako@google.com>
Date:   Mon Sep 12 17:28:40 2022

    Update README.md with a link to GitHub discussions (#109)

[33mcommit 09f6672a66750c26a850b49830f3e80f01d2b41c[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 9 21:20:42 2022

    Minor consistency updates to spec constraints (#108)

[33mcommit e26b3ba523bb3f06d5248f4790cf1b5794074173[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Fri Sep 9 18:52:22 2022

    Add specification for log op (#100)
    
    address comments to update branch and add more context to complex element types

[33mcommit 55bde37e8d3c9c44f070c9a8f0e448f3dc6bddb3[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Sep 9 16:36:31 2022

    Refactor constraints (#103)
    
    - All constraints can be referred to now (this will be useful in the
      future when writing verifiers for more complicated ops).
    - Some constraints are merged into one.
    - "Supported shapes: all static shapes" is redundant at this point,
      because the spec only defines static shapes.

[33mcommit d29de3b46450431468ee46c697dd1993a56aa4bb[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 9 02:14:43 2022

    Remove the redundant fp equality comparison function. (#102)

[33mcommit 05ab854951df5cfc09ddb07ece2f79ff7a174ae0[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Thu Sep 8 18:51:42 2022

    Add specifications for sqrt and rsqrt operators (#89)
    
    * Add specifications for sqrt and rsqrt operators
    
    * address comments to add IEEE-754 specification
    
    * update operation name to match that of IEEE-754
    
    * address comments to add complex element examples and clarify numeric precision

[33mcommit 3b2559c433708cca90593611fade9e10be82ad7d[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Wed Sep 7 19:46:26 2022

    Add specifications for sine, cosine, and tanh operators (#86)
    
    * Add specifications for sine, cosine, and tanh operators
    
    * address comments
    
    * address comments to include op name from IEEE-754 standard

[33mcommit fc68e297ce9d7c286ba4c85860f112f9c96955f3[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 7 19:06:33 2022

    spec-draft: negate/abs (#74)
    
    * spec-draft: negate/abs
    
    * fix review comments
    
    * Iteration 2: fix review comments
    
    * Iteration 3: fix review comments

[33mcommit 7744f77b27bf5034cba53c5a6542a1dcdd5addb8[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 7 01:21:00 2022

    spec-draft: floor/ceil (#72)
    
    * spec-draft: floor/ceil
    
    * Iteration 1: fix review comments

[33mcommit 17728175c4e1cb2ad9562756eb8a03578d03c14d[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Sep 7 00:23:33 2022

    Clarify the floating point semantics of add/max/min (#94)
    
    * Clarify the floating point semantics of add/max/min
    
    * Fixing a few typos
    
    * fix review comments

[33mcommit c544258f7e5353ec66892b26b26a12977f7400e5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Sep 6 19:38:17 2022

    Update the "Discussions" link (#92)
    
    At the GitHub organization level, we've restructured discussions, so
    that they all end up at https://github.com/orgs/openxla/discussions,
    similarly to how we have the OpenXLA Discord server and
    multiple channels within that server.

[33mcommit f96c56d4d889dce51eff1f2977828f5a20945283[m
Author: Eugene Burmako <burmako@google.com>
Date:   Tue Sep 6 19:37:35 2022

    Remove "Additional context" from bug and FR templates (#93)
    
    So far, additional context in our tickets has always been empty.
    Let's remove it for the time being.

[33mcommit aa42ab1aa68d28120ddb288b22b90264bfda6245[m
Author: Gunhyun Park <gunhyun@google.com>
Date:   Tue Sep 6 19:08:32 2022

    Fix examples to include operand and result names (#84)
    
    * minor tweaks to examples and fix typo
    
    * fix typos

[33mcommit 5163c58c0fecfde666062edd7fe86261af733c9c[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Tue Sep 6 06:31:11 2022

    spec: Add an index of documented ops (#87)
    
    * spec: Add an index of documented ops

[33mcommit 8d3ee6e9d17574c73172025e867c447ca998d034[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Tue Sep 6 06:18:34 2022

    Add type inference for stablehlo.batch_norm_* ops. (#67)
    
    This change adds type inference method for `stablehlo.batch_norm_grad`,  `stablehlo.batch_norm_training` and `stablehlo.batch_norm_inference`, and revisit their `verify()`:
    1. Existing comments in `verify()` are verbose and hard to read and maintain: ODS provides clear constrains for those many operands/atts/result and much more readable than the long comments in nature language. Besides, the existing comments also miss some points like element type match for some items. So they are removed, instead, this change states clearly that `verify()`only contains checks beyond ODS.
    2. These ops shares quite similar logic, so refactor by reusing them.
    3. XLA semantics is very long because it includes everything that the ODS of StableHLO already covers. The missing pieces are covered by `verify()`.

[33mcommit 10327c72aa80f4ec743fc6015d0993984ed7fb4d[m
Author: Xin Zhou <zhouxin@google.com>
Date:   Mon Sep 5 23:37:29 2022

    Add type inference for stablehlo.reduce (#50)
    
    The new `ReduceOp::inferReturnTypeComponents()` replaces the old `ReduceOp::verify()` with all the checking logic.
    
    Note that a unit test is removed: it had a `llvm.add` inside its region, and intended to return a scale from its region and catch the error in `ReduceOp::verify()`. Now when we replace verifier with infer function, there will be two errors:
    1. `'llvm.add' op operand #0 must be integer or LLVM dialect-compatible vector of integer, but got 'f32'`: can be fixed by change `f32` -> `i32`
    2. In the last line of region, stablehlo.return supports return tensor only, instead of scalar: No way to fix this.
    
    The HEAD code tolerated these errors because the `ReduceOp::verify()` fails first. Now decide to remove this unit test which is a case that will not happen.

[33mcommit b63a5197ce5cca991bb90ffd38bdda4380594676[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Sat Sep 3 14:38:02 2022

    CHLO Bytecode Implementation (#66)
    
    - CHLO Bytecode implementation
    - Moved common machinery to Base.h

[33mcommit 2ac02215d90b26572760cd198a02c7ced1e811a7[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Sat Sep 3 00:11:22 2022

    interpreter: Fix a few outdated links in interpreter.md file (#78)
    
    interpreter: Fix the outdated links in interpreter.md file

[33mcommit e947f10d45003669fa7cb6e42efb125b45041389[m
Author: Gunhyun Park <ghpvnist28@gmail.com>
Date:   Fri Sep 2 23:15:09 2022

    Add specifications for constant operator (#65)
    
    * Add specifications for constant operator
    
    * Add specifications for constant operator
    
    * Fix merge conflicts
    
    * Address comments

[33mcommit 5b9a5fe55ad69e8c8d401af7dbe93986eab56034[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Fri Sep 2 23:02:54 2022

    Interpreter skeleton (#44)
    
    Here is the skeleton of interpreter supporting a single stablehlo.add op.

[33mcommit e408d65a348c385eb2e455e3ba75e2354d61b21d[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 2 17:07:51 2022

    Implement pretty printing for SelectOp, ClampOp, AfterAllOp types. Clean up type prints. (#37)
    
    ```
    %0 = stablehlo.select %arg0, %arg1, %arg1 : tensor<2x3xi1>, tensor<2x3xi32>
    %0 = stablehlo.clamp %arg0, %arg0, %arg0 : tensor<4xf32>
    %5 = stablehlo.after_all %arg1, %arg1 : !stablehlo.token
    %6 = stablehlo.after_all : !stablehlo.token
    ```

[33mcommit 4f7fe55996c6be38a7b2eac4bdbeee7bb54f6ea6[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Fri Sep 2 00:29:47 2022

    Bytecode implementation for StableHLO (#60)
    
    Bytecode implementation for StableHLO.
    
    - Implementation of bytecode in `StablehloBytecode.cpp`
    - Documentation of work done/todo, as well as how to serialize new types/attrs in `bytecode.md`
    - Round trip testing and comparison with non-bytecoded mlir in `ops_stablehlo_roundtrip.mlir`

[33mcommit d805e421d01d22fd18be9ec66c243a1233111bfc[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 1 15:32:12 2022

    Add .vscode to gitignore (#68)
    
    VS Code makes a directory `.vscode/` with json configurations in it for C++ extensions. If we end up wanting certain configs for vs code to be stored in this repo, we can edit this later, for now will save a lot of trouble to ignore everything.

[33mcommit 7248c1b55c4efb8fe6cb9985c0623e822a9f9342[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Thu Sep 1 15:24:53 2022

    Implement GitHub Actions for lint and buildAndTest (#49)
    
    Implement GitHub Actions for lint and buildAndTest on Linux

[33mcommit 2a2fb97bf6c61e8d28ca5268dce64ea8c6a8cb65[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Thu Sep 1 00:41:51 2022

    Add specifications for and, or, xor, not operators (#54)
    
    * Add specification for StableHLO ops: and,or,xor,not

[33mcommit 0699b8aaa147cf2448f50ccc21e48aea44bc978e[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 31 23:19:22 2022

    Implement pretty print for CustomCallOp, ReducePrecisionOp (#38)
    
    Implement pretty print for ReducePrecisionOp, CustomCallOp

[33mcommit 1ce73e81beebf3fd70e4d8acaf03ef4e897d9ffc[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Aug 31 23:10:53 2022

    Add specifications for maximum and minimum operators (#56)
    
    Add spefication for StableHLO ops: maximum and minimum

[33mcommit 9ca259d5092e9cf2c1fa0788a470df6a4fc95f0a[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Mon Aug 29 22:41:52 2022

    Integrate LLVM at llvm/llvm-project@6732896bbf0c (#59)
    
    bump llvm revision to use bytecode machinery

[33mcommit 2ebb4cee0f9eff5468271cbe2d7b0ce9e35e91e6[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Mon Aug 29 18:00:32 2022

    clarified the semantics of 0 dimension of a tensor (#52)

[33mcommit d88d0fe0f839aa1fe350dd777375e367bbb30cac[m
Author: Eugene Burmako <burmako@google.com>
Date:   Fri Aug 26 06:53:50 2022

    Prefix includes with "stablehlo" (#48)
    
    At the moment, our includes are not prefixed with the project name, e.g.
    `#include dialect/ChloOps.h`. Now that StableHLO is starting to get used,
    we should change that to `#include stablehlo/dialect/ChloOps.h` to
    disambiguate with other projects.
    
    Fixes #46.

[33mcommit 98ca6bb86a412bec689a2cc17e0b23fc4a63c696[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Aug 25 12:57:43 2022

    Add Python bindings for CHLO/StableHLO (#40)
    
    In this pull request, I took the existing Python bindings for MLIR-HLO
    and mostly copy/pasted them into the StableHLO repository.
    
    One change from MLIR-HLO is that we have two separate bindings for
    CHLO and StableHLO in the StableHLO repository. This follows the recent
    trend in both repositories where we severed the dependency from CHLO
    to MHLO.

[33mcommit 5d1e783987d71d306f74c763bd6d10a50958bceb[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Wed Aug 24 22:09:07 2022

    Allow blank issues to be created. (#41)
    
    Allow blank issues.

[33mcommit 2fc088f9d696e86eb0b6728246b30f0fee64d3a5[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Aug 24 20:35:15 2022

    Allows StableHLO to be built as part of other CMake projects (#39)
    
    This is similar to how things are done in Clang: http://google3/third_party/llvm/llvm-project/clang/CMakeLists.txt;l=3-13;rcl=469417476.
    Thank you, Mehdi, for the tip!
    
    Backported from https://github.com/tensorflow/mlir-hlo/commit/32be07f12c82144965fd90671ca7c742e5c5f3f0

[33mcommit 27259786691d11c7bb4b12949765d539798c8491[m
Author: Sandeep Dasgupta <sdasgup@google.com>
Date:   Wed Aug 24 20:27:42 2022

    Skeleton of the StableHLO spec (#23)
    
    Add a skeleton of the StableHLO spec
    
    Here is the skeleton of the StableHLO specification: Presented as a draft to signify that this is a preliminary version of the specification and will follow additions in future w.r.t different ops.

[33mcommit dcdcfe4be837fe67c8ab0d78e3bcfee7001d7efe[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 23 23:34:36 2022

    Pretty printing for StableHLO types. (#29)
    
    Pretty printing for StableHLO types.
    
    Added the following custom print directives, and used them in `StablehloOps.td`:
    
    ```
    custom<TupleOpType>(...)
    custom<PairwiseOpType>(...)
    custom<SameOperandsAndResultType>(...)
    ```
    
    Removed `parse/printUnaryOp` and `parse/printBinaryOp` functions, and re-wrote them in declarative format since both can now use `custom<SameOperandsAndResultType>(...)`. Unary/Binary-ops now consistently do not have parentheses around operands.
    
    There are a few other ops that can use this SameOperandsAndResultType printer/parser (ClampOp, AfterAll, other ops with single output that _may_ have a single operand type), but since this CL was already getting sizable, I think it is best to split those into a separate CL.
    
    Changed `optimization_barrier` to use `PairwiseOpType` printer/parser, and `tuple` to use the `TupleOpType` printer. Both these ops print their result type only, since the operand types are trivially inferable.
    
    Also moved `tests/print_binary_invalid.mlir` to `tests/print_types_invalid.mlir` and added tests for `Tuple/Pairwise/SameOperandAndResultType` invalid cases. Created `tests/ops_stablehlo_prettyprint.mlir` for testing the custom assembly of ops as pretty printing gets enabled.
    
    Closes #19

[33mcommit e57ac9fcf75b7dbfbd4b1beb6d2e2208b9c05975[m
Author: Kevin Gleason <gleasonk@google.com>
Date:   Tue Aug 23 23:34:09 2022

    Add issue templates. Based on IREE templates. (#28)
    
    Add issue templates. Based on IREE templates.
    
    Adding some more community friendly issue templates. I believe much of the early work will be completed using blank templates for tasks, but as users begin to use this repo, a bug_report template will come in handy. Also a link to our discord for users encountering issues is helpful too.
    
    Based on IREE templates:
    https://github.com/iree-org/iree/issues/new/choose
    https://github.com/iree-org/iree/tree/main/.github/ISSUE_TEMPLATE

[33mcommit 7e8a41c1b9550759ab4dbacc43b25d02c1f191bc[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Aug 17 20:34:08 2022

    Make the "Roadmap" section in the README more specific (#18)
    
    Now that we have bootstrapped the repository and created initial tickets to keep track of ongoing work, let's reflect that in the roadmap.

[33mcommit d6c918d9e2eec74b43b198cbc102398f40e7a981[m
Author: Eugene Burmako <burmako@google.com>
Date:   Wed Aug 17 09:44:11 2022

    Bootstrap StableHLO from CHLO/MHLO (#1)
    
    * Bootstrap StableHLO from CHLO/MHLO
    
    Motivation
    ----------
    
    [Recent discussions](https://discourse.llvm.org/t/coordinate-llvm-commits-for-different-project/63990)
    highlight an acute need for stability of interchange dialects in between
    ML frameworks and ML compilers in the opensource community.
    
    In [a Discourse post](https://discourse.llvm.org/t/coordinate-llvm-commits-for-different-project/63990/7),
    silvasean@ called out a potential solution - something called "shallow dialects"
    that producers could vendor into their repositories and upgrade with
    well-defined backward compatibility windows.
    
    I think this presents a great opportunity for StableHLO: to start as shallow MHLO
    which will bootstrap us from a well-understood baseline and will enable us to
    provide a service to the community right away - backward compatibility
    guarantees for MHLO (and its sister dialect CHLO as well).
    
    This doesn't mean that the scope of StableHLO should be limited by the scope of
    CHLO/MHLO. Bootstrapping from CHLO/MHLO is just the beginning of the journey,
    and there are a lot of ideas to explore to evolve StableHLO beyond that.
    (See "Future work" below for some of these ideas).
    
    Summary
    -------
    
    All of the CHLO ops are forked into CHLO ops in this repository.
    
    Most of the MHLO ops are forked into StableHLO ops in this repository
    (except for the 8 ops which are private to XLA, i.e. don't seem to be created by
    existing producers and are only created inside XLA itself: `add_dependency`,
    `bitcast`, `copy`, `domain`, `fusion`, `partition_id`, `tuple`,
    `xla.rng_get_and_update_state`).
    For further details, see the categorization of MHLO ops at:
    https://discourse.llvm.org/t/rfc-proposal-for-a-high-level-ml-dialect-in-mlir/64249/46.
    
    These forks are "shallow", i.e. they include only essential
    functionality for producers. All folders and canonicalizers are dropped
    (except for `ConstantOp` folders which are necessary to satisfy the
    `ConstantLike` trait). No passes are forked, except for a pass which helps
    test shape inference.
    
    This uses MLIR-HLO at revision eb0a4d2ae01e9a39dfa079648303d9f3576b096a:
    https://github.com/tensorflow/mlir-hlo/commit/eb0a4d2ae01e9a39dfa079648303d9f3576b096a
    If we decide to proceed with this pull request, we can have a discussion which
    commit from MLIR-HLO to fork from (e.g. we might want to use the MLIR-HLO HEAD
    at that time).
    
    Details
    -------
    
    1) `hlo_ops_base.{h,td}` are forked into `Base.{h,td}` and include only the
    functionality which is shared between CHLO and StableHLO.
    
    2) `hlo_ops_base_attrs.td` is forked into `StablehloAttrs.td` because it's
    not used in CHLO. `hlo_ops_base_enums.td` is forked into a combination of
    `ChloEnums.td` and `StablehloEnums.td`. `ComparisonType` and
    `ComparisonDirection` now exist in both CHLO and StableHLO, to avoid
    a dependency from the former on the latter.
    
    3) `hlo_ops_common.h` is merged into `StablehloOps.h`. The "common" part
    is referring to this functionality shared between MHLO and LMHLO
    dialects, but LMHLO already depends on MHLO anyway, so there's no need
    to have a separate abstraction for this.
    
    4) All dependencies from CHLO to MHLO are severed (via introducing
    some extra functions in `Base.h`, creating `chlo.constant` and forking the
    enums described above from MHLO to CHLO), so that CHLO and StableHLO are
    two independent dialects now.
    
    5) I also think it will also be a good idea to move 10 decomposable ops:
    `batch_norm_grad`, `batch_norm_inference`, `batch_norm_training, broadcast`,
    `create_token`, `cross-replica-sum`, `dot`, `einsum`, `torch_index_select`,
    `unary_einsum` - into CHLO, but that is left for future work.
    
    Future work
    -----------
    
    A) As the next step, I propose that we vendor this repository into
    MLIR-HLO (with the plan to regularly integrate changes into MLIR-HLO)
    and introduce a conversion from StableHLO to MHLO in order to connect
    StableHLO producers and MHLO consumers.
    
    B) That would enable MHLO producers to consider experimenting with StableHLO
    by changing the library they are depending on (from MHLO to StableHLO) and
    changing `mhlo.` and `mhlo::` references to `stablehlo.` and `stablehlo::`.
    We're talking with JAX, TF/XLA and Torch-MLIR teams about these experiments.
    
    C) Speaking of compatibility guarantees, I propose that we initially provide
    them at the boundary between StableHLO and MHLO (so that StableHLO provides
    stability, and MHLO can keep evolving mostly freely). Potential MLIR upstream
    functionality along the lines of [this patch](https://reviews.llvm.org/D117761]
    would enable StableHLO to be versioned.
    
    D) In the longer term, StableHLO work can be structured into three workstreams:
      * Workstream #1: Stable version of HLO/MHLO.
        Specification, test suite, reference implementation - ETA: H2 2022.
      * Workstream #2: Evolution beyond what's currently in HLO/MHLO.
        Ongoing work on dynamism, sparsity, quantization and extensibility -
        ETA: H2 2022.
      * Workstream #3: Adoption of StableHLO.
        Support for ML frameworks (TensorFlow, JAX, PyTorch) and ML compilers
        (XLA and IREE) - ETA: H2 2022.
    
    E) This repository is just getting bootstrapped. In the future, work items will
    be tracked via Issues / Projects and perhaps other documentation.

[33mcommit 6b1711bdcf16ca1fca3dd588ea801f1d3d67d3e8[m
Author: Eugene Burmako <burmako@google.com>
Date:   Thu Aug 4 00:30:22 2022

    Initial commit
