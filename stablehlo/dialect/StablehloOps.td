/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
   Copyright 2022 The StableHLO Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#ifndef STABLEHLO_DIALECT_STABLEHLO_OPS
#define STABLEHLO_DIALECT_STABLEHLO_OPS

include "mlir/Dialect/Shape/IR/ShapeBase.td"
include "mlir/IR/OpBase.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/OpAsmInterface.td"
include "stablehlo/dialect/Base.td"

def StableHLO_Dialect : Dialect {
  let name = "stablehlo";
  let cppNamespace = "::mlir::stablehlo";

  let description = [{
    StableHLO is an operation set that expresses ML computations. It has been
    originally bootstrapped from the MHLO dialect and enhances it with additional
    functionality, including serialization and versioning, to be used as
    a portability layer between ML frameworks and ML compilers.
  }];

  let useDefaultAttributePrinterParser = 0;
  let useDefaultTypePrinterParser = 0;
}

class StableHLO_Op<string mnemonic, list<Trait> traits> :
    Op<StableHLO_Dialect, mnemonic, traits> {
  let extraClassDeclaration = [{
    // Relax the strict default implementation with one that allows
    // for StableHLO-specific differences.
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];
}

include "stablehlo/dialect/StablehloEnums.td"
include "stablehlo/dialect/StablehloAttrs.td"

class StableHLO_ShapedInterfaceOp<string mnemonic, list<Trait> traits> :
    StableHLO_Op<mnemonic, traits # [DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
    ["reifyReturnTypeShapes"]>]> {
  let extraClassDeclaration = [{
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO nullary op definitions.
//===----------------------------------------------------------------------===//

def StableHLO_ConstantOp : StableHLO_Op<"constant",
    [ConstantLike, Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Constant operator";
  let description = [{
    Produces an `output` tensor from a constant `value`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloconstant

    Example:
    ```mlir
    %output = stablehlo.constant dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32>
    ```
  }];
  let arguments = (ins
    ElementsAttr:$value
  );

  let results = (outs
    HLO_StaticShapeTensor:$output
  );

  let builders = [
    OpBuilder<(ins "Attribute":$value)>];

  let hasCustomAssemblyFormat = 1;
  let hasFolder = 1;

  let extraClassDeclaration = [{
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
  }];
}

def StableHLO_IotaOp : StableHLO_Op<"iota", [Pure]> {
  let summary = "Iota operator";
  let description = [{
    Fills an `output` tensor with values in increasing order starting from zero
    along the `iota_dimension` dimension.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloiota

    Example:
    ```mlir
    %output = stablehlo.iota dim = 0 : tensor<4x5xi32>
    ```
  }];
  let arguments = (ins I64Attr:$iota_dimension);

  let results = (outs HLO_IntFpOrComplexTensor:$output);

  let hasVerifier = 1;

  let assemblyFormat = "`dim` `=` $iota_dimension attr-dict `:` type($output)";
}

def StableHLO_DynamicIotaOp: StableHLO_ShapedInterfaceOp<"dynamic_iota", [Pure]> {
  let summary = "Create linear increasing values from 0 to length -1.";
  let description = [{
    Produces an HLO Tensor of the specified shape, with an incremental set of
    values along the specified dimension starting at 0.

    Requires:
    - The output length of the tensor result.

    Example:

    ```mlir
    %0 = stablehlo.dynamic_iota %arg0, dim = 0 : (tensor<1xindex>) -> tensor<4xi32>
    ```
  }];

  let arguments = (ins HLO_DimensionTensor:$output_shape, I64Attr:$iota_dimension);
  let results = (outs HLO_Tensor:$result);

  let assemblyFormat = [{
    $output_shape `,` `dim` `=` $iota_dimension attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_CreateTokenOp : StableHLO_Op<"create_token", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Create Token operator";

  let description = [{
    Produces a HLO token. Tokens are used for ordering side-effecting operations.
    This is exported to HLO as an AfterAll operation with no operands to
    generate a token.

    Example:

    ```mlir
    %1 = stablehlo.create_token : !stablehlo.token
    ```
  }];

  let results = (outs HLO_Token:$output);

  let assemblyFormat = "attr-dict `:` type(results)";
}

//===----------------------------------------------------------------------===//
// StableHLO unary elementwise op definitions.
//===----------------------------------------------------------------------===//
// See https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions

class StableHLO_UnaryElementwiseOp<string mnemonic, list<Trait> traits,
    Type OperandType, Type ResultType = OperandType> : StableHLO_Op<mnemonic, traits # [Elementwise,
    InferShapedTypeOpInterface, SameOperandsAndResultShape]> {
  let arguments = (ins OperandType:$operand);
  let results = (outs ResultType:$result);
  let extraClassDeclaration = [{
    LogicalResult reifyReturnTypeShapes(
        OpBuilder& builder, ValueRange operands,
        SmallVectorImpl<Value>& reifiedReturnShapes) {
      return ::mlir::hlo::deriveShapeFromOperand(&builder, getOperation(),
                                                operands.front(),
                                                &reifiedReturnShapes);
    }
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];

  let assemblyFormat = [{
    $operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))
  }];
}

// Abs supports complex to real, so element type is not guaranteed to match.
def StableHLO_AbsOp: StableHLO_UnaryElementwiseOp<"abs",
    [Pure,
     DeclareOpInterfaceMethods<InferTypeOpInterface>],
     TensorOf<[HLO_SInt, HLO_Float, HLO_Complex]>> {
  let summary = "Absolute value operator";
  let description = [{
    Performs element-wise absolute value of `operand` tensor and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloabs

    Example:
    ```mlir
    %result = stablehlo.abs %operand : tensor<3xi32>
    ```
  }];
}

def StableHLO_CbrtOp: StableHLO_UnaryElementwiseOp<"cbrt",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpTensor> {
  let summary = "Cubic root operator";
  let description = [{
    Performs element-wise cubic root operation on `operand` tensor and produces
    a `result` tensor, implementing the `rootn(x, 3)` operation from the
    IEEE-754 specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocbrt

    Example:
    ```mlir
    %result = stablehlo.cbrt %operand : tensor<4xf32>
    ```
  }];
}

def StableHLO_CeilOp: StableHLO_UnaryElementwiseOp<"ceil",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpTensor> {
  let summary = "Ceil operator";
  let description = [{
    Performs element-wise ceil of `operand` tensor and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloceil

    Example:
    ```mlir
    %result = stablehlo.ceil %operand : tensor<5xf32>
    ```
  }];
}

def StableHLO_ConvertOp : StableHLO_UnaryElementwiseOp<"convert",
    [Pure, SameOperandsAndResultShape], HLO_Tensor> {
  let summary = "Convert operator";
  let description = [{
    Performs element-wise conversion of values from one type to another, e.g.
    float to int.

    See https://www.tensorflow.org/xla/operation_semantics#convertelementtype.

    Example:

    ```mlir
    %0 = stablehlo.convert %arg0 : (tensor<2xi32>) -> tensor<2xf32>
    ```
  }];
  let builders = [
    OpBuilder<(ins "Value":$operand, "Type":$result_element_ty)>];
}

def StableHLO_ClzOp: StableHLO_UnaryElementwiseOp<"count_leading_zeros",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_IntTensor> {
  let summary = "Count-leading-zeros (Clz) operator";
  let description = [{
    Performs element-wise count of the number of leading zero bits in the
    `operand` tensor and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocount_leading_zeros

    Example:
    ```mlir
    %result = stablehlo.count_leading_zeros %operand : tensor<2x2xi8>
    ```
  }];
}

def StableHLO_CosineOp: StableHLO_UnaryElementwiseOp<"cosine",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Cos operator";
  let description = [{
    Performs element-wise cosine operation on `operand` tensor and produces a
    `result` tensor, implementing the `cos` operation from the IEEE-754
    specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocosine

    Example:
    ```mlir
    %result = stablehlo.cosine %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_ExpOp: StableHLO_UnaryElementwiseOp<"exponential",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Exponential operator";
  let description = [{
    Performs element-wise exponential operation on `operand` tensor and produces
    a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloexponential

    Example:
    ```mlir
    %result = stablehlo.exponential %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_Expm1Op: StableHLO_UnaryElementwiseOp<"exponential_minus_one",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Exponential minus one operator";
  let description = [{
    Performs element-wise exponential minus one operation on `operand` tensor
    and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloexponential_minus_one

    Example:
    ```mlir
    %result = stablehlo.exponential_minus_one %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_FloorOp: StableHLO_UnaryElementwiseOp<"floor",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpTensor> {
  let summary = "Floor operator";
  let description = [{
    Performs element-wise floor of `operand` tensor and produces a `result`
    tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlofloor

    Example:
    ```mlir
    %result = stablehlo.floor %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_ImagOp: StableHLO_UnaryElementwiseOp<"imag",
    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>],
    HLO_FpOrComplexTensor, HLO_FpTensor> {
  let summary = "Imag operator";
  let description = [{
    Extracts the imaginary part, element-wise, from the `operand` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloimag

    Example:
    ```mlir
    %result = stablehlo.imag %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>
    ```
  }];
}

def StableHLO_IsFiniteOp: StableHLO_UnaryElementwiseOp<"is_finite", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>], HLO_Tensor> {
  let summary = "IsFinite operator";
  let description = [{
    Performs element-wise check whether the value in `x` is finite (i.e. is
    neither +Inf, -Inf, nor NaN) and produces a `y` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlois_finite

    Example:
    ```mlir
    %y = stablehlo.is_finite %x : (tensor<7xf32>) -> tensor<7xi1>
    ```
  }];
  let arguments = (ins HLO_FpTensor:$x);
  let results = (outs HLO_PredTensor:$y);

  let assemblyFormat = [{
    operands attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_LogOp: StableHLO_UnaryElementwiseOp<"log",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Logarithm operator";
  let description = [{
    Performs element-wise logarithm operation on `operand` tensor and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlolog

    Example:
    ```mlir
    %result = stablehlo.log %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_Log1pOp: StableHLO_UnaryElementwiseOp<"log_plus_one",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Log1p operator";
  let description = [{
    Computes element-wise logarithm of `operand` tensor plus one and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlolog_plus_one

    Example:
    ```mlir
    %result = stablehlo.log_plus_one %operand : tensor<6xf32>
    ```
  }];
}

def StableHLO_LogisticOp: StableHLO_UnaryElementwiseOp<"logistic",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Logistic operator";
  let description = [{
    Performs element-wise logistic (sigmoid) function on `operand` tensor and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlologistic

    Example:
    ```mlir
    %result = stablehlo.logistic %operand : tensor<2x2xf32>
    ```
  }];
}

def StableHLO_NotOp: StableHLO_UnaryElementwiseOp<"not",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_PredOrIntTensor> {
  let summary = "Not operator";
  let description = [{
    Performs element-wise bitwise NOT of tensor `operand` of type integer and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlonot

    Example:
    ```mlir
    %result = stablehlo.not %operand : tensor<5x3x1xi1>
    ```
  }];
}

def StableHLO_NegOp: StableHLO_UnaryElementwiseOp<"negate",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_IntFpOrComplexTensor> {
  let summary = "Negation operator";
  let description = [{
    Performs element-wise negation of `operand` tensor and produces a `result`
    tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlonegate

    Example:
    ```mlir
    %result = stablehlo.negate %operand : tensor<2x3xi32>
    ```
  }];
}

def StableHLO_PopulationCountOp: StableHLO_UnaryElementwiseOp<"popcnt",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_IntTensor> {
  let summary = "PopulationCount operator";
  let description = [{
    Performs element-wise count of the number of bits set in the `operand`
    tensor and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlopopcnt

    Example:
    ```mlir
    %result = stablehlo.popcnt %operand : tensor<4xi8>
    ```
  }];
}

def StableHLO_RealOp: StableHLO_UnaryElementwiseOp<"real",
    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>],
    HLO_FpOrComplexTensor, HLO_FpTensor> {
  let summary = "Real operator";
  let description = [{
    Extracts the real part, element-wise, from the `operand` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreal

    Example:
    ```mlir
    %result = stablehlo.real %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>
    ```
  }];
}

def StableHLO_RoundOp: StableHLO_UnaryElementwiseOp<"round_nearest_afz",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpTensor> {
  let summary = "Round operator, ties away from zero";
  let description = [{
    Performs element-wise rounding towards the nearest integer, breaking ties
    away from zero, on the `operand` tensor and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloround_nearest_afz

    Example:
    ```mlir
    %result = stablehlo.round_nearest_afz %operand : tensor<5xf32>
    ```
  }];
}

def StableHLO_RoundNearestEvenOp: StableHLO_UnaryElementwiseOp<"round_nearest_even",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpTensor> {
  let summary = "Round operator, ties to even";
  let description = [{
    Performs element-wise rounding towards the nearest integer, breaking ties
    towards the even integer, on the `operand` tensor and produces a `result`
    tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloround_nearest_even

    Example:

    ```mlir
    %result = stablehlo.round_nearest_even %operand : tensor<5xf32>
    ```
  }];
}

def StableHLO_RsqrtOp: StableHLO_UnaryElementwiseOp<"rsqrt",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Reciprocal Square-root operator";
  let description = [{
    Performs element-wise reciprocal square root operation on `operand` tensor
    and produces a `result` tensor, implementing the `rSqrt` operation from the
    IEEE-754 specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlorsqrt

    Example:
    ```mlir
    %result = stablehlo.rsqrt %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_SignOp: StableHLO_UnaryElementwiseOp<"sign",
    [Pure, HLO_CompatibleOperandsAndResultType],
    TensorOf<[HLO_SInt, HLO_Float, HLO_Complex]>> {
  let summary = "Sign operator";
  let description = [{
    Returns the sign of the `operand` element-wise and produces a `result`
    tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlosign

    Example:
    ```mlir
    %result = stablehlo.sign %operand : tensor<7xf32>
    ```
  }];
}

def StableHLO_SineOp: StableHLO_UnaryElementwiseOp<"sine",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Sin operator";
  let description = [{
    Performs element-wise sine operation on `operand` tensor and produces a
    `result` tensor, implementing the `sin` operation from the IEEE-754
    specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlosine

    Example:
    ```mlir
    %result = stablehlo.sine %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_SqrtOp: StableHLO_UnaryElementwiseOp<"sqrt",
    [Pure, HLO_CompatibleOperandsAndResultType], HLO_FpOrComplexTensor> {
  let summary = "Square-root operator";
  let description = [{
    Performs element-wise square root operation on `operand` tensor and produces
    a `result` tensor, implementing the `squareRoot` operation from the IEEE-754
    specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlosqrt

    Example:
    ```mlir
    %result = stablehlo.sqrt %operand : tensor<2xf32>
    ```
  }];
}

def StableHLO_TanhOp: StableHLO_UnaryElementwiseOp<"tanh",
    [Pure, HLO_CompatibleOperandsAndResultType],
    HLO_FpOrComplexTensor> {
  let summary = "Tanh operator";
  let description = [{
    Performs element-wise tanh operation on `operand` tensor and produces a
    `result` tensor, implementing the `tanh` operation from the IEEE-754
    specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlotanh

    Example:
    ```mlir
    %result = stablehlo.tanh %operand : tensor<2xf32>
    ```
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO binary elementwise op definitions.
//===----------------------------------------------------------------------===//
// See https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations

class StableHLO_BinaryElementwiseOp<string mnemonic, list<Trait> traits> :
    StableHLO_Op<mnemonic, traits # [InferShapedTypeOpInterface,
    SameOperandsAndResultShape, Elementwise]> {
  let arguments = (ins
    HLO_Tensor:$lhs,
    HLO_Tensor:$rhs
  );

  let extraClassDeclaration = [{
    LogicalResult reifyReturnTypeShapes(
        OpBuilder& builder, ValueRange operands,
        SmallVectorImpl<Value>& reifiedReturnShapes) {
      return ::mlir::hlo::deriveShapeFromOperand(&builder, getOperation(),
                                                 operands.front(),
                                                 &reifiedReturnShapes);
    }
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];

  let results = (outs HLO_Tensor:$result);

  let assemblyFormat = [{
    $lhs `,` $rhs attr-dict
      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))
  }];
}

def StableHLO_AddOp : StableHLO_BinaryElementwiseOp<"add",
      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Addition operator";
  let description = [{
    Performs element-wise addition of two tensors `lhs` and `rhs` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloadd

    Example:
    ```mlir
    %result = stablehlo.add %lhs, %rhs : tensor<2x2xi32>
    ```
  }];
}

def StableHLO_Atan2Op : StableHLO_BinaryElementwiseOp<"atan2",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Atan2 operator";
  let description = [{
    Performs element-wise atan2 operation on `lhs` and `rhs` tensor and produces
    a `result` tensor, implementing the `atan2` operation from the IEEE-754
    specification.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloatan2

    Example:
    ```mlir
    %result = stablehlo.atan2 %lhs, %rhs : tensor<3xf32>
    ```
  }];
}

def StableHLO_ComplexOp: StableHLO_BinaryElementwiseOp<"complex", [Pure,
    SameOperandsElementType, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Complex operator";
  let description = [{
    Performs element-wise conversion to a complex value from a pair of real and
    imaginary values, `lhs` and `rhs`, and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocomplex

    Example:
    ```mlir
    %result = stablehlo.complex %lhs, %rhs : tensor<2xcomplex<f32>>
    ```
  }];
  let arguments = (ins HLO_Fp32Or64Tensor:$lhs, HLO_Fp32Or64Tensor:$rhs);
  let results = (outs HLO_ComplexTensor:$result);

  let assemblyFormat = [{
    operands attr-dict
      `:` custom<ComplexOpType>(type($lhs), type($rhs), type($result))
  }];
}

def StableHLO_DivOp : StableHLO_BinaryElementwiseOp<"divide",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Division operator";
  let description = [{
    Performs element-wise division of dividend `lhs` and divisor `rhs` tensors
    and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlodivide

    Example:
    ```mlir
    %result = stablehlo.divide %lhs, %rhs : tensor<2xi32>
    ```
  }];
}

def StableHLO_MaxOp : StableHLO_BinaryElementwiseOp<"maximum",
      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Maximum operator";
  let description = [{
    Performs element-wise max operation on tensors `lhs` and `rhs` and produces
    a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlomaximum

    Example:
    ```mlir
    %result = stablehlo.maximum %lhs, %rhs : tensor<4xf32>
    ```
  }];
}

def StableHLO_MinOp : StableHLO_BinaryElementwiseOp<"minimum",
      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Minimum operator";
  let description = [{
    Performs element-wise min operation on tensors `lhs` and `rhs` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlominimum

    Example:
    ```mlir
    %result = stablehlo.minimum %lhs, %rhs : tensor<4xf32>
    ```
  }];
}

def StableHLO_MulOp : StableHLO_BinaryElementwiseOp<"multiply",
      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Multiplication operator";
  let description = [{
    Performs element-wise product of two tensors `lhs` and `rhs` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlomultiply

    Example:
    ```mlir
    %result = stablehlo.multiply %lhs, %rhs : tensor<2xi32>
    ```
  }];
}

def StableHLO_PowOp : StableHLO_BinaryElementwiseOp<"power",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Power operator";
  let description = [{
    Performs element-wise exponentiation of `lhs` tensor by `rhs` tensor and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlopower

    Example:
    ```mlir
    %result = stablehlo.power %lhs, %rhs : tensor<6xf32>
    ```
  }];
}

def StableHLO_RemOp : StableHLO_BinaryElementwiseOp<"remainder",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Remainder operator";
  let description = [{
    Performs element-wise remainder of dividend `lhs` and divisor `rhs` tensors
    and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloremainder

    Example:
    ```mlir
    %result = stablehlo.remainder %lhs, %rhs : tensor<4xi64>
    ```
  }];
}

def StableHLO_ShiftLeftOp : StableHLO_BinaryElementwiseOp<"shift_left",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Shift Left operator";
  let description = [{
    Performs element-wise left-shift operation on the `lhs` tensor by `rhs`
    number of bits and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloshift_left

    Example:
    ```mlir
    %result = stablehlo.shift_left %lhs, %rhs : tensor<6xi8>
    ```
  }];
}

def StableHLO_ShiftRightArithmeticOp : StableHLO_BinaryElementwiseOp<"shift_right_arithmetic",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Shift right arithmetic operator";
  let description = [{
    Performs element-wise arithmetic right-shift operation on the `lhs` tensor
    by `rhs` number of bits and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloshift_right_arithmetic

    Example:
    ```mlir
    %result = stablehlo.shift_right_arithmetic %lhs, %rhs : tensor<6xi8>
    ```
  }];
}

def StableHLO_ShiftRightLogicalOp : StableHLO_BinaryElementwiseOp<"shift_right_logical",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Shift right logical operator";
  let description = [{
    Performs element-wise logical right-shift operation on the `lhs` tensor by
    `rhs` number of bits and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloshift_right_logical

    Example:
    ```mlir
    %result = stablehlo.shift_right_logical %lhs, %rhs : tensor<6xi8>
    ```
  }];
}

def StableHLO_SubtractOp : StableHLO_BinaryElementwiseOp<"subtract",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Subtraction operator";
  let description = [{
    Performs element-wise subtraction of two tensors `lhs` and `rhs` and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlosubtract

    Example:
    ```mlir
    %result = stablehlo.subtract %lhs, %rhs : tensor<2xi32>
    ```
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO binary logical elementwise op definitions.
//===----------------------------------------------------------------------===//

// See https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations
class StableHLO_BinaryBiwiseOrLogicalElementwiseOp<string mnemonic> :
        StableHLO_BinaryElementwiseOp<mnemonic,
          [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
  let arguments = (ins
    HLO_PredOrIntTensor:$lhs,
    HLO_PredOrIntTensor:$rhs
  );
}

def StableHLO_AndOp: StableHLO_BinaryBiwiseOrLogicalElementwiseOp<"and"> {
  let summary = "And operator";
  let description = [{
    Performs element-wise bitwise or logical AND of two tensors `lhs` and `rhs`
    and produces a `result` tensor

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloand

    Example:
    ```mlir
    %result = stablehlo.and %lhs, %rhs : tensor<2x2xi32>
    ```
  }];
}

def StableHLO_OrOp: StableHLO_BinaryBiwiseOrLogicalElementwiseOp<"or"> {
  let summary = "Or operator";
  let description = [{
    Performs element-wise bitwise OR of two tensors `lhs` and `rhs` of integer types
    and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloor

    Example:
    ```mlir
    %result = stablehlo.or %lhs, %rhs : tensor<2xi1>
    ```
  }];
}

def StableHLO_XorOp : StableHLO_BinaryBiwiseOrLogicalElementwiseOp<"xor"> {
  let summary = "Xor operator";
  let description = [{
    Performs element-wise bitwise XOR of two tensors `lhs` and `rhs` of integer
    types and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloxor

    Example:
    ```mlir
    %result = stablehlo.xor %lhs, %rhs : tensor<2xi32>
    ```
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO communication op definitions.
//===----------------------------------------------------------------------===//

// InfeedOp corresponds to 'InfeedWithToken' xla client API and not 'Infeed'.
// InfeedWithToken allows ordering of infeed HLO instructions using tokens.
def StableHLO_InfeedOp : StableHLO_Op<"infeed", []> {

  let summary = "Infeed operator";

  let description = [{
    Reads data from the infeed and produces `results`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloinfeed

    Example:
    ```mlir
    %results:2 = "stablehlo.infeed"(%token) {
      infeed_config = ""
    } : (!stablehlo.token) -> (tensor<3x3x3xi32>, !stablehlo.token)
    ```
  }];

  let arguments = (ins
    HLO_Token:$token,
    DefaultValuedStrAttr<StrAttr, "">:$infeed_config,
    OptionalAttr<ArrayAttr>:$layout
  );
  let results = (outs Variadic<HLO_TensorOrToken>);
  let hasVerifier = 1;
}

// OutfeedOp corresponds to 'OutfeedWithToken' xla client API and not 'Outfeed'.
// OutfeedWithToken allows ordering of outfeed HLO instructions using tokens.
def StableHLO_OutfeedOp : StableHLO_Op<"outfeed",
    [DeclareOpInterfaceMethods<InferTypeOpInterface>]> {

  let summary = "Outfeed operator";

  let description = [{
    Writes `inputs` to the outfeed and produces a `result` token.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlooutfeed

    Example:
    ```mlir
    %result = "stablehlo.outfeed"(%inputs0, %token) {
      outfeed_config = ""
    } : (tensor<3x3x3xi32>, !stablehlo.token) -> !stablehlo.token
    ```
  }];

  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    HLO_Token:$token,
    DefaultValuedStrAttr<StrAttr, "">:$outfeed_config
  );
  let results = (outs HLO_Token);
}

def StableHLO_SendOp : StableHLO_Op<"send",
    [DeclareOpInterfaceMethods<InferTypeOpInterface>]> {

  let summary = "Send operator";

  let description = [{
    Sends `inputs` to a channel `channel_id` and produces a `result` token.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlosend

    Example:
    ```mlir
    %result = "stablehlo.send"(%operand, %token) {
      // channel_id = 5 : i64,
      // channel_type = #stablehlo<channel_type DEVICE_TO_HOST>,
      channel_handle = #stablehlo.channel_handle<handle = 5, type = 2>,
      is_host_transfer = true
    } : (tensor<3x4xi32>, !stablehlo.token) -> !stablehlo.token
    ```
  }];

  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    HLO_Token:$token,
    StableHLO_ChannelHandle:$channel_handle,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer
  );

  let results = (outs HLO_Token);
}

def StableHLO_RecvOp : StableHLO_Op<"recv", []> {

  let summary = "Recv operator";

  let description = [{
    Receives data from a channel with `channel_id` and produces `results`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlorecv

    Example:
    ```mlir
    %results:2 = "stablehlo.recv"(%token) {
      // channel_id = 5 : i64,
      // channel_type = #stablehlo<channel_type HOST_TO_DEVICE>,
      channel_handle = #stablehlo.channel_handle<handle = 5, type = 3>,
      is_host_transfer = true
    } : (!stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token)
    ```
  }];

  let arguments = (ins
    HLO_Token:$token,
    StableHLO_ChannelHandle:$channel_handle,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer
  );

  let results = (outs Variadic<HLO_TensorOrToken>);
  let hasVerifier = 1;
}

//===----------------------------------------------------------------------===//
// StableHLO parallelism related op definitions.
//===----------------------------------------------------------------------===//

def StableHLO_ReplicaIdOp : StableHLO_Op<"replica_id", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "ReplicaId operator";
  let description = [{
    Produces `replica_id` of the current process.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreplica_id

    Example:
    ```mlir
    %result = stablehlo.replica_id : tensor<ui32>
    ```
  }];
  let results = (outs TensorOf<[UI32]>);

  let assemblyFormat = "attr-dict `:` type(results)";
}

//===----------------------------------------------------------------------===//
// StableHLO control flow op definitions.
//===----------------------------------------------------------------------===//

def StableHLO_AfterAllOp : StableHLO_Op<"after_all", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {

  let summary = "AfterAll operator";

  let description = [{
    Ensures that the operations producing the `inputs` are executed before any
    operations that depend on `result`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloafter_all

    Example:

    ```mlir
    %result = stablehlo.after_all %input0, %input1 : !stablehlo.token
    ```
  }];

  let arguments = (ins Variadic<HLO_Token>:$inputs);
  let results = (outs HLO_Token:$result);

  let assemblyFormat = [{
    $inputs attr-dict
      `:` custom<VariadicSameOperandsAndResultType>(ref($inputs), type($inputs), type($result))
  }];
}

// Xla Client API has two separate calls for indexed and predicated conditional,
// although both eventually map to kConditional HLO. IfOp maps to predicated
// conditional use of kConditional HLO.
def StableHLO_IfOp: StableHLO_Op<"if", [
    RecursiveMemoryEffects,
    SingleBlockImplicitTerminator<"ReturnOp">,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "If operator";

  let description = [{
    Produces the output from executing exactly one branch from `true_branch` or
    `false_branch` depending on the value of `pred`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloif

    Example:
    %result = "stablehlo.if"(%pred) ({
      "stablehlo.return"(%result_true_branch) : (tensor<i32>) -> ()
    }, {
      "stablehlo.return"(%result_false_branch) : (tensor<i32>) -> ()
    }) : (tensor<i1>) -> tensor<i32>
  }];

  let arguments = (ins
    HLO_PredTensor:$pred
  );

  let regions = (region SizedRegion<1>:$true_branch,
                        SizedRegion<1>:$false_branch);

  let results = (outs Variadic<HLO_TensorOrToken>);
}

// Xla Client API has two separate calls for indexed and predicated conditional,
// although both eventually map to kConditional HLO. CaseOp maps to indexed
// conditional use of kConditional HLO.
def StableHLO_CaseOp: StableHLO_Op<"case", [
      RecursiveMemoryEffects,
      SingleBlockImplicitTerminator<"ReturnOp">,
      DeclareOpInterfaceMethods<InferTypeOpInterface>
    ]> {
  let summary = "Switch-Case operator";
  let description = [{
    Produces the output from executing exactly one `function` from `branches`
    depending on the value of `index`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocase

    Example:
    ```mlir
    %result = "stablehlo.case"(%index) ({
      stablehlo.return %result_branch0 : tensor<i32>
    }, {
      stablehlo.return %result_branch1 : tensor<i32>
    }) : (tensor<i32>) -> tensor<i32>
    ```
  }];

  let arguments = (ins
    I32Tensor:$index
  );

  let regions = (region VariadicRegion<SizedRegion<1>>:$branches);

  let results = (outs Variadic<HLO_TensorOrToken>);
}


def StableHLO_WhileOp: StableHLO_Op<"while", [
      RecursiveMemoryEffects,
      SingleBlockImplicitTerminator<"ReturnOp">,
      DeclareOpInterfaceMethods<InferTypeOpInterface>,
      OpAsmOpInterface
    ]> {
  let summary = "While operator";
  let description = [{
    Produces the output from executing `body` function 0 or more times while the
    `cond` function outputs `true`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlowhile

    Example:
    %results:2 = "stablehlo.while"(%input0, %input1) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = "stablehlo.compare"(%arg0, %arg1) {
          comparison_direction = #stablehlo<comparison_direction LT>
        } : (tensor<i32>, tensor<i32>) -> tensor<i1>
        "stablehlo.return"(%0) : (tensor<i1>) -> ()
    }, {
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = "stablehlo.add"(%arg0, %constant0) : (tensor<i32>, tensor<i32>) -> tensor<i32>
        "stablehlo.return"(%0, %arg1) : (tensor<i32>, tensor<i32>) -> ()
    }) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>)
  }];
  let arguments = (ins Variadic<HLO_TensorOrToken>:$operand);

  let regions = (region SizedRegion<1>:$cond, SizedRegion<1>:$body);

  let results = (outs Variadic<HLO_TensorOrToken>);

  let extraClassDeclaration = [{
    // Method of OpAsmOpInterface used during custom printing to name the block
    // arguments in the nested regions. We name both the condition and the body
    // regions entry arguments the same way, with a `iterArg` prefix. Since the
    // two regions are side-by-side they will have the same name, which allows
    // us to print them once and share it for the two regions, and still be able
    // to parse them back.
    void getAsmBlockArgumentNames(Region &region, OpAsmSetValueNameFn setNameFn) {
      for (BlockArgument arg : region.getArguments())
        setNameFn(arg, "iterArg");
    }

    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];
  let hasCustomAssemblyFormat = 1;
}

def StableHLO_AllGatherOp : StableHLO_Op<"all_gather", [SameOperandsAndResultElementType]> {

  string summary = "AllGather operator";

  string description = [{
    Within each process group in the StableHLO grid, concatenates the values of the
    `operand` tensor from each process along `all_gather_dim` and produces a
    `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloall_gather

    Example:
    ```mlir
    %result = "stablehlo.all_gather"(%operand) {
      all_gather_dim = 1 : i64,
      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>
      // use_global_device_ids = false
    } : (tensor<2x2xf32>) -> tensor<2x4xf32>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64Attr:$all_gather_dim,
    I64ElementsAttr:$replica_groups,
    OptionalAttr<StableHLO_ChannelHandle>:$channel_handle,
    UnitAttr:$use_global_device_ids
  );
  let results = (outs HLO_Tensor);
  let hasVerifier = 1;
}

def StableHLO_AllReduceOp : StableHLO_Op<"all_reduce",
    [HLO_CompatibleOperandsAndResultType]> {
  let summary = "AllReduce operator";
  let description = [{
    Within each process group in the StableHLO grid, applies a reduction function
    `computation` to the values of the `operand` tensor from each process and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloall_reduce

    Example:
    ```mlir
    %result = "stablehlo.all_reduce"(%operand) ({
      ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
        %0 = stablehlo.add %arg1, %arg2 : tensor<f32>
        stablehlo.return %0 : tensor<f32>
    }) {
      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>
    } : (tensor<4xf32>) -> tensor<4xf32>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$replica_groups,
    OptionalAttr<StableHLO_ChannelHandle>:$channel_handle,
    UnitAttr:$use_global_device_ids
  );
  let regions = (region SizedRegion<1>:$computation);
  let results = (outs HLO_Tensor);
  // use_global_device_ids is rarely used, so we add a simplified
  // builder method for convenience.
  let builders = [
    OpBuilder<(ins
      "::mlir::Type":$result_type, "::mlir::Value":$operand,
      "::mlir::DenseIntElementsAttr":$replica_groups,
      "::mlir::stablehlo::ChannelHandleAttr":$channel_handle)>];
  let hasVerifier = 1;
}

def StableHLO_ReduceScatterOp : StableHLO_Op<"reduce_scatter",
    [SameOperandsAndResultElementType]> {
  let summary = "ReduceScatter operator";
  let description = [{
     Performs all_reduce followed by a scatter.

     See https://www.tensorflow.org/xla/operation_semantics#reducescatter
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64Attr:$scatter_dimension,
    I64ElementsAttr:$replica_groups,
    OptionalAttr<StableHLO_ChannelHandle>:$channel_handle,
    UnitAttr:$use_global_device_ids
  );
  let regions = (region SizedRegion<1>:$computation);
  let results = (outs HLO_Tensor);
  let hasVerifier = 1;
}

def StableHLO_AllToAllOp : StableHLO_Op<"all_to_all",
    [SameOperandsAndResultElementType, InferTensorType]> {
  let summary = "AllToAll operator";
  let description = [{
    Within each process group in the StableHLO grid, splits the values of the
    `operand` tensor along `split_dimension` into parts, scatters the split parts
    between the processes, concatenates the scattered parts along `concat_dimension`
    and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloall_to_all

    Example:
    ```mlir
    %result = "stablehlo.all_to_all"(%operand) {
      split_dimension = 1 : i64,
      concat_dimension = 0 : i64,
      split_count = 2 : i64,
      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>
    } : (tensor<2x4xf32>) -> tensor<4x2xf32>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64Attr:$split_dimension,
    I64Attr:$concat_dimension,
    I64Attr:$split_count,
    I64ElementsAttr:$replica_groups
  );
  let results = (outs HLO_Tensor);
}

def StableHLO_ReduceOp: StableHLO_ShapedInterfaceOp<"reduce", [
      RecursiveMemoryEffects,
      SameVariadicOperandSize,
      InferTensorTypeWithReify,
      SingleBlockImplicitTerminator<"ReturnOp">
    ]> {
  let summary = "Reduce operator";
  let description = [{
    Applies a reduction function `body` to `inputs` and `init_values` along the
    `dimensions` and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreduce

    Example:
    ```mlir
    %result = "stablehlo.reduce"(%input, %init_value) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
        "stablehlo.return"(%0) : (tensor<i32>) -> ()
    }) {
      dimensions = dense<1> : tensor<1xi64>
    } : (tensor<1x6xi32>, tensor<i32>) -> tensor<1xi32>
    ```
  }];
  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    Variadic<HLO_Tensor>:$init_values,
    I64ElementsAttr:$dimensions
  );

  let results = (outs Variadic<HLO_Tensor>);

  let hasCustomAssemblyFormat = 1;

  // TODO(hinsu): Verify that the attached body arguments and results are
  // compatible with reduce op's operands.
  let regions = (region SizedRegion<1>:$body);
}

//===----------------------------------------------------------------------===//
// StableHLO tuple op definitions.
//===----------------------------------------------------------------------===//
def StableHLO_GetTupleElementOp: StableHLO_Op<"get_tuple_element", [Pure,
     DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "GetTupleElement operator";
  let description = [{
    Extracts element at `index` position of the `operand` tuple and produces a
    `result`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloget_tuple_element

    Example:
    ```mlir
    %result = stablehlo.get_tuple_element %operand[0] : (tuple<tensor<2xf32>, tuple<tensor<i32>>>) -> tensor<2xf32>
    ```
  }];
  let arguments = (ins
    HLO_Tuple:$operand,
    I32Attr:$index
  );

  let results = (outs HLO_TensorOrTokenOrTuple);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `[` $index `]` attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_TupleOp : StableHLO_Op<"tuple", [Pure,
     DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "XLA's tuple op";
  let description = [{
    Produces a `result` tuple from values `val`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlotuple

    Example:
    ```mlir
    %result = stablehlo.tuple %val0, %val1 : tuple<tensor<2xf32>, tuple<tensor<i32>>>
    ```
   }];

  let arguments = (ins Variadic<HLO_TensorOrTokenOrTuple>:$val);
  let results = (outs HLO_Tuple:$result);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $val attr-dict `:` custom<TupleOpType>(type($val), type($result))
  }];
}

def StableHLO_CompareOp: StableHLO_Op<"compare", [Pure, SameOperandsElementType,
    SameOperandsAndResultShape, Elementwise, InferTensorTypeWithReify]> {
  let summary = "Comparison operator";
  let description = [{
    Performs element-wise comparison of `lhs` and `rhs` tensors according to
    `comparison_direction` and `compare_type`, and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocompare

    Example:
    ```mlir
    %result = stablehlo.compare LT, %lhs, %rhs, FLOAT : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$lhs,
    HLO_Tensor:$rhs,
    StableHLO_ComparisonDirectionAttr:$comparison_direction,
    OptionalAttr<StableHLO_ComparisonTypeAttr>:$compare_type
  );
  let results = (outs HLO_PredTensor);

  let builders = [
    OpBuilder<(ins "Value":$lhs, "Value":$rhs,
      "::mlir::stablehlo::ComparisonDirection":$comparison_direction,
      CArg<"::mlir::stablehlo::ComparisonType",
      "::mlir::stablehlo::ComparisonType::NOTYPE">:$compare_type)>,
  ];

  let assemblyFormat = [{
    $comparison_direction `,` $lhs `,` $rhs (`,` $compare_type^)?
      attr-dict `:` functional-type(operands, results)
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO Slice definitions.
//===----------------------------------------------------------------------===//

def StableHLO_SliceOp: StableHLO_Op<
      "slice",
      [Pure, SameOperandsAndResultElementType,
       AllTypesMatch<["start_indices", "limit_indices", "strides"]>,
       DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Slice operator";
  let description = [{
    Extracts a slice from the `operand` using statically-computed starting
    indices and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloslice

    Example:
    ```mlir
    %result = "stablehlo.slice" (%operand) {
      start_indices = dense<2> : tensor<1xi64>,
      limit_indices = dense<4> : tensor<1xi64>,
      strides = dense<1> : tensor<1xi64>
    } : (tensor<5xi64>) -> tensor<2xi64>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$start_indices,
    I64ElementsAttr:$limit_indices,
    I64ElementsAttr:$strides
  );

  let results = (outs HLO_Tensor);
}

def StableHLO_DynamicSliceOp: StableHLO_Op<"dynamic_slice",
      [Pure, AllElementTypesMatch<["operand", "result"]>,
       InferTensorType]> {
  let summary = "Dynamic Slice operator";
  let description = [{
    Extracts a slice from the `operand` using dynamically-computed starting
    indices and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlodynamic_slice

    Example:
    ```mlir
    %result = stablehlo.dynamic_slice %operand, %start_indices0, %start_indices1, sizes = [2, 2]
      : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    Variadic<HLO_ScalarIntTensor>:$start_indices,
    I64ElementsAttr:$slice_sizes
  );

  let results = (outs HLO_Tensor:$result);
  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` custom<VariadicOperandWithAttribute>($start_indices)
      `sizes` `=` custom<DenseI64Array>($slice_sizes)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_DynamicUpdateSliceOp: StableHLO_Op<"dynamic_update_slice",
      [Pure, AllElementTypesMatch<["operand", "update", "result"]>,
       AllShapesMatch<["operand", "result"]>, InferTensorType]> {
  let summary = "Dynamic Update Slice operator";
  let description = [{
    Produces a `result` tensor which is equal to the `operand` tensor except
    that the slice starting at `start_indices` is updated with the values in
    `update`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloslice

    Example:
    ```mlir
    %result = stablehlo.dynamic_update_slice %operand, %update, %start_indices0, %start_indices1
      : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_Tensor:$update,
    Variadic<HLO_ScalarIntTensor>:$start_indices
  );
  let results = (outs HLO_Tensor:$result);

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}


//===----------------------------------------------------------------------===//
// StableHLO Other op definitions.
//===----------------------------------------------------------------------===//

def StableHLO_BatchNormGradOp : StableHLO_Op<"batch_norm_grad", [Pure,
    AllShapesMatch<["scale", "mean", "variance", "grad_scale",
        "grad_offset"]>,
    AllShapesMatch<["operand", "grad_output"]>,
    AllElementTypesMatch<["operand", "grad_scale", "grad_offset"]>,
    AllTypesMatch<["operand", "grad_operand"]>,
    InferTensorType]> {
  let summary = "Batch Normalization Gradient";
  let description = [{
    Computes gradients of several inputs of BatchNormTrainingOp backpropagating
    from `grad_output`, and produces `grad_operand`, `grad_scale` and
    `grad_offset` tensors.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlobatch_norm_grad

    Example:
    ```mlir
    %grad_operand, %grad_scale, %grad_offset =
    "stablehlo.batch_norm_grad"(%operand, %scale, %mean, %variance, %grad_output) {
      epsilon = 0.0 : f32,
      feature_index = 2 : i64
    } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>,
        tensor<2x2x2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>)
    ```
  }];

  let arguments = (ins
    RankedTensorOf<[HLO_Float]>:$operand,
    1DTensorOf<[HLO_Float]>:$scale,
    1DTensorOf<[HLO_Float]>:$mean,
    1DTensorOf<[HLO_Float]>:$variance,
    RankedTensorOf<[HLO_Float]>:$grad_output,
    F32Attr:$epsilon,
    I64Attr:$feature_index
  );

  let results = (outs
      RankedTensorOf<[HLO_Float]>:$grad_operand,
      1DTensorOf<[HLO_Float]>:$grad_scale,
      1DTensorOf<[HLO_Float]>:$grad_offset);
}

def StableHLO_BatchNormInferenceOp : StableHLO_Op<"batch_norm_inference",
    [Pure, AllTypesMatch<["operand", "result"]>,
    AllShapesMatch<["scale", "offset", "mean", "variance"]>,
    InferTensorType]> {
  let summary = "Batch Normalization for Inference";
  let description = [{
    Normalizes the `operand` tensor across all dimensions except for the
    `feature_index` dimension and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlobatch_norm_inference

    Example:
    ```mlir
    %result = "stablehlo.batch_norm_inference"(%operand, %scale, %offset, %mean, %variance) {
      epsilon = 0.0 : f32,
      feature_index = 2 : i64
    } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>) -> tensor<2x2x2xf32>
    ```
  }];

  let arguments = (ins
    RankedTensorOf<[HLO_Float]>:$operand,
    1DTensorOf<[HLO_Float]>:$scale,
    1DTensorOf<[HLO_Float]>:$offset,
    1DTensorOf<[HLO_Float]>:$mean,
    1DTensorOf<[HLO_Float]>:$variance,
    F32Attr:$epsilon,
    I64Attr:$feature_index
  );

  let results = (outs RankedTensorOf<[HLO_Float]>:$result);
}

def StableHLO_BatchNormTrainingOp : StableHLO_Op<"batch_norm_training",
    [Pure, AllTypesMatch<["operand", "output"]>,
    AllElementTypesMatch<["operand", "batch_mean", "batch_var"]>,
    AllShapesMatch<["scale", "offset", "batch_mean", "batch_var"]>,
    InferTensorType]> {
  let summary = "Batch Normalization for Training";
  let description = [{
    Computes mean and variance across batch and spatial dimensions and
    normalizes the `operand` tensor, for each feature in the `feature_index`
    dimension and produces `output`, `batch_mean` and `batch_var` tensors.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlobatch_norm_training

    Example:
    ```mlir
    %output, %batch_mean, %batch_var = "stablehlo.batch_norm_training"(%operand, %scale, %offset) {
      epsilon = 0.0 : f32,
      feature_index = 2 : i64
    } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>)
    ```
  }];

  let arguments = (ins
    RankedTensorOf<[HLO_Float]>:$operand,
    1DTensorOf<[HLO_Float]>:$scale,
    1DTensorOf<[HLO_Float]>:$offset,
    F32Attr:$epsilon,
    I64Attr:$feature_index
  );

  let results = (outs
      RankedTensorOf<[HLO_Float]>:$output,
      1DTensorOf<[HLO_Float]>:$batch_mean,
      1DTensorOf<[HLO_Float]>:$batch_var);
}

def StableHLO_BitcastConvertOp : StableHLO_ShapedInterfaceOp<"bitcast_convert",
    [Pure]> {
  let summary = "BitcastConvert operator";
  let description = [{
    Performs a bitcast operation on `operand` tensor and produces a `result`
    tensor where the bits of the entire `operand` tensor are reinterpreted using
    the type of the `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlobitcast_convert

    Example:
    ```mlir
    %result = stablehlo.bitcast_convert %operand : (tensor<2xf32>) -> tensor<2x4xi8>
    ```
  }];

  let arguments = (ins HLO_Tensor:$operand);
  let results = (outs HLO_Tensor);
  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_BroadcastOp : StableHLO_ShapedInterfaceOp<"broadcast",
    [Pure, SameOperandsAndResultElementType, InferTensorType]> {
  let summary = "Broadcast a tensor to a higher rank by prepending dimensions";
  let description = [{
    Broadcasts the operand tensor to a higher rank by prepending
    `broadcast_sizes` to the dimensions. The current values of the operand are
    copied into the other dimensions.

    This is a more limited form of broadcasting, that corresponds to the XLA
    client Broadcast method. For a more general form of broadcasting, see the
    BroadcastInDimOp.

    See https://www.tensorflow.org/xla/operation_semantics#broadcast.

    Example:

    ```mlir
    %0 = stablehlo.broadcast %arg1, sizes = [1, 2] : (tensor<3xi32>) -> tensor<1x2x3xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$broadcast_sizes
  );

  let results = (outs HLO_Tensor);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` `sizes` `=` custom<DenseI64Array>($broadcast_sizes)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_BroadcastInDimOp : StableHLO_Op<"broadcast_in_dim",
      [Pure, SameOperandsAndResultElementType]> {
  let summary = "Broadcast a tensor into the given shape by adding dimensions.";
  let description = [{
    Expands the dimensions and/or rank of an input tensor by duplicating the
    data in the `operand` tensor and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlobroadcast_in_dim

    Example:

    ```mlir
    %result = stablehlo.broadcast_in_dim %operand, dims = [2, 1] : (tensor<1x3xi32>) -> tensor<2x3x2xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    BroadcastDimAttr:$broadcast_dimensions
  );

  let results = (outs HLO_StaticShapeTensor);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` `dims` `=` custom<DenseI64Array>($broadcast_dimensions)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_DynamicBroadcastInDimOp : StableHLO_ShapedInterfaceOp<
    "dynamic_broadcast_in_dim", [Pure]> {
  let summary = "Broadcast a tensor into the given dynamic shape by adding dimensions.";
  let description = [{
    This is a generalization of the BroadcastInDimOp which accepts its output
    dimensions as an argument. It should eventually supercede the statically
    shaped original, but is being phased as a separate op in order to support
    compatibility with lowerings and translations that precede dynamic shapes.

    The op accepts optional attributes to express static knowledge about the
    expanding behavior of dimensions. If not specified, all dimensions are
    assumed to be possibly expanding. The sets of dimensions that are known to
    be expanding and the set of dimensions that are known to be non-expanding
    must be disjoint and they must be a subset of the operand's dimensions.
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_DimensionTensor:$output_dimensions,
    BroadcastDimAttr:$broadcast_dimensions,
    OptionalAttr<BroadcastDimAttr>:$known_expanding_dimensions,
    OptionalAttr<BroadcastDimAttr>:$known_nonexpanding_dimensions
  );

  let results = (outs HLO_Tensor);

  let builders = [
    OpBuilder<(ins
        "Type":$result_type, "Value":$operand, "Value":$output_dimensions,
        "DenseIntElementsAttr":$broadcast_dimensions), [{
      build($_builder, $_state, result_type, operand, output_dimensions,
          broadcast_dimensions, /*known_expanding_dimensions=*/{},
          /*known_nonexpanding_dimensions=*/{});
    }]>
  ];

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` $output_dimensions `,` `dims` `=` custom<DenseI64Array>($broadcast_dimensions)
      attr-dict `:` functional-type(operands, results)
  }];
}

// Note: There is no HLO_CallOp because the standard call operation mlir::func::CallOp
// is used instead. A mlir::func::CallOp is exported to a HLO call instruction
// directly.

def StableHLO_CholeskyOp : StableHLO_Op<"cholesky",
      [Pure, SameOperandsAndResultElementType, InferTensorType]> {
  let summary = "Cholesky operator";
  let description = [{
    Computes the Cholesky decomposition of a batch of matrices.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlocholesky

    Example:
    ```mlir
    %result = stablehlo.cholesky %a, lower = true : tensor<3x3xf32>
    ```
  }];
  let arguments = (ins
    HLO_FpOrComplexTensor:$a,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$lower
  );

  let results = (outs HLO_FpOrComplexTensor:$result);

  let assemblyFormat = [{
    $a `,` `lower` `=` $lower attr-dict `:` custom<SameOperandsAndResultType>(type($a), type($result))
  }];
}

def StableHLO_ClampOp : StableHLO_ShapedInterfaceOp<"clamp", [Pure,
  SameOperandsAndResultElementType, HLO_BroadcastingElementwise,
  InferTensorType]> {
  let summary = "Clamp operator";
  let description = [{
    Clamps every element of the `operand` tensor between a minimum and maximum
    value and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloclamp

    Example:
    ```mlir
    %result = stablehlo.clamp %min, %operand, %max : tensor<3xi32>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$min,
    HLO_Tensor:$operand,
    HLO_Tensor:$max
  );
  let results = (outs HLO_Tensor:$result);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $min `,` $operand `,` $max attr-dict
      `:` custom<SameOperandsAndResultType>(type($min), type($operand), type($max), type($result))
  }];
}

def StableHLO_ConcatenateOp : StableHLO_ShapedInterfaceOp<"concatenate",
    [Pure, SameOperandsAndResultElementType,
     DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "XLA's concatenate op";
  let description = [{
    Concatenates a variadic number of tensors in `inputs` along `dimension`
    dimension in the same order as the given arguments and produces a `result`
    tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloconcatenate

    Example:
    ```mlir
     %result = stablehlo.concatenate %input0, %input1, dim = 1 : (tensor<3x2xi32>, tensor<1x2xi32>) -> tensor<4x2xi32>
    ```
   }];

  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    I64Attr:$dimension
  );

  let results = (outs HLO_Tensor);

  let assemblyFormat = [{
     custom<VariadicOperandWithAttribute>($inputs) `dim` `=` $dimension attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_CollectivePermuteOp: StableHLO_Op<"collective_permute",
    [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "CollectivePermute operator";
  let description = [{
    CollectivePermute is a collective operation that sends and receives data
    cross replicas.
    Note that there are the following restrictions on the source_target_pair:
    - Any two pairs should not have the same target replica id, and they should
    not have the same source replica id.
    - If a replica id is not a target in any pair, then the output on that
    replica is a tensor consists of 0(s) with the same shape as the input.

    See https://www.tensorflow.org/xla/operation_semantics#collectivepermute.

  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$source_target_pairs,
    OptionalAttr<StableHLO_ChannelHandle>:$channel_handle
  );
  let results = (outs HLO_Tensor);
  let hasVerifier = 1;
  // channel_handle is only used for the SPMD partitioner, so we add a
  // simplified builder method for convenience.
  let builders = [
    OpBuilder<(ins
      "::mlir::Type":$result_type, "::mlir::Value":$operand,
      "::mlir::DenseIntElementsAttr":$source_target_pairs)>];
}

def StableHLO_ConvolutionOp : StableHLO_Op<"convolution", [Pure, InferTensorType]> {
  let summary = "Convolution operator";
  let description = [{
    Computes a convolution of the kind used in neural networks.

    See https://www.tensorflow.org/xla/operation_semantics#conv_convolution.
  }];
  let arguments = !con(
    (ins
       HLO_Tensor:$lhs,
       HLO_Tensor:$rhs),
    StableHLO_ConvolutionAttributes.attributes);

  let results = (outs HLO_Tensor);

  let extraClassDeclaration = [{
    bool hasWindowReversal() {
      auto reversal = getWindowReversalAttr();
      return reversal && llvm::any_of(reversal.getValues<bool>(),
                                      [](bool v) { return v; });
    }
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return succeeded(mlir::verifyCompatibleShapes(l, r));
    }
  }];

  let assemblyFormat = [{
    `(`operands`)`
       `dim_numbers` `=` custom<ConvolutionDimensions>($dimension_numbers) `,`
       `window` `=` `{` custom<WindowAttributes>($window_strides, $padding,
                                                 $lhs_dilation, $rhs_dilation,
                                                 $window_reversal) `}`
       attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_CrossReplicaSumOp : StableHLO_Op<"cross-replica-sum",
    [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Sums input across replicated instances.";
  let description = [{
     For each of the replica groups, operands of the group devices are summed
     so that each device has the sum.

     For example, suppose there are 8 TPU devices: `[A, B, C, D, E, F, G, H]`.
     Passing group_assignment=`[[0,2,4,6],[1,3,5,7]]` sets `A, C, E, G` as group 0,
     and `B, D, F, H` as group 1. Thus we get the outputs:
     `[A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H]`.

     See https://www.tensorflow.org/xla/operation_semantics#crossreplicasum.
   }];

  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$replica_groups
  );

  let results = (outs HLO_Tensor);
}

def StableHLO_CustomCallOp: StableHLO_Op<"custom_call",
    [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
  let summary = "CustomCall operator";
  let description = [{
    A custom call invokes code external to XLA. The `args` are passed to the
    external code, and the external code is expected to produce a result of the
    given type. The exact mechanism is backend-specific. For example, in the CPU
    backend, a call instruction is emitted which targets a symbol with the name
    `call_target_name`.

    `call_target_name` and `backend_config` can be arbitrary strings, but
    `call_target_name` should be short as it may be used in labels.
    `backend_config` can encode arbitrarily large amounts of information.

    `has_side_effect` must be true if the custom call has side-effects.
    `api_version` specifies the version of the API used by the custom call
    function.

    A custom call may apply functions within the scope of the parent module.
    They can be referenced using `called_computations` attribute.

    A custom call can also have layout constraints on operands and results which
    can be specified as optional `operand_layouts` and `result_layouts`
    attributes. The layout attribute is an array of rank-1 index tensors and the
    i-th layout attribute specifies the layout for i-th operand/result.

    The `operand_layouts` & `result_layouts` attributes can be specified under
    the following constraints:
    1) Either both `operand_layouts` and `result_layouts` are specified or none.
    2) None of the operands are of tuple type.
    3) None of the results are of tuple type except the common case of single
       tuple result packing non-tuple values is allowed. In this case the i-th
       `result_layouts` attribute specifies the layout of i-th element in the
       result tuple.

    See https://www.tensorflow.org/xla/operation_semantics#customcall.

    Example:

    ```mlir
    %1 = stablehlo.custom_call @foo(%arg0, %arg1) {backend_config = "bar", has_side_effect = true}
          : (tensor<2x3xf32>, tensor<5x5xf32>) -> tensor<1x2x3xf32>
    ```
  }];

  let arguments = (ins
    Variadic<HLO_TensorOrTokenOrTuple>:$inputs,
    StrAttr:$call_target_name,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$has_side_effect,
    DefaultValuedStrAttr<StrAttr, "">:$backend_config,
    // TODO(b/189822916): Remove this field when all clients are migrated to
    // the status-returning API.
    DefaultValuedOptionalAttr<
        StableHLO_CustomCallApiVersionAttr,
        "::mlir::stablehlo::CustomCallApiVersion::API_VERSION_ORIGINAL">:
        $api_version,
    DefaultValuedOptionalAttr<StableHLO_FlatSymbolRefArrayAttr, "{}">:$called_computations,
    OptionalAttr<StableHLO_ArrayOfLayoutAttr>:$operand_layouts,
    OptionalAttr<StableHLO_ArrayOfLayoutAttr>:$result_layouts,
    DefaultValuedOptionalAttr<
        TypedArrayAttrBase<
            StableHLO_OutputOperandAlias,
            "Aliasing attribute for outputs and operands of CustomCall">,
        "{}">:$output_operand_aliases
  );
  let results = (outs Variadic<HLO_TensorOrTokenOrTuple>);
  let hasVerifier = 1;

  let assemblyFormat = [{
    custom<CustomCallTarget>($call_target_name) `(` $inputs `)`
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_DotOp: StableHLO_Op<"dot",
    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Dot operator";
  let description = [{
    Performs dot products between vectors, vector/matrix and matrix/matrix
    multiplication.

    See https://www.tensorflow.org/xla/operation_semantics#dot.

    Example:

    ```mlir
    %0 = stablehlo.dot %arg0, %arg1 : (tensor<1x2xi32>, tensor<2x1xi32>) -> tensor<1x1xi32>
    %1 = stablehlo.dot %arg0, %arg1, precision = [DEFAULT, DEFAULT] : (tensor<1x2xi32>, tensor<2x1xi32>) -> tensor<1x1xi32>
    ```
  }];
  let arguments = (
    ins HLO_Tensor:$lhs,
    HLO_Tensor:$rhs,
    StableHLO_PrecisionConfigAttr:$precision_config
  );
  let results = (outs HLO_Tensor);
  let hasVerifier = 1;

  let extraClassDeclaration = [{
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return succeeded(mlir::verifyCompatibleShapes(l, r));
    }
  }];

  // Use empty `` to prevent extra whitespace before precision config.
  let assemblyFormat = [{
    $lhs `,` $rhs `` custom<PrecisionConfig>($precision_config) attr-dict
      `:` functional-type(operands, results)
  }];
}

def StableHLO_DotGeneralOp: StableHLO_ShapedInterfaceOp<"dot_general",
    [Pure, InferTensorTypeWithReify]> {
  let summary = "General Dot operator";
  let description = [{
    Performs general dot products between vectors, vector/matrix and
    matrix/matrix multiplication.

    See https://www.tensorflow.org/xla/operation_semantics#dotgeneral.
  }];
  let arguments = (ins
    HLO_Tensor:$lhs,
    HLO_Tensor:$rhs,
    StableHLO_DotDimensionNumbers:$dot_dimension_numbers,
    StableHLO_PrecisionConfigAttr:$precision_config
  );

  let results = (outs HLO_Tensor);

  let extraClassDeclaration = [{
    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return succeeded(mlir::verifyCompatibleShapes(l, r));
    }
  }];
}

// Define Base Einsum op within the HLO dialect as these are client ops and
// therefore this class is not common between HLO and LHLO ops.
class BASE_EinsumOp {
  string summary = "Einsum operator";

  string description = [{
    Returns a tensor whose elements are defined by equation, which is written
    in a shorthand form inspired by the Einstein summation convention.
  }];
}

def StableHLO_EinsumOp: StableHLO_Op<"einsum", [Pure]>, BASE_EinsumOp {
  let arguments = (ins
    HLO_Tensor:$lhs,
    HLO_Tensor:$rhs,
    StrAttr:$einsum_config
  );

  let results = (outs HLO_Tensor);

  // TODO(hinsu): Canonicalize to lower this client side HLO op to server
  // side HLO ops.
}

def StableHLO_UnaryEinsumOp: StableHLO_Op<"unary_einsum", [Pure]>, BASE_EinsumOp {
  let arguments = (ins
    HLO_Tensor:$operand,
    StrAttr:$einsum_config
  );

  let results = (outs HLO_Tensor);
}

def StableHLO_FftOp: StableHLO_Op<"fft", [InferTensorType, Pure]> {
  let summary = "Fast fourier transform operator";
  let description = [{
    Performs the forward and inverse Fourier transforms for real and complex
    inputs/outputs.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlofft

    Example:
    ```mlir
    %result = stablehlo.fft %operand, type = FFT, length = [4] : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>>
    ```
  }];
  let arguments = (ins
    HLO_FpOrComplexTensor:$operand,
    StableHLO_FftTypeAttr:$fft_type,
    I64ElementsAttr:$fft_length
  );

  let results = (outs HLO_Tensor);

  let assemblyFormat = [{
    $operand `,` `type` `=` $fft_type `,` `length` `=` custom<DenseI64Array>($fft_length)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_GatherOp: StableHLO_Op<"gather", [InferTensorTypeWithReify, Pure]> {
  let summary = "Gather operator";
  let description = [{
    Gathers slices from `operand` tensor from offsets specified in
    `start_indices` and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlogather

    Example:
    ```mlir
    %result = "stablehlo.gather"(%operand, %start_indices) {
      dimension_numbers = #stablehlo.gather<
        offset_dims = [2, 3],
        collapsed_slice_dims = [0],
        start_index_map = [0, 2],
        index_vector_dim = 2>,
      slice_sizes = dense<[0, 2, 2]> : tensor<3xi64>,
      indices_are_sorted = false
    } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>) -> tensor<2x3x2x2xi32>
    ```
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_IntTensor:$start_indices,
    StableHLO_GatherDimensionNumbers:$dimension_numbers,
    I64ElementsAttr:$slice_sizes,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$indices_are_sorted
  );

  let results = (outs HLO_Tensor);
}

def StableHLO_GetDimensionSizeOp: StableHLO_Op<"get_dimension_size", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "GetDimensionSize operator";
  let description = [{
    Returns the size of the given dimension of the operand.

    See
    https://www.tensorflow.org/xla/operation_semantics#getdimensionsize.

    Example:

    ```mlir
    %0 = stablehlo.get_dimension_size %arg0, dim = 1 : (tensor<4x2xf32>) -> tensor<i32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    I64Attr:$dimension
  );
  // TODO(hinsu): Allow 64-bit result types once XLA HLO dialect based on the
  // XLA semantics is available. This limitation is because of the current XLA
  // implementation.
  let results = (outs I32Tensor);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` `dim` `=` $dimension attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_MapOp: StableHLO_ShapedInterfaceOp<"map",
      [RecursiveMemoryEffects, SameOperandsAndResultShape,
       SingleBlockImplicitTerminator<"ReturnOp">, InferTensorTypeWithReify]> {
  let summary = "Map operator";
  let description = [{
    Applies a map function `computation` to `inputs` along the `dimensions` and
    produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlomap

    Example:
    ```mlir
    %result = "stablehlo.map"(%input0, %input1) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
        stablehlo.return %0 : tensor<i32>
    }) {
      dimensions = dense<[0, 1]> : tensor<2xi64>
    } : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32>
  ```
  }];
  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    I64ElementsAttr:$dimensions
  );
  let regions = (region SizedRegion<1>:$computation);
  let results = (outs HLO_Tensor);
}

def StableHLO_ReshapeOp: StableHLO_Op<"reshape",
      [Pure, SameOperandsAndResultElementType]> {
  let summary = "Reshape operator";
  let description = [{
    Performs reshape of `operand` tensor to a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreshape

    Example:
    ```mlir
    %result = stablehlo.reshape %operand : (tensor<2xf32>) -> tensor<1x2xf32>
    ```
  }];

  let arguments = (ins HLO_Tensor:$operand);

  let results = (outs HLO_StaticShapeTensor);
  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_DynamicReshapeOp: StableHLO_ShapedInterfaceOp<"dynamic_reshape", [Pure]> {
  let summary = "Reshape a tensor to a given, possibly dynamic, shape.";
  let description = [{
    Reshapes `operand` to `output_shape`.

    Requires:
    - The length of `output_shape` is equal to the rank of `result`.
    - The number of elements in `operand` (that is, the product of extents of
      its shape) is equal to the number of elements in `output_shape` (that is,
      the product of values in `output_shape`).

    Example:

    ```mlir
    %0 = stablehlo.dynamic_reshape %arg0, %shape : (tensor<?xf32>, tensor<2xindex>) -> tensor<?x?xf32>
    ```
  }];

  let arguments = (ins HLO_Tensor:$operand, HLO_DimensionTensor:$output_shape);
  let results = (outs HLO_Tensor:$result);

  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_ScatterOp: StableHLO_Op<"scatter",
      [SameVariadicOperandSize, RecursiveMemoryEffects,
      DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Scatter operator";
  let description = [{
    Produces `results` tensors which are equal to `inputs` tensors except that
    several slices specified by `scatter_indices` are updated with the values
    `updates` using `update_computation`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloscatter

   Example:

   ```mlir
   %result = "stablehlo.scatter"(%input, %scatter_indices, %update) ({
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %0 : tensor<i32>
   }) {
     scatter_dimension_numbers = #stablehlo.scatter<
       update_window_dims = [2,3],
       inserted_window_dims = [0],
       scatter_dims_to_operand_dims = [1, 0],
       index_vector_dim = 2>,
     indices_are_sorted = false,
     unique_indices = false
   } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>, tensor<2x3x2x2xi32>) -> tensor<3x4x2xi32>
   ```

  }];
  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    TensorOf<[AnyInteger, Index]>:$scatter_indices,
    Variadic<HLO_Tensor>:$updates,
    StableHLO_ScatterDimensionNumbers:$scatter_dimension_numbers,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$indices_are_sorted,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$unique_indices
  );

  let regions = (region SizedRegion<1>:$update_computation);

  let results = (outs Variadic<HLO_Tensor>);

  let hasVerifier = 1;
}

def StableHLO_SelectOp: StableHLO_Op<"select", [Pure, HLO_BroadcastingElementwise,
    InferTensorTypeWithReify]> {
  let summary = "Select operator";
  let description = [{
    Produces a `result` tensor where each element is selected from `on_true` or
    `on_false` tensor based on the value of the corresponding element of `pred`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloselect

    Example:
    ```mlir
    %result = stablehlo.select %pred, %on_true, %on_false : tensor<2x2xi1>, tensor<2x2xi32>
    ```
  }];
  let arguments = (ins
    HLO_PredTensor:$pred,
    HLO_Tensor:$on_true,
    HLO_Tensor:$on_false
  );

  let results = (outs HLO_Tensor:$result);

  let assemblyFormat = [{
    operands attr-dict `:`
      custom<SelectOpType>(type($pred), type($on_true), type($on_false), type($result))
  }];
}

def StableHLO_SelectAndScatterOp: StableHLO_Op<"select_and_scatter",
      [RecursiveMemoryEffects, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "SelectAndScatter operator";
  let description = [{
    Scatters the values from the `source` tensor using `scatter` based on the
    outcome of `reduce_window` of the `input` tensor using `select` and produces
    a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloselect_and_scatter

    Example:
    ```mlir
    %result = "stablehlo.select_and_scatter"(%operand, %source, %init_value) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = "stablehlo.compare"(%arg0, %arg1) {
          comparison_direction = #stablehlo<comparison_direction GE>
        } : (tensor<i32>, tensor<i32>) -> tensor<i1>
        "stablehlo.return"(%0) : (tensor<i1>) -> ()
    }, {
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
        "stablehlo.return"(%0) : (tensor<i32>) -> ()
    }) {
      window_dimensions = dense<[3, 1]> : tensor<2xi64>,
      window_strides = dense<[2, 1]> : tensor<2xi64>,
      padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64>
    } : (tensor<4x2xi32>, tensor<2x2xi32>, tensor<i32>) -> tensor<4x2xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_Tensor:$source,
    HLO_Tensor:$init_value,
    OptionalAttr<I64ElementsAttr>:$window_dimensions,
    OptionalAttr<I64ElementsAttr>:$window_strides,
    OptionalAttr<I64ElementsAttr>:$padding
  );

  let regions = (region SizedRegion<1>:$select, SizedRegion<1>:$scatter);

  let results = (outs HLO_Tensor);

  let hasVerifier = 1;
}

def StableHLO_SetDimensionSizeOp: StableHLO_Op<"set_dimension_size", [Pure,
    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "SetDimensionSize operator";
  let description = [{
    Sets the dynamic size of operand's given dimension. Pass through the operand
    as result, with dynamic dimension tracked by the compiler. Padded values
    will be ignored by downstream reduction ops.

    See https://www.tensorflow.org/xla/operation_semantics#setdimensionsize.

    Example:

    ```mlir
    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 1 : (tensor<4x2xf32>, tensor<i32>) -> tensor<4x2xf32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    I32Tensor:$size,
    I64Attr:$dimension
  );
  let results = (outs HLO_Tensor);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $operand `,` $size  `,` `dim` `=` $dimension attr-dict
      `:` functional-type(operands, results)
  }];
}

def StableHLO_SortOp : StableHLO_Op<"sort",
      [RecursiveMemoryEffects, SameOperandsAndResultShape, InferTensorType]> {
  let summary = "Sort operator";
  let description = [{
    Sorts a variadic number of tensors in `inputs` together, according to a
    custom `comparator`, along the given `dimension` and produces a variadic
    number of tensors as `results`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreverse

    Example:
    %result0, %result1 = "stablehlo.sort"(%input0, %input1) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<i32>):
        %predicate = "stablehlo.compare"(%arg0, %arg1) {
          comparison_direction = #stablehlo<comparison_direction GT>
          } : (tensor<i32>, tensor<i32>) -> tensor<i1>
        "stablehlo.return"(%predicate) : (tensor<i1>) -> ()
    }) {
      dimension = 0 : i64,
      is_stable = true
    } : (tensor<2x3xi32>, tensor<2x3xi32>) -> (tensor<2x3xi32>, tensor<2x3xi32>)
  }];
  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    DefaultValuedOptionalAttr<I64Attr, "-1">:$dimension,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$is_stable
  );

  let results = (outs Variadic<HLO_Tensor>);

  let regions = (region SizedRegion<1>:$comparator);

  let builders = [
    OpBuilder<(ins "ValueRange":$inputs, CArg<"int64_t", "-1">:$dimension,
      CArg<"bool", "false">:$is_stable)>];
}

def StableHLO_ReverseOp: StableHLO_Op<"reverse",
      [Pure, HLO_CompatibleOperandsAndResultType]> {
  let summary = "Reverse operator";
  let description = [{
    Reverses the order of elements in the `operand` along the specified
    `dimensions` and produces a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreverse

    Example:
    ```mlir
    %0 = stablehlo.reverse %arg0, dims = [1] : tensor<3x2xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$dimensions
  );

  let results = (outs HLO_Tensor:$result);

  let assemblyFormat = [{
    $operand `,` `dims` `=` custom<DenseI64Array>($dimensions)
      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))
  }];
}

def StableHLO_PadOp: StableHLO_ShapedInterfaceOp<"pad",
      [Pure, SameOperandsAndResultElementType,
      DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Pad operator";
  let description = [{
    Expands `operand` by padding around the tensor as well as between the
    elements of the tensor with the given `padding_value`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlopad

    Example:
    ```mlir
    %0 = stablehlo.pad %arg0, %arg1, low = [0, 1], high = [2, 1], interior = [1, 2]
      : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_Tensor:$padding_value,
    I64ElementsAttr:$edge_padding_low,
    I64ElementsAttr:$edge_padding_high,
    I64ElementsAttr:$interior_padding
  );

  let results = (outs HLO_Tensor);

  let assemblyFormat = [{
    $operand `,` $padding_value `,`
      `low` `=` custom<DenseI64Array>($edge_padding_low) `,`
      `high` `=` custom<DenseI64Array>($edge_padding_high) `,`
      `interior` `=` custom<DenseI64Array>($interior_padding)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_TraceOp: StableHLO_Op<"trace", []> {
  let summary = "Trace operator";
  let description = [{
    Emits a logging message `tag` with the `operand`.

    Example:

    ```mlir
    stablehlo.trace %arg0, "In test code." : tensor<5x1x5xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    StrAttr:$tag
  );
  let assemblyFormat = "$operand `,` $tag attr-dict `:` type($operand)";
}

def StableHLO_TransposeOp: StableHLO_ShapedInterfaceOp<"transpose",
      [Pure, SameOperandsAndResultElementType,
      DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = "Transpose operator";
  let description = [{
    Permutes the dimensions of `operand` tensor using `permutation` and produces
    a `result` tensor.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlotranspose

    Example:
    ```mlir
    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<1x2x3xi32>) -> tensor<3x2x1xi32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    I64ElementsAttr:$permutation
  );
  let results = (outs HLO_Tensor);

  let assemblyFormat = [{
    $operand `,` `dims` `=` custom<DenseI64Array>($permutation)
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_TriangularSolveOp: StableHLO_Op<"triangular_solve",
    [Pure, SameOperandsAndResultElementType, InferTensorType]> {
  let summary = "TriangularSolve operator";
  let description = [{
    Solves batches of systems of linear equations with lower or upper triangular
    coefficient matrices.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlotriangular_solve

    Example:
    ```mlir
    %result = "stablehlo.triangular_solve"(%a, %b) {
      left_side = true,
      lower = true,
      unit_diagonal = false,
      transpose_a = #stablehlo<transpose NO_TRANSPOSE>
    } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32>
    ```
  }];
  let arguments = (ins
    HLO_FpOrComplexTensor:$a,
    HLO_FpOrComplexTensor:$b,
    BoolAttr:$left_side,
    BoolAttr:$lower,
    BoolAttr:$unit_diagonal,
    StableHLO_TransposeAttr:$transpose_a
  );
  let results = (outs HLO_FpOrComplexTensor);
}

def StableHLO_ReduceWindowOp: StableHLO_Op<"reduce_window", [
      RecursiveMemoryEffects,
      SameVariadicOperandSize,
      SingleBlockImplicitTerminator<"ReturnOp">,
      InferTensorType,
    ]> {
  let summary = "ReduceWindow operator";
  let description = [{
    Applies a reduction function `body` to windows of `inputs` and `init_values`
    and produces `results`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehloreduce_window

    Example:
    ```mlir
    %result = "stablehlo.reduce_window"(%input, %init_value) ({
      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
        %0 = stablehlo.add %arg0, %arg1 : tensor<i32>
        stablehlo.return %0 : tensor<i32>
    }) {
      window_dimensions = dense<[2, 1]> : tensor<2xi64>,
      window_strides = dense<[4, 1]> : tensor<2xi64>,
      base_dilations = dense<[2, 1]> : tensor<2xi64>,
      window_dilations = dense<[3, 1]> : tensor<2xi64>,
      padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
    } : (tensor<3x2xi32>, tensor<i32>) -> tensor<2x2xi32>
    ```
  }];

  let arguments = (ins
    Variadic<HLO_Tensor>:$inputs,
    Variadic<HLO_Tensor>:$init_values,
    I64ElementsAttr:$window_dimensions,
    // If strides or dilations attributes are missing then the default value is
    // one for each of the operand dimensions. Similarly, padding values are zero
    // for both low and high in each of the dimensions, if not specified.
    OptionalAttr<I64ElementsAttr>:$window_strides,
    OptionalAttr<I64ElementsAttr>:$base_dilations,
    OptionalAttr<I64ElementsAttr>:$window_dilations,
    OptionalAttr<I64ElementsAttr>:$padding
  );

  let results = (outs Variadic<HLO_Tensor>);

  let regions = (region SizedRegion<1>:$body);

  // Builder for non-variadic version of the operation.
  let builders = [
    OpBuilder<(ins "Type":$result_type, "Value":$operand,
      "Value":$init_value,
      "DenseIntElementsAttr":$window_dimensions,
      "DenseIntElementsAttr":$window_strides,
      "DenseIntElementsAttr":$base_dilations,
      "DenseIntElementsAttr":$window_dilations,
      "DenseIntElementsAttr":$padding),
    [{
      build($_builder, $_state, TypeRange(result_type), ValueRange(operand),
            ValueRange(init_value), window_dimensions, window_strides,
            base_dilations, window_dilations, padding);
    }]>
  ];

  // TODO(hinsu): Implement custom printer and parser.

  let extraClassDeclaration = [{
    // Get the operation used for reduction applied to `result_index`th result.
    Operation *getReductionOp(int result_index);

    static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {
      return mlir::hlo::isCompatibleForHloTypeInference(l, r);
    }
  }];
}

def StableHLO_ReturnOp : StableHLO_Op<"return",
      [Pure, Terminator,
      DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = [{
    The `hlo.return` operation terminates a region and returns values.

    Example:

    ```mlir
    %0 = stablehlo.reduce %arg0, %arg1 {
      ...
      stablehlo.return %1 : tensor<f32>
    }
    ```
  }];

  let arguments = (ins
    Variadic<HLO_TensorOrTokenOrTuple >:$results
  );

  let assemblyFormat = "$results attr-dict (`:` type($results)^)?";
}

def StableHLO_TorchIndexSelectOp : StableHLO_Op<"torch_index_select", [Pure]> {
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_Tensor:$index,
    I64Attr:$dim,
    I64Attr:$batch_dims
  );

  let results = (outs HLO_Tensor);

  // TODO(hinsu): Canonicalize to lower this client side HLO op to server
  // side HLO ops.
}

def StableHLO_OptimizationBarrierOp : StableHLO_Op<"optimization_barrier",
      [Pure, HLO_PairwiseSameOperandAndResultType,
      DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
  let summary = [{
    The `stablehlo.optimization_barrier` op blocks optimizations.

  }];

  let description = [{
    Ensures that the operations that produce the `operand` are executed before any
    operations that depend on the `result` and prevents compiler transformations
    from moving operations across the barrier. Other than that, the operation is
    an identity, i.e. `result` = `operand`.

    See
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlooptimization_barrier

    Example:
    ```mlir
    %result0, %result1 = stablehlo.optimization_barrier %operand0, %operand1 : tensor<f32>, tensor<f32>
    ```
  }];

  let arguments = (ins Variadic<HLO_TensorOrToken>:$operand);

  let results = (outs Variadic<HLO_TensorOrToken>:$result);

  // Use `attr-dict` before `$operand` because Optional Group anchors in custom
  // directives are currently not supported. Also since inputs are variadic,
  // print `()` if no arguments are present, otherwise parsing is ambiguous:
  //   stablehlo.optimization_barrier
  //   %1 = stablehlo.add ...
  //   ^ Without lookahead, ambiguous if this is an operand to the previous line
  //     or the start of a separate operation, since newlines are ignored.
  let assemblyFormat = [{
    attr-dict ($operand^ `:` custom<PairwiseOpType>(type($operand), type($result))):(`(` `)`)?
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO RNG Operators.
//===----------------------------------------------------------------------===//

def StableHLO_RngOp : StableHLO_Op<"rng", [InferTensorTypeWithReify, AllElementTypesMatch<["a", "b", "result"]>]> {
  let summary = "RNG operator with uniform or normal distribution.";
  let description = [{
    Generates random numbers using the `rng_distribution` algorithm and produces
    a `result` tensor of a given shape `shape`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlorng

    Example:
    ```mlir
    %result = stablehlo.rng %a, %b, %shape, distribution = NORMAL : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32>
    ```
  }];
  let arguments = (ins
    0DTensorOf<[HLO_Pred, HLO_Int, HLO_Float]>:$a,
    0DTensorOf<[HLO_Pred, HLO_Int, HLO_Float]>:$b,
    HLO_DimensionTensor:$shape,
    StableHLO_RngDistributionAttr:$rng_distribution
  );

  let results = (outs HLO_PredIntOrFpTensor:$result);

  let hasVerifier = 1;

  let assemblyFormat = [{
    $a `,` $b `,` $shape `,` `distribution` `=` $rng_distribution
      attr-dict `:` functional-type(operands, results)
  }];
}

def StableHLO_RngBitGeneratorOp : StableHLO_Op<"rng_bit_generator", [Pure]> {
  let summary = "Uniform pseudo random number generator operator";
  let description = [{
    Returns an `output` filled with uniform random data and an updated output
    state `output_state` given an initial state `initial_state` using the
    pseudorandom number generator algorithm `rng_algorithm`.

    See:
    https://github.com/openxla/stablehlo/blob/main/docs/spec_draft.md#stablehlorng_bit_generator

    Example:
    ```mlir
    %output_state, %output = stablehlo.rng_bit_generator %initial_state, algorithm = THREE_FRY : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>)
    ```
  }];
  let arguments = (ins
    StableHLO_RngAlgorithmAttr:$rng_algorithm,
    HLO_IntOrFpTensor:$initial_state
  );

  let results = (outs
    HLO_IntOrFpTensor:$output_state,
    HLO_IntOrFpTensor:$output
  );

  let hasVerifier = 1;

  let assemblyFormat = [{
    $initial_state `,` `algorithm` `=` $rng_algorithm attr-dict
      `:` functional-type(operands, results)
  }];
}

//===----------------------------------------------------------------------===//
// StableHLO Quantize Operator.
//===----------------------------------------------------------------------===//

// TODO(b/230662142): Implement unknown scales/zero_point cases.
def StableHLO_UniformQuantizeOp : StableHLO_UnaryElementwiseOp<"uniform_quantize",
      [Pure], TensorOf<[F32, BF16, HLO_QuantizedInt]>,
      HLO_QuantizedIntTensor> {
  let summary = "Uniform quantize operator";
  let description = [{
    Converts floating point tensors or uniform quantized integer tensors to
    uniform quantized integer tensors according to the quantization parameters
    defined by the output type.

    Example:

    ```mlir
    %0 = stablehlo.uniform_quantize %arg0 : (tensor<16x16xf32>) -> tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>
    ```
  }];
}

def StableHLO_UniformDequantizeOp : StableHLO_UnaryElementwiseOp<"uniform_dequantize",
      [InferTensorType, Pure], HLO_QuantizedIntTensor, TensorOf<[F32, BF16]>> {
  let summary = "Uniform dequantize operator";
  let description = [{
    Converts quantized array of integers to floating-points according to the
    quantization parameters defined by the input type.

    Example:

    ```mlir
    %0 = stablehlo.uniform_dequantize %arg0 : (tensor<16x16x!quant.uniform<i8:f32, 34.0:16>>) -> tensor<16x16xf32>
    ```
  }];
}

def StableHLO_ReducePrecisionOp :
    StableHLO_Op<"reduce_precision", [HLO_CompatibleOperandsAndResultType]> {
  let summary = "Reduce precision operator";
  let description = [{
    Models the effect of converting floating - point values to a lower -
    precision format(such as IEEE - FP16) and back to the original
    format. The number of exponent and mantissa bits in the lower -
    precision format can be specified arbitrarily,
    although all bit sizes may not be supported on all hardware
    implementations.

    See https://www.tensorflow.org/xla/operation_semantics#reduceprecision.

    Example:

    ```mlir
    %0 = stablehlo.reduce_precision %arg0, format = e8m10 : tensor<3x4xf32>
    ```
  }];
  let arguments = (ins
    HLO_FpTensor:$operand,
    I32Attr:$exponent_bits,
    I32Attr:$mantissa_bits
  );
  let hasVerifier = 1;
  let results = (outs HLO_FpTensor:$output);

  let assemblyFormat = [{
    $operand `,` `format` `=` custom<ExponentMantissa>($exponent_bits, $mantissa_bits)
      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($output))
  }];
}

def StableHLO_RealDynamicSliceOp: StableHLO_ShapedInterfaceOp<
      "real_dynamic_slice",
      [Pure, AllElementTypesMatch<["operand", "result"]>,
       AllTypesMatch<["start_indices", "limit_indices", "strides"]>]> {
  let summary = "Real Dynamic Slice operator";
  let description = [{
    The dynamic shape version of SliceOp. Extracts a sub-array from the input
    array according to start_indices, limit_indices and strides. Expect
    start_indices/limit_indices/strides to be statically shaped and matching
    the rank of the input.

    Example:

    ```mlir
    %0 = stablehlo.real_dynamic_slice %input, %start, %limit, %strides
           : (tensor<256x?xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<256x?xf32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_DimensionTensor:$start_indices,
    HLO_DimensionTensor:$limit_indices,
    HLO_DimensionTensor:$strides
  );
  let results = (outs HLO_Tensor:$result);
  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_DynamicPadOp: StableHLO_ShapedInterfaceOp<"dynamic_pad",
      [Pure, AllElementTypesMatch<["operand", "padding_value", "result"]>,
      AllTypesMatch<["edge_padding_low", "edge_padding_high", "interior_padding"]>]> {
  let summary = "Dynamic Pad operator";
  let description = [{
    The dynamic shape version of PadOp. Pads the edges of `operand` with the
    `padding_value` and according to the passed configuration. Expect
    edge_padding_low/edge_padding_high/interior_padding to be statically shaped
    and matching the rank of the input.

    See https://www.tensorflow.org/xla/operation_semantics#pad.

    Example:

    ```mlir
    %0 = stablehlo.dynamic_pad %arg0, %arg1, %arg2, %arg3, %arg4
           : (tensor<?x?xf32>, tensor<f32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<?x?xf32>
    ```
  }];
  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_Tensor:$padding_value,
    HLO_DimensionTensor:$edge_padding_low,
    HLO_DimensionTensor:$edge_padding_high,
    HLO_DimensionTensor:$interior_padding
  );
  let results = (outs HLO_Tensor:$result);
  let description = [{
    Dynamically Pads the `operand`, with amount of padding added at
    low-end/high-end/interior is passed through input tensors.
  }];
  let hasVerifier = 1;

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_DynamicGatherOp: StableHLO_Op<"dynamic_gather",
                                [InferTensorTypeWithReify, Pure]> {
  string summary = "Dynamic Gather operator";
  string description = [{
    The dynamic shape version of GatherOp. Stitches together several slices of
    an input array.
  }];

  let arguments = (ins
    HLO_Tensor:$operand,
    HLO_IntTensor:$start_indices,
    HLO_IntTensor:$slice_sizes,
    StableHLO_GatherDimensionNumbers:$dimension_numbers,
    DefaultValuedOptionalAttr<BoolAttr, "false">:$indices_are_sorted
  );
  let results = (outs HLO_Tensor);
}

def StableHLO_DynamicConvOp : StableHLO_Op<"dynamic_conv", [Pure]> {
  let summary = "Dynamic Convolution operator";
  let description = [{
    The dynamic shape version of ConvOp. Computes a convolution with dynamic padding.
  }];

  let arguments = !con(
    (ins
       HLO_Tensor:$lhs,
       HLO_Tensor:$rhs,
       HLO_Tensor:$d_padding),
    StableHLO_ConvolutionAttributes.attributes);
  let results = (outs HLO_Tensor);
}

def StableHLO_ComputeReshapeShapeOp :
    StableHLO_Op<"compute_reshape_shape", [Pure]> {
  string summary = "Compute input for reshape with any dynamic dim resolved";

  string description = [{
    This operation handles the dynamic aspect of a TF/NumPy/CHLO reshape. The
    dynamic aspect is that a single extent can be -1 and that dimension will
    instead be computed. This handles the computation and can then be passed to
    an HLO DynamicReshapeOp to replicate the TF/NumPy reshape behavior.

    This op has undefined behavior if the dimensions do not evenly divide the
    number of elements, or if there are multiple -1 values. It is an identity op
    if no dimensions are -1.

    ```
    %0 = hlo.compute_reshape_shape 12, [2, -1] -> [2, 6]
    ```

    Example:

    ```mlir
    %0 = stablehlo.compute_reshape_shape %arg0, %arg1 : (index, tensor<2xi32>) -> tensor<2xi32>
    ```
  }];

  let arguments = (ins Index:$num_elements, 1DTensorOf<[AnyInteger, Index]>:$dynamic_shape);
  let results = (outs 1DTensorOf<[AnyInteger, Index]>:$result);

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

def StableHLO_CstrReshapableOp :
    StableHLO_Op<"cstr_reshapable", [Pure]> {
  string summary = "Compute input for reshape with any dynamic dim resolved";

  string description = [{
    This operation creates a witness on the constraint that a given shape would
    be a valid reshape for the given number of elements.

    ```
    %0 = stablehlo.cstr_reshapable 12, [2, -1] -> success
    %1 = stablehlo.cstr_reshapable 13, [2, -1] -> failure
    ```

    Example:

    ```mlir
    %0 = stablehlo.cstr_reshapable %arg0, %arg1 : (index, tensor<3xi32>) -> !shape.witness
    ```
  }];

  let arguments = (ins Index:$num_elements, 1DTensorOf<[AnyInteger, Index]>:$dynamic_shape);
  let results = (outs Shape_WitnessType:$result);

  let assemblyFormat = "operands attr-dict `:` functional-type(operands, results)";
}

#endif // STABLEHLO_DIALECT_STABLEHLO_OPS
